dataset_settings: 
  dataset_name: math_expression_dataset
  config_name: b5887579-5742-4e0f-91ce-2fae26bf2c01

model_settings:
  model_name: meta-llama/Llama-3.1-8B-Instruct
  target_modules: 
    - q_proj
    - v_proj
  max_tokens: 20

optimizer_settings:
  lr: 0.0002
  weight_decay: 0.01

optuna_settings: 
  n_trials: 1
  metric_name: f1

  study_settings:
    storage: sqlite:///optuna_lora_study.db
    study_name: optuna-lora-study
    direction: maximize
    load_if_exists: true

  trial_start_settings:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.0

  trial_settings:
    r:
      name: r
      low: 2
      high: 16
      step: 2

    lora_alpha: 
      name: lora_alpha
      low: 4
      high: 68
      step: 8
      
    lora_dropout:
      name: lora_dropout
      low: 0.0
      high: 0.3
      step: 0.05

sft_settings:
  learning_rate: 2e-4
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  num_train_epochs: 3
  weight_decay: 0.01
  fp16: true
