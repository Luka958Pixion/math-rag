# https://chatgpt.com/share/683cd917-c774-8007-a74b-7bae56366d4a
dataset_settings: 
  dataset_name: mathexpressiondataset
  config_name: 5fd00395-72b3-45bd-9326-d11f2a7a73dc

model_settings:
  model_name: meta-llama/Llama-3.1-8B
  tokenizer_name: meta-llama/Llama-3.1-8B
  target_modules: 
    - q_proj
    - v_proj

optimizer_settings:
  lr: 0.0002
  weight_decay: 0.01

optuna_settings: 
  storage_name: sqlite:///optuna_lora_study.db
  study_name: optuna_lora_study
  metric_name: f1
  direction: maximize
  n_trials: 1

  lora_initial_settings:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.0

  lora_settings:
    r:
      name: r
      low: 2
      high: 16
      step: 2

    lora_alpha: 
      name: lora_alpha
      low: 4
      high: 64
      step: 8
      
    lora_dropout:
      name: lora_dropout
      low: 0.0
      high: 0.3
      step: 0.05

sft_settings:
  learning_rate: 2e-4
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  num_train_epochs: 3
  weight_decay: 0.01
  fp16: true
