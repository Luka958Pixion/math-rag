meta-llama/Llama-3.1-8B:
  dataset_settings: 
    dataset_name: str
    config_name: str

  lora_initial_settings:
    r: int
    lora_alpha: int
    lora_dropout: float

  lora_settings:
    r_range: 
      low: int
      high: int
      step: int
    lora_alpha_range: 
      low: int
      high: int
      step: int
    lora_dropout_range: 
      low: int
      high: int
      step: int

  model_settings:
    model_name: meta-llama/Llama-3.1-8B
    tokenizer_name: meta-llama/Llama-3.1-8B
    target_modules: 
      - q_proj
      - k_proj
      - v_proj

  optuna_settings: 
    study_name: str
    metric_name: str
    direction: maximize
    n_trials: 1

  sft_settings:
    learning_rate: 2e-4
    per_device_train_batch_size: 8
    gradient_accumulation_steps: 4
    num_train_epochs: 3
    weight_decay: 0.01
    fp16: true
