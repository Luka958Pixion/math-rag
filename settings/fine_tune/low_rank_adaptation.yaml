# https://chatgpt.com/share/683cd917-c774-8007-a74b-7bae56366d4a
meta-llama/Llama-3.1-8B:
  dataset_settings: 
    dataset_name: mathexpressiondataset
    config_name: 5fd00395-72b3-45bd-9326-d11f2a7a73dc

  lora_initial_settings:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.0

  lora_settings:
    r_range: 
      low: 2
      high: 16
      step: 2
    lora_alpha_range: 
      low: 4
      high: 64
      step: 8
    lora_dropout_range: 
      low: 0.0
      high: 0.3
      step: 0.05

  model_settings:
    model_name: meta-llama/Llama-3.1-8B
    tokenizer_name: meta-llama/Llama-3.1-8B
    target_modules: 
      - q_proj
      - v_proj

  optuna_settings: 
    study_name: str
    metric_name: str
    direction: maximize
    n_trials: 1

  sft_settings:
    learning_rate: 2e-4
    per_device_train_batch_size: 8
    gradient_accumulation_steps: 4
    num_train_epochs: 3
    weight_decay: 0.01
    fp16: true
