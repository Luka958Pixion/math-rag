{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import Queue, create_task, sleep\n",
    "from collections import deque\n",
    "from time import ctime, perf_counter\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from tiktoken import get_encoding\n",
    "\n",
    "from math_rag.application.models.inference import LLMRequest, LLMRequestBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMStatusTracker(BaseModel):\n",
    "    num_tasks_started: int = 0\n",
    "    num_tasks_in_progress: int = 0\n",
    "    num_tasks_succeeded: int = 0\n",
    "    num_tasks_failed: int = 0\n",
    "    num_rate_limit_errors: int = 0\n",
    "    num_api_errors: int = 0\n",
    "    num_other_errors: int = 0\n",
    "    time_of_last_rate_limit_error: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMRequestWrapper:\n",
    "    request: LLMRequest\n",
    "    errors: list = Field(default_factory=list)\n",
    "\n",
    "    # NOTE: important for processing\n",
    "    token_consumption: int\n",
    "    attempts_left: int\n",
    "\n",
    "    async def call_api(  # TODO reuse existing generate method?\n",
    "        self,\n",
    "        retry_queue: Queue['LLMRequestWrapper'],\n",
    "        status_tracker: LLMStatusTracker,\n",
    "    ):\n",
    "        error = None\n",
    "        try:\n",
    "            response = ...  # call api\n",
    "\n",
    "            if 'error' in response:\n",
    "                logging.warning(\n",
    "                    f'Request {self.request.id} failed with {response['error']}'\n",
    "                )\n",
    "                status_tracker.num_api_errors += 1\n",
    "                error = response\n",
    "\n",
    "                if 'rate limit' in response['error'].get('message', '').lower():\n",
    "                    status_tracker.time_of_last_rate_limit_error = perf_counter()\n",
    "                    status_tracker.num_rate_limit_errors += 1\n",
    "                    status_tracker.num_api_errors -= 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f'Request {self.request.id} failed with {e}')\n",
    "            status_tracker.num_other_errors += 1\n",
    "            error = e\n",
    "\n",
    "        if error:\n",
    "            self.errors.append(error)\n",
    "\n",
    "            if self.attempts_left:\n",
    "                retry_queue.put_nowait(self)\n",
    "            else:\n",
    "                logging.error(f'Request {self.request} failed after all attempts')\n",
    "                data = [self.request, [str(e) for e in self.errors]]\n",
    "                # save data\n",
    "                status_tracker.num_tasks_in_progress -= 1\n",
    "                status_tracker.num_tasks_failed += 1\n",
    "        else:\n",
    "            data = [self.request, response]\n",
    "            # save data\n",
    "            status_tracker.num_tasks_in_progress -= 1\n",
    "            status_tracker.num_tasks_succeeded += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_request(request_json: dict):\n",
    "    encoding = get_encoding(...)\n",
    "\n",
    "    max_tokens = request_json.get('max_tokens', 15)\n",
    "    n = request_json.get('n', 1)\n",
    "    completion_tokens = n * max_tokens\n",
    "\n",
    "    num_tokens = 0\n",
    "    for message in request_json['messages']:\n",
    "        num_tokens += (\n",
    "            4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        )\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == 'name':  # if there's a name, the role is omitted\n",
    "                num_tokens -= 1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "\n",
    "    return num_tokens + completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def concurrent_generate(\n",
    "    request_batch: LLMRequestBatch,\n",
    "    max_requests_per_minute: float,\n",
    "    max_tokens_per_minute: float,\n",
    "    max_attempts: int,\n",
    "):\n",
    "    SECONDS_TO_PAUSE_AFTER_RATE_LIMIT_ERROR = 15\n",
    "    SECONDS_TO_SLEEP_EACH_LOOP = 0.001\n",
    "\n",
    "    queue_of_requests_to_retry: Queue[LLMRequestWrapper] = Queue()\n",
    "    status_tracker = LLMStatusTracker()\n",
    "    next_request: LLMRequestWrapper | None = None\n",
    "\n",
    "    available_request_capacity = max_requests_per_minute\n",
    "    available_token_capacity = max_tokens_per_minute\n",
    "    last_update_time = perf_counter()\n",
    "\n",
    "    file_not_finished = True\n",
    "\n",
    "    requests: deque[LLMRequest] = deque(request_batch.requests)\n",
    "\n",
    "    while True:\n",
    "        if next_request is None:\n",
    "            if not queue_of_requests_to_retry.empty():\n",
    "                next_request = queue_of_requests_to_retry.get_nowait()\n",
    "                logging.debug(f'Retrying request {next_request.request.id}')\n",
    "\n",
    "            elif file_not_finished:\n",
    "                if requests:\n",
    "                    request = requests.popleft()\n",
    "                    token_consumption = num_tokens_from_request(request)\n",
    "                    next_request = LLMRequestWrapper(\n",
    "                        request=request,\n",
    "                        token_consumption=token_consumption,\n",
    "                        attempts_left=max_attempts,\n",
    "                    )\n",
    "                    status_tracker.num_tasks_started += 1\n",
    "                    status_tracker.num_tasks_in_progress += 1\n",
    "                    logging.debug(f'Reading request {next_request.request.id}')\n",
    "\n",
    "                else:\n",
    "                    file_not_finished = False\n",
    "\n",
    "        current_time = perf_counter()\n",
    "        seconds_since_update = current_time - last_update_time\n",
    "        available_request_capacity = min(\n",
    "            available_request_capacity\n",
    "            + max_requests_per_minute * seconds_since_update / 60.0,\n",
    "            max_requests_per_minute,\n",
    "        )\n",
    "        available_token_capacity = min(\n",
    "            available_token_capacity\n",
    "            + max_tokens_per_minute * seconds_since_update / 60.0,\n",
    "            max_tokens_per_minute,\n",
    "        )\n",
    "        last_update_time = current_time\n",
    "\n",
    "        if next_request:\n",
    "            next_request_tokens = next_request.token_consumption\n",
    "\n",
    "            if (\n",
    "                available_request_capacity >= 1\n",
    "                and available_token_capacity >= next_request_tokens\n",
    "            ):\n",
    "                available_request_capacity -= 1\n",
    "                available_token_capacity -= next_request_tokens\n",
    "                next_request.attempts_left -= 1\n",
    "\n",
    "                create_task(\n",
    "                    next_request.call_api(\n",
    "                        retry_queue=queue_of_requests_to_retry,\n",
    "                        status_tracker=status_tracker,\n",
    "                    )\n",
    "                )\n",
    "                next_request = None\n",
    "\n",
    "        if status_tracker.num_tasks_in_progress == 0:\n",
    "            break\n",
    "\n",
    "        await sleep(SECONDS_TO_SLEEP_EACH_LOOP)\n",
    "        seconds_since_rate_limit_error = (\n",
    "            perf_counter() - status_tracker.time_of_last_rate_limit_error\n",
    "        )\n",
    "\n",
    "        if seconds_since_rate_limit_error < SECONDS_TO_PAUSE_AFTER_RATE_LIMIT_ERROR:\n",
    "            remaining_seconds_to_pause = (\n",
    "                SECONDS_TO_PAUSE_AFTER_RATE_LIMIT_ERROR - seconds_since_rate_limit_error\n",
    "            )\n",
    "            await sleep(remaining_seconds_to_pause)\n",
    "\n",
    "            logging.warning(\n",
    "                f'Pausing to cool down until {ctime(status_tracker.time_of_last_rate_limit_error + SECONDS_TO_PAUSE_AFTER_RATE_LIMIT_ERROR)}'\n",
    "            )\n",
    "\n",
    "    if status_tracker.num_tasks_failed > 0:\n",
    "        logging.warning(\n",
    "            f'{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed'\n",
    "        )\n",
    "\n",
    "    if status_tracker.num_rate_limit_errors > 0:\n",
    "        logging.warning(\n",
    "            f'{status_tracker.num_rate_limit_errors} rate limit errors received'\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math-rag-QVTscwFS-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
