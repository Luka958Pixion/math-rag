{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import Queue, create_task, sleep\n",
    "from collections import deque\n",
    "from time import ctime, perf_counter\n",
    "from typing import Generic\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from tiktoken import get_encoding\n",
    "\n",
    "from math_rag.application.models.inference import LLMRequest, LLMRequestBatch\n",
    "from math_rag.application.types.inference import LLMResponseType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMStatusTracker(BaseModel):\n",
    "    num_tasks_started: int = 0\n",
    "    num_tasks_in_progress: int = 0\n",
    "    num_tasks_succeeded: int = 0\n",
    "    num_tasks_failed: int = 0\n",
    "    num_rate_limit_errors: int = 0\n",
    "    num_api_errors: int = 0\n",
    "    time_of_last_rate_limit_error: int = 0\n",
    "\n",
    "\n",
    "class LLMError(BaseModel):\n",
    "    message: str\n",
    "    body: object | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import (\n",
    "    APIConnectionError,\n",
    "    APIError,\n",
    "    APITimeoutError,\n",
    "    AuthenticationError,\n",
    "    BadRequestError,\n",
    "    ConflictError,\n",
    "    InternalServerError,\n",
    "    NotFoundError,\n",
    "    PermissionDeniedError,\n",
    "    RateLimitError,\n",
    "    UnprocessableEntityError,\n",
    ")\n",
    "\n",
    "\n",
    "class LLMRequestWrapper(LLMRequest[LLMResponseType], Generic[LLMResponseType]):\n",
    "    errors: list[APIError] = Field(default_factory=list)\n",
    "\n",
    "    # NOTE: for status\n",
    "    token_consumption: int\n",
    "    attempts_left: int\n",
    "\n",
    "    async def call_api(  # TODO reuse existing generate method?\n",
    "        self,\n",
    "        retry_queue: Queue['LLMRequestWrapper'],\n",
    "        status_tracker: LLMStatusTracker,\n",
    "    ):\n",
    "        error = None\n",
    "\n",
    "        try:\n",
    "            response = ...  # call api\n",
    "\n",
    "        # TODO new ----\n",
    "        except RateLimitError as e:\n",
    "            error = e\n",
    "            status_tracker.time_of_last_rate_limit_error = perf_counter()\n",
    "            status_tracker.num_rate_limit_errors += 1\n",
    "\n",
    "        except (APITimeoutError, InternalServerError, UnprocessableEntityError) as e:\n",
    "            error = e\n",
    "            status_tracker.num_api_errors += 1\n",
    "\n",
    "        except (\n",
    "            APIConnectionError,\n",
    "            AuthenticationError,\n",
    "            BadRequestError,\n",
    "            ConflictError,\n",
    "            NotFoundError,\n",
    "            PermissionDeniedError,\n",
    "        ) as e:\n",
    "            raise\n",
    "\n",
    "        # TODO new end----\n",
    "\n",
    "        if error:\n",
    "            self.errors.append(error)\n",
    "\n",
    "            if self.attempts_left:\n",
    "                retry_queue.put_nowait(self)\n",
    "\n",
    "            else:\n",
    "                logging.error(f'Request {self.id} failed after all attempts')\n",
    "                # TODO: save self.errors\n",
    "                status_tracker.num_tasks_in_progress -= 1\n",
    "                status_tracker.num_tasks_failed += 1\n",
    "        else:\n",
    "            # TODO: save response\n",
    "            status_tracker.num_tasks_in_progress -= 1\n",
    "            status_tracker.num_tasks_succeeded += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_request(request_json: dict):\n",
    "    encoding = get_encoding(...)\n",
    "\n",
    "    max_tokens = request_json.get('max_tokens', 15)\n",
    "    n = request_json.get('n', 1)\n",
    "    completion_tokens = n * max_tokens\n",
    "\n",
    "    num_tokens = 0\n",
    "    for message in request_json['messages']:\n",
    "        num_tokens += (\n",
    "            4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        )\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == 'name':  # if there's a name, the role is omitted\n",
    "                num_tokens -= 1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "\n",
    "    return num_tokens + completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def concurrent_generate(\n",
    "    request_batch: LLMRequestBatch,\n",
    "    max_requests_per_minute: float,\n",
    "    max_tokens_per_minute: float,\n",
    "    max_attempts: int,\n",
    "):\n",
    "    SECONDS_TO_PAUSE_AFTER_RATE_LIMIT_ERROR = 15\n",
    "    SECONDS_TO_SLEEP_EACH_LOOP = 0.001\n",
    "\n",
    "    retry_queue: Queue[LLMRequestWrapper] = Queue()\n",
    "    status_tracker = LLMStatusTracker()\n",
    "    next_request: LLMRequestWrapper | None = None\n",
    "\n",
    "    available_request_capacity = max_requests_per_minute\n",
    "    available_token_capacity = max_tokens_per_minute\n",
    "    last_update_time = perf_counter()\n",
    "\n",
    "    file_not_finished = True\n",
    "\n",
    "    requests: deque[LLMRequest] = deque(request_batch.requests)\n",
    "\n",
    "    while True:\n",
    "        if next_request is None:\n",
    "            if not retry_queue.empty():\n",
    "                next_request = retry_queue.get_nowait()\n",
    "                logging.debug(f'Retrying request {next_request.id}')\n",
    "\n",
    "            elif file_not_finished:\n",
    "                if requests:\n",
    "                    request = requests.popleft()\n",
    "                    token_consumption = num_tokens_from_request(request)\n",
    "                    next_request = LLMRequestWrapper(\n",
    "                        request=request,\n",
    "                        token_consumption=token_consumption,\n",
    "                        attempts_left=max_attempts,\n",
    "                    )\n",
    "                    status_tracker.num_tasks_started += 1\n",
    "                    status_tracker.num_tasks_in_progress += 1\n",
    "                    logging.debug(f'Reading request {next_request.id}')\n",
    "\n",
    "                else:\n",
    "                    file_not_finished = False\n",
    "\n",
    "        current_time = perf_counter()\n",
    "        seconds_since_update = current_time - last_update_time\n",
    "        available_request_capacity = min(\n",
    "            available_request_capacity\n",
    "            + max_requests_per_minute * seconds_since_update / 60.0,\n",
    "            max_requests_per_minute,\n",
    "        )\n",
    "        available_token_capacity = min(\n",
    "            available_token_capacity\n",
    "            + max_tokens_per_minute * seconds_since_update / 60.0,\n",
    "            max_tokens_per_minute,\n",
    "        )\n",
    "        last_update_time = current_time\n",
    "\n",
    "        if next_request:\n",
    "            next_request_tokens = next_request.token_consumption\n",
    "\n",
    "            if (\n",
    "                available_request_capacity >= 1\n",
    "                and available_token_capacity >= next_request_tokens\n",
    "            ):\n",
    "                available_request_capacity -= 1\n",
    "                available_token_capacity -= next_request_tokens\n",
    "                next_request.attempts_left -= 1\n",
    "\n",
    "                create_task(\n",
    "                    next_request.call_api(\n",
    "                        retry_queue=retry_queue,\n",
    "                        status_tracker=status_tracker,\n",
    "                    )\n",
    "                )\n",
    "                next_request = None\n",
    "\n",
    "        if status_tracker.num_tasks_in_progress == 0:\n",
    "            break\n",
    "\n",
    "        await sleep(SECONDS_TO_SLEEP_EACH_LOOP)\n",
    "        seconds_since_rate_limit_error = (\n",
    "            perf_counter() - status_tracker.time_of_last_rate_limit_error\n",
    "        )\n",
    "\n",
    "        if seconds_since_rate_limit_error < SECONDS_TO_PAUSE_AFTER_RATE_LIMIT_ERROR:\n",
    "            remaining_seconds_to_pause = (\n",
    "                SECONDS_TO_PAUSE_AFTER_RATE_LIMIT_ERROR - seconds_since_rate_limit_error\n",
    "            )\n",
    "            await sleep(remaining_seconds_to_pause)\n",
    "\n",
    "            logging.warning(\n",
    "                f'Pausing to cool down until {ctime(status_tracker.time_of_last_rate_limit_error + SECONDS_TO_PAUSE_AFTER_RATE_LIMIT_ERROR)}'\n",
    "            )\n",
    "\n",
    "    if status_tracker.num_tasks_failed > 0:\n",
    "        logging.warning(\n",
    "            f'{status_tracker.num_tasks_failed} / {status_tracker.num_tasks_started} requests failed'\n",
    "        )\n",
    "\n",
    "    if status_tracker.num_rate_limit_errors > 0:\n",
    "        logging.warning(\n",
    "            f'{status_tracker.num_rate_limit_errors} rate limit errors received'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 404 - {'error': {'message': 'The model `gpt-4o-mini-NONE` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "{'message': 'The model `gpt-4o-mini-NONE` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}\n"
     ]
    }
   ],
   "source": [
    "from openai import APIConnectionError, NotFoundError\n",
    "\n",
    "from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "\n",
    "RESET = False\n",
    "\n",
    "infrastructure_container = InfrastructureContainer()\n",
    "infrastructure_container.init_resources()\n",
    "\n",
    "llm = infrastructure_container.openai_llm()\n",
    "\n",
    "try:\n",
    "    # llm.client.api_key = \"\"\n",
    "    completion = await llm.client.chat.completions.create(\n",
    "        messages=[{'role': 'user', 'content': 'what is 2+2'}],\n",
    "        model='gpt-4o-mini-NONE',\n",
    "        temperature=0.0,\n",
    "    )\n",
    "except APIConnectionError as e:\n",
    "    print(e)\n",
    "except NotFoundError as e:\n",
    "    print(e.message)\n",
    "    print(e.body)  # body (can be None) has message, type, param, code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math-rag-QVTscwFS-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
