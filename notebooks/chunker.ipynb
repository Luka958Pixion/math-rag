{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500b0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from math_rag.application.containers import ApplicationContainer\n",
    "    from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "    application_container: ApplicationContainer\n",
    "    infrastructure_container: InfrastructureContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcce5a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 21:37:49,533 - INFO - datasets - config.py:54 - PyTorch version 2.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "RESET = False\n",
    "%load_ext hooks.notebook_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1e59340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 21:37:50,168 - INFO - googleapiclient.discovery_cache - __init__.py:49 - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "google_drive_repository = infrastructure_container.google_drive_repository()\n",
    "\n",
    "file_id = google_drive_repository.get_file_id(\n",
    "    Path('ml/lectures/L07-LogisticRegression2/2024_08_10_2174b40686820b4cb591g.tex')\n",
    ")\n",
    "\n",
    "if not file_id:\n",
    "    raise ValueError()\n",
    "\n",
    "file_content = google_drive_repository.get_file_by_id(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c589e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = file_content.getvalue().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da193472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathArticle\n",
    "\n",
    "\n",
    "math_article = MathArticle(\n",
    "    math_expression_dataset_id=None, index_id=None, name='article', bytes=file_content.getvalue()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab51f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_article_parser_service = infrastructure_container.math_article_parser_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad74d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_nodes_, positions, template = math_article_parser_service.parse_for_index(math_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be7c64e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jan Šnajder, lectures, v2.0\n",
      "\n",
      "Last time we introduced the logistic regression algorithm. We defined the model and derived the cross-entropy error function as the negative probability of the labels in the training set. We established that minimizing that error had no solution in closed form, so we turned to iterative procedures. We have considered the simplest such procedure, the gradient descent algorithm, and we applied it to logistic regression, in standard (batch) and stochastic variant. In the end, we talked about regularization, specifically [math_placeholder | 0] regularization, which we incorporated quite straightforwardly into the optimization process.\n",
      "\n",
      "Today we'll talk a bit more about logistic regression. First, we'll consider some more efficient (read: faster) alternatives to gradient descent. Second, we'll consider the extension of binary logistic regression to multiclass logistic regression. Third, we'll look at all the models discussed thus far and see what they have in common and how they can be generalized. Finally, we'll talk about adaptive basis functions, which is a way to learn the feature mapping function directly from data, instead of defining it manually.\n",
      "\n",
      "Unlike previous lectures, this one won't go into details, because we do not have the time for this. My goal is to give you enough information to know where to look for more, should you feel motivated to do so or if you need it\n",
      "\n",
      "\n",
      "Last time we established that the gradient descent needs to be coupled with line search, otherwise there is no guarantee for global convergence. This means that, depending on where the descent initially starts, it can happen that the gradient descent does not converge but diverge (effectively it starts to ascend instead of descend). Line search prevents this from happening. However, line search can result in a zig-zag descent. Let's recall the figure we discussed last time:\n",
      "\n",
      "\n",
      "[image_placeholder | 0]\n",
      "\n",
      "\n",
      "The blue trajectory corresponds to the best scenario, and the red to the worst-case scenario for a gradient descent with line search. Which one will play out depends on the choice of the starting point of the search. As we can see, it can happen that the descent zig-zags quite a lot, which means that the optimization will consume a lot of iterations. Obviously, the problem arises because the descent direction is not as good as it could be. Imagine descending into a pit from the starting point for the red trajectory. It is hard to imagine that you will descend\n",
      "as pointed to by the line. It is more likely that the gravity would pull you and your descend direction would be steeper (i.e., at a smaller angle to the blue line). This reasoning of ours is based on the curvature of the isocontours (that is, the curvature of the surface down which we descend). In other words, the curvature of the surface gives us, in addition to the gradient, additional information about where the minimum is likely to be located (at least when it comes to convex functions).\n",
      "\n",
      "The above observations apply to the standard (batch) gradient. In stochastic gradient descent we typically do not use line search. But there the descent will zig-zag quite a lot anyway, since each step is taken based on the gradient calculated for a single example. In the following, we focus on non-stochastic, i.e., batch gradient descent.\n",
      "\n",
      "Based on the above consideration, we can conclude that the batch gradient descent could be improved if we take into account not only the slope (gradient) but also the curvature (the change in gradient, i.e., the second derivative) of the error function. Such optimization methods are referred to as second-order optimization, as opposed to first-order optimization methods, such as gradient descent. The basic second-order optimization method is the Newton's method.\n",
      "\n",
      "\n",
      "Consider minimization of function [math_placeholder | 1]. We know that the parameter update in gradient descent is as follows:\n",
      "\n",
      "[math_placeholder | 2]\n",
      "\n",
      "If we introduce an index for the iterations, then we can write this as an equation:\n",
      "\n",
      "[math_placeholder | 3]\n",
      "\n",
      "The idea with Newton's method is to take the point [math_placeholder | 4] (the current minimum) and compute at it the quadratic approximation of the function [math_placeholder | 5], and then move to the minimizer of this quadratic approximation (which is known analytically). If [math_placeholder | 6] is a function of one variable, this would look like this:\n",
      "\n",
      "\n",
      "[image_placeholder | 1]\n",
      "\n",
      "\n",
      "The black curve is the function [math_placeholder | 7] that we minimize. We start from point [math_placeholder | 8]. At this point we do a quadratic approximation of the function [math_placeholder | 9], thus obtaining a parabola that is tangential to the function [math_placeholder | 10] at the point [math_placeholder | 11]. The search then moves to a point that minimizes the quadratic approximation of [math_placeholder | 12]. In the picture it is the point [math_placeholder | 13]. At that point we again do a quadratic approximation of the function [math_placeholder | 14] and move to the point that minimizes that approximation. The procedure is repeated until the update is small enough.\n",
      "\n",
      "The methods works just the same for functions of several variables, i.e., in a multidimensional parameter space. In a two-dimensional space, it looks like this:\n",
      "\n",
      "\n",
      "[image_placeholder | 2]\n",
      "\n",
      "\n",
      "So, the idea is to take the step exactly so that we land in the minimum of the quadratic approximation. This will work well if [math_placeholder | 15] is a convex function, which happens to be the case with logistic regression.\n",
      "\n",
      "Let us now deal with the technical details. We need to find a quadratic approximation of a function at some point. How can we do this? Recall, from calculus, that every differentiable function [math_placeholder | 16] can be expressed at some given point [math_placeholder | 17] in the form of a power series. More precisely, every differentiable function can be expanded into a Taylor series about the point [math_placeholder | 18], as follows:\n",
      "\n",
      "[math_placeholder | 19]\n",
      "\n",
      "As we only need a quadratic approximation, we will take only the first three terms of the Taylor series. This then is the second-order Taylor expansion:\n",
      "\n",
      "[math_placeholder | 20]\n",
      "\n",
      "Note that this now is an approximation; the equality between [math_placeholder | 21] on the left-hand side and the series on the right-hand side is valid only when the series is infinite.\n",
      "\n",
      "Our error function, [math_placeholder | 22], is a function of multiple variables (i.e., vector w). For a multivariate function [math_placeholder | 23], the quadratic expansion about the point [math_placeholder | 24] is:\n",
      "\n",
      "[math_placeholder | 25]\n",
      "\n",
      "where [math_placeholder | 26] is the Hessian matrix (hrv. Hesseova matrica) of function [math_placeholder | 27] at point [math_placeholder | 28]. The Hessian matrix is a square [math_placeholder | 29] matrix of second-order partial derivatives of function [math_placeholder | 30], which is a function that maps [math_placeholder | 31]-dimensional vectors into scalars. The Hessian matrix is defined as follows:\n",
      "\n",
      "[math_placeholder | 32]\n",
      "\n",
      "that is, an element of the Hessian matrix is defined as:\n",
      "\n",
      "[math_placeholder | 33]\n",
      "\n",
      "Note that the Hessian matrix is a symmetric matrix, as the order of partial differentiation does not affect the result (commutativity).\n",
      "\n",
      "Using the expansion of the quadratic function [math_placeholder | 34], one can now derive its minimum, which us exactly where we want to step to when descending (we omit the derivation). The parameter update is then:\n",
      "\n",
      "[math_placeholder | 35]\n",
      "\n",
      "If the Hessian matrix is computed exactly (i.e., it is not an approximation), then we can set [math_placeholder | 36], because then the step will land exactly in the minimum of the quadratic approximation.\n",
      "\n",
      "We see that the Newton's optimization method requires us to compute the inverse of the Hessian matrix. In general, the Hessian matrix [math_placeholder | 37] is positive semi-definite ( [math_placeholder | 38] for every non-zero vector [math_placeholder | 39] ) if and only if [math_placeholder | 40] is a convex function. However, for its inverse to exist, the matrix [math_placeholder | 41] has to be positive definite ( [math_placeholder | 42] for each non-zero vector [math_placeholder | 43] ). If [math_placeholder | 44] is positive semi-definite, but not positive definite, then [math_placeholder | 45] has no inverse and we cannot apply the Newton's method.\n",
      "\n",
      "\n",
      "Let us now apply Newton's procedure to logistic regression. The weights update rule is as follows:\n",
      "\n",
      "[math_placeholder | 46]\n",
      "\n",
      "One can easily derive that the Hessian matrix for the cross-entropy error equals:\n",
      "\n",
      "[math_placeholder | 47]\n",
      "\n",
      "where [math_placeholder | 48], i.e., a diagonal matrix that on its diagonal has the firstorder derivatives of the logistic output [math_placeholder | 49] for each of the [math_placeholder | 50] examples from the training set. Just in case, let's check the compatibility of the matrices we multiply here: [math_placeholder | 51] [math_placeholder | 52]. Everything is fine: the dimension of the matrix [math_placeholder | 53] is [math_placeholder | 54], where [math_placeholder | 55] is the number of features in the feature space (i.e., after mapping).\n",
      "\n",
      "Let's go back quickly to the problem of computing the inverse of the Hessian matrix [math_placeholder | 56]. As we already said, [math_placeholder | 57] is positive semi-definite if and only if the function is convex. The crossentropy error is convex, so the [math_placeholder | 58] matrix for logistic regression will be positive semi-definite. But this alone does not guarantee it is invertible. However, one can show that the Hessian matrix [math_placeholder | 59] of cross-entropy, which can be decomposed as [math_placeholder | 60], must be positive definite. In other words, the Hessian matrix of the cross entropy error always has an inverse, so we can always use the Newton's method to optimize the parameters of logistic regression. However, due to multicollinearity, [math_placeholder | 61] may be ill-conditioned and in this case the solution will be unstable, which means we have to either select a linearly independent subset of features or apply regularization (we'll see how soon).\n",
      "\n",
      "If we were now to plug in [math_placeholder | 62] for [math_placeholder | 63] in the weights update rule above, and rearrange the expression a bit, we would obtain the iteratively reweighted least squares (IRLS) algorithm (hrv. algoritam najmanjih kvadrata s iterativnim ažuriranjem težina). We won't go into the details; it suffices for you to know that this algorithm is used for faster optimization for logistic regression, and that it is in fact an application of the Newton's method, which is a second-order optimization procedure.\n",
      "\n",
      "\n",
      "The problem with Newton's method (and thus with IRLS) is that the computation of the Hessian (and especially its inverse) can be expensive (especially if [math_placeholder | 64], the number of dimensions of the feature space, is large). Note that one has to compute the Hessian matrix and its inverse in every step of the optimization procedure. It might well be the case that doing gradient descent is less taxing computationally, even if the descent zig-zags a lot!\n",
      "\n",
      "The alternative is that, instead of computing the exact Hessian matrix, we compute its approximation. This is what the so-called quasi-Newton methods do. These procedures using the gradient vectors (from the current and previous step) to approximate the Hessian matrix (or its inverse) in each step of the descent. The most common such method is the BFSG algorithm. Again, we won't go into details, we just want you to be aware that the method exists.\n",
      "\n",
      "Another problem is that, even if we approximate the Hessian matrix, storing it in memory can be problematic because its dimensions are [math_placeholder | 65]. In that case, the matrix can be \"compressed\" using a low-rank approximation method. The algorithm that works that way is the limited BFSG (L-BFSG) (hrv. ograničen BFSG). Again, it suffices for you to know that this algorithm exists.\n",
      "\n",
      "\n",
      "We already know about the benefits of regularization, and last time we talked about the additional benefit of regularization in logistic regression (when optimizing the parameters for linearly separable problems, we have [math_placeholder | 66] and logistic regression overfits easily). Extending the Newton's method to regularized cross-entropy error is simple. Let's look at [math_placeholder | 67] regulation. We already considered adding the [math_placeholder | 68] regularization term to the gradient:\n",
      "\n",
      "[math_placeholder | 69]\n",
      "\n",
      "We have the same for the Hessian matrix:\n",
      "\n",
      "[math_placeholder | 70]\n",
      "\n",
      "where [math_placeholder | 71] is a diagonal matrix with a regularization factor [math_placeholder | 72] on the diagonal (except for the upper-left element, because the weight [math_placeholder | 73] is not regularized).\n",
      "\n",
      "We see, thus, that [math_placeholder | 74] regularization is easily incorporated into the gradient descent and in the Newton's method, and it was just as easily incorporated into the least squares optimization method for the linear regression. All in all, [math_placeholder | 75] regularization is a tame beast.\n",
      "\n",
      "What about [math_placeholder | 76] regularization? So far, we haven't dealt with it at all, and we won't deal with it now, either. Let it be said that things get complicated here, which is expected because [math_placeholder | 77] norm is not differentiable, so we cannot calculate the gradient. We instead compute the subgradient (engl. podgradijent). We then typically use the coordinate descent (engl. koordinatni spust), where we optimize by each variable in term (dimension after dimension), or we use proximal or projection optimization methods. For now, you only need to know that optimization for [math_placeholder | 78] regularized logistic regression is possible, that there are algorithms for it, and that they are implemented in standard tools.\n",
      "\n",
      "\n",
      "Last time we talked about binary logistic regression: we classified into classes [math_placeholder | 79] and [math_placeholder | 80]. To get the probabilities, we used the sigmoid for the activation function:\n",
      "\n",
      "[math_placeholder | 81]\n",
      "\n",
      "But what if we have more than two classes, i.e. [math_placeholder | 82] ? We could apply the OVO or OVR decomposition scheme, but the problem is that the probabilities for the individual classes would not add up to 1 . Moreover, in statistical sense the estimates for the parameters for the individual models are less reliable than estimates for a model that considers all classes at once. Instead, it's better to use the multinomial logistic regression (MNR, occasionally MLR), also referred to as maximum entropy classifier (hrv. klasifikator maksimalne entropije).\n",
      "\n",
      "\n",
      "The idea is actually very simple: use a separate weight vector [math_placeholder | 83] for each of the [math_placeholder | 84] classes, but then pass the scalar product [math_placeholder | 85] through an appropriate activation function to be make sure that the probabilities of all classes add up to 1. A function that does exactly this is the softmax\n",
      "\n",
      "function. For some input example [math_placeholder | 86], the softmax function takes the values [math_placeholder | 87] for each of the [math_placeholder | 88] classes, i.e., a [math_placeholder | 89]-dimensional vector, and maps them to a [math_placeholder | 90]-dimensional vector whose components sum to 1 . Formally, softmax : [math_placeholder | 91], where [math_placeholder | 92] is the component of the output vector equal to:\n",
      "\n",
      "[math_placeholder | 93]\n",
      "\n",
      "The softmax function accomplishes two things: it normalizes all values to a total of 1 , but it also amplifies the larger values and attenuates the smaller values. The function is called softmax because it corresponds a \"soft\" variant of the max function (in the sense that, unlike the max function, it is continuous and differentiable). Let's look at an example.\n",
      "\n",
      "[math_placeholder | 94] EXAMPLE\n",
      "\n",
      "\n",
      "[image_placeholder | 3]\n",
      "\n",
      "\n",
      "[math_placeholder | 95]\n",
      "\n",
      "\n",
      "[image_placeholder | 4]\n",
      "\n",
      "\n",
      "[math_placeholder | 96]\n",
      "\n",
      "We will define the model [math_placeholder | 97] of multinomial logistic regression as a set of models [math_placeholder | 98], where each model [math_placeholder | 99] is responsible for class [math_placeholder | 100] out of [math_placeholder | 101] classes. We define each model [math_placeholder | 102] so that it outputs the probability of example [math_placeholder | 103] belonging to class [math_placeholder | 104], using the softmax function:\n",
      "\n",
      "[math_placeholder | 105]\n",
      "\n",
      "where [math_placeholder | 106] is a matrix [math_placeholder | 107] weight vectors [math_placeholder | 108]. Note that, by virtue of the softmax function, the model [math_placeholder | 109] for class [math_placeholder | 110] takes into account the outputs of the other [math_placeholder | 111] models for the remaining classes.\n",
      "\n",
      "This defines the model. Let us now derive the error function.\n",
      "\n",
      "\n",
      "In binary logistic regression, we defined derived the error function staring from the negative logarithm of the probability of the labels. The labels were binary, [math_placeholder | 112], that is, they were Bernoulli variables. Now, since the output of multiclass regression can take more than two [math_placeholder | 113] values, then we move to categorical variable, which is also also called multinomial, or, perhaps better, the multinoulli variable. We represent such a variable as a vector of indicator (binary) variables:\n",
      "\n",
      "[math_placeholder | 114]\n",
      "\n",
      "where [math_placeholder | 115] if the outcome of the variable is [math_placeholder | 116], otherwise [math_placeholder | 117]. For example, [math_placeholder | 118] indicates that the multinomial variable has taken the third state out of four possible states. [math_placeholder | 119] is valid (outcomes are mutually exclusive and complete). Let's denote the probability [math_placeholder | 120] as [math_placeholder | 121].\n",
      "\n",
      "We will now define the distribution of this variable. Recall, the distribution of the Bernoulli variable, which has only two values, [math_placeholder | 122] and [math_placeholder | 123], is defined via the [math_placeholder | 124] parameter as follows:\n",
      "\n",
      "[math_placeholder | 125]\n",
      "\n",
      "We can generalize this to [math_placeholder | 126] values as follows. First, we need [math_placeholder | 127] parameters, so we will define a parameter vector:\n",
      "\n",
      "[math_placeholder | 128]\n",
      "\n",
      "where parameters [math_placeholder | 129] satisfy [math_placeholder | 130] and [math_placeholder | 131], as they represent probabilities.\n",
      "\n",
      "Now, by analogy with the Bernoulli distribution, the distribution of a categorical variable can be defined as:\n",
      "\n",
      "[math_placeholder | 132]\n",
      "\n",
      "Note: as with binary logistic regression, the probability that example [math_placeholder | 133] belongs to class [math_placeholder | 134] is exactly what we are given by the model:\n",
      "\n",
      "[math_placeholder | 135]\n",
      "\n",
      "Now we can finally write the logarithm of the probability of the labels from [math_placeholder | 136] as:\n",
      "\n",
      "[math_placeholder | 137]\n",
      "\n",
      "The error function we wish to minimize is the negative logarithm of the probability of the labels:\n",
      "\n",
      "[math_placeholder | 138]\n",
      "\n",
      "We see that we arrived at the generalization of the cross-entropy error to [math_placeholder | 139] classes. Also, from this we can read off the loss function as:\n",
      "\n",
      "[math_placeholder | 140]\n",
      "\n",
      "The logic is the same as with binary logistic regression: if the label [math_placeholder | 141] of some example [math_placeholder | 142] for class [math_placeholder | 143] is equal to 1 , then we want the model prediction (softmax output) to be a high probability close to 1 , because then [math_placeholder | 144] and the loss will be zero. Otherwise, if the model for the example whose label is 1 gives a value close to 0 , then the logarithm will be a large negative number, its negation will be a large positive number, and consequently the loss will be large.\n",
      "\n",
      "\n",
      "As with binary logistic regression, we cannot minimize [math_placeholder | 145] in closed form, so we need to rely on iterative optimization. For gradient descent, one can show (although it is a bit clumsy) that the gradient of the error function is equal to:\n",
      "\n",
      "[math_placeholder | 146]\n",
      "\n",
      "This is the gradient of the weights specifically for class [math_placeholder | 147]. The idea is that we can update the weights for each class separately. From this we can directly derive the stochastic gradient descent (we update the weights for each example, for each class). We can also derive the standard (batch) gradient descent, where we accumulate updates for all examples, for each class separately. We can also derive the Newton method (the Hessian matrix), but we will skip that.\n",
      "\n",
      "\n",
      "You may have noticed that the gradient of the loss we derived for multinomial logistic regression is actually the same as that for binary logistic regression: \"model's error times the example vector\". When we use this for gradient descent, we update the weights as follows:\n",
      "\n",
      "[math_placeholder | 148]\n",
      "\n",
      "We have already (in the context of the perceptron, if you remember) said this rule is called the Widrow-Hoff rule, while the alternative name is least-mean-squares (LMS) algorithm (not to be confused with least squares, although there is obviously a connection).\n",
      "\n",
      "The LMS algorithm, that is, this kind of learning where we use stochastic gradient descent to minimize the error by updating the model's weights in the way defined above, allows for online learning. We mentioned online learning the last time. Recall: online learning is a type of learning where not all learning examples need to be available up front, rather they can become available one after the other, and the model weights will be updated as new examples arrive.\n",
      "\n",
      "Let's go back a few weeks, to linear regression. When we introduced linear regression, we were actually talking only about batch optimization, which is achieved by calculating the pseudoinverse of the design matrix. However, already then we could have resorted to a stochastic (i.e., online) weight update. Namely, instead of searching analytically for the minimum of the error function, we can calculate the gradient of the error function, and then apply the gradient descent. (We didn't do that at that at that time, probably because we were blinded by the sheer elegance of the closed-form solution). Well, let's do it now. The quadratic error function of linear regression is:\n",
      "\n",
      "[math_placeholder | 149]\n",
      "\n",
      "The gradient (for one example [math_placeholder | 150] ) is:\n",
      "\n",
      "[math_placeholder | 151]\n",
      "\n",
      "So, the rule for updating the weights is:\n",
      "\n",
      "[math_placeholder | 152]\n",
      "\n",
      "which, again, is the LMS rule! So, there seems to be some underlying principle at play here all three regression algorithms use the same rule (LMS) for online learning (i.e., for stochastic gradient descent). How can that be? Well, this is because all these models - linear regression, logistic regression, and multinomial regression - belong to the generalized linear models family.\n",
      "\n",
      "Now it's time to round out the story and give a unified perspective on the three algorithms.\n",
      "\n",
      "\n",
      "For starters, let's recall from last lecture that generalized linear models are models that wrap the scalar product of the weight vector and example vector into an activation function [math_placeholder | 153]. So:\n",
      "\n",
      "[math_placeholder | 154]\n",
      "\n",
      "Let's look at three generalized linear models we've considered so far. For each of them, let's consider four points: (1) how the model is defined, (2) what is the probability distribution to which their output corresponds, (3) how is the loss is defined, and (4) what is the gradient of the loss, which we need for gradient descent.\n",
      "\n",
      "First, let's look at the linear regression algorithm:\n",
      "\n",
      "[math_placeholder | 155]\n",
      "\n",
      "For batch learning we use the least squares method (the pseudoinverse), and for online learning we use the LMS rule.\n",
      "\n",
      "Let's look at the logistic regression algorithm:\n",
      "\n",
      "[math_placeholder | 156]\n",
      "\n",
      "For batch learning we use gradient descent, the Newton's (IRLS) or the quasi-Newton method (BFSG, L-BFSG). We use the LMS rule for online learning.\n",
      "\n",
      "Finally, let's look at the multinomial logistic regression:\n",
      "\n",
      "[math_placeholder | 157]\n",
      "\n",
      "We use the same (mutatis mutandis) optimization procedures for model learning as for logistic regression.\n",
      "\n",
      "Notice the commonalities. For all three algorithms, we derived the loss function from the negative logarithm of the probability of labels of the examples from the dataset. We did this by using the normal, Bernoulli, and multinoulli distribution distribution for linear, binary logistic, and multinoulli logistic regression, respectively. Furthermore, for all three algorithms we derived an identical rule (the LMS) for online weights update.\n",
      "\n",
      "The question is: how come we always get the same weights update rule? Also, what is the relationship between the logistic function and the Bernoulli variable, and between the softmax function and the multinoulli distribution? There seems to be a connection, because in both cases we obtained the cross-entropy error. The answer lies in the properties of the distributions we used to model the labels [math_placeholder | 158].\n",
      "\n",
      "\n",
      "The distributions we have encountered so far (Gaussian, Bernoulli, multinoulli), but also some others that are often used in machine learning (binomial, multinomial, Student's t-distribution, uniform, beta distribution, gamma distribution, Dirichlet's) belong to the so-called exponential family (hrv. eksponencijalna familija). What is an exponential family? The exponential family is a broad group of distributions that can be written in the following form:\n",
      "\n",
      "[math_placeholder | 159]\n",
      "\n",
      "The exponential family distributions have many properties that are important for machine learning, but mostly for probabilistic approaches to machine learning, so we won't go into further into that here.\n",
      "\n",
      "What is of interesting to us here is that it is that the exponential family is crucial for for generalized linear models. Specifically, for the distributions belonging to the exponential family (including Gaussian, Bernoulli, and multinoulli) there is a relationship between the distribution and its (possibly nonlinear) activation function [math_placeholder | 160]. This function is in this context is referred to as the mean function (hrv. funkcija sredine), because it defines the [math_placeholder | 161] parameter of a distribution, i.e., the distribution's mean. Thus, the activation function [math_placeholder | 162] therefore [math_placeholder | 163] as a function of [math_placeholder | 164]. You probably already guessed that for the Gaussian distribution the activation function is an identity function, since [math_placeholder | 165]. For the Bernoulli's distribution it is the logistic function, and for the multinoulli distribution it is the softmax function.\n",
      "\n",
      "After this inspiring topic, let's take a look at another no less inspiring thing ...\n",
      "\n",
      "\n",
      "In generalized linear models (for both regression and classification) we had the option to map examples to the feature space using a feature mapping function:\n",
      "\n",
      "[math_placeholder | 166]\n",
      "\n",
      "where [math_placeholder | 167] is a set of [math_placeholder | 168] basis functions (nonlinear functions of the input variables): [math_placeholder | 169]. For example, polynomial mapping for [math_placeholder | 170] and [math_placeholder | 171] :\n",
      "\n",
      "[math_placeholder | 172]\n",
      "\n",
      "We then easily incorporated such a mapping into any generalized linear model:\n",
      "\n",
      "[math_placeholder | 173]\n",
      "\n",
      "where [math_placeholder | 174] is a chosen activation function (i.e., the mean function, to make use of the term we just introduced).\n",
      "\n",
      "Although we haven't tried it, the basis functions [math_placeholder | 175] need not necessarily be potencies or factors of the input features, but rather these can really be any functions. One interesting possibility is to use functions that measure the similarity of an example with some prototype\n",
      "examples in the input space. This then is called a kernel machine, and we'll talk about that in two weeks.\n",
      "\n",
      "At any rate, the limiting factor is that these are fixed basis functions: their number and shape is predetermined. This is a problem because in most cases we do not know in advance which basis functions are good for our problem. In other words, we generally don't know which feature mapping will make our problem linearly separable in feature space.\n",
      "\n",
      "We can solve this problem by letting the basis functions adapt to our data (examples from the training set). Here we have two options: the first, used by the aforementioned kernel machines, is to select some examples from the training set as prototypes, and then make the basis functions measure the similarity between an input example and these prototypes. This adjusts the total number of basis functions depending on the data. Another possibility is to use a fixed number of basis functions, but let each of them adapt to the data. Let's look into that in a bit more detail.\n",
      "\n",
      "The idea of adaptive basis functions is to define them up to some parameters, which we can then adjust to the data. That is, we will define parameterized basis functions. Does that sound familiar? Of course. It is exactly the same as training machine learning models: we define a function up to some parameters, and the parameters are determined by optimizing the empirical error on the training set. But let's look first at how one could go about parameterizing the basis functions. One possibility is to say that each basis function is a small generalized linear model on its own! So, we will have a generalized linear model and within it we'll have generalized linear models as its basis functions:\n",
      "\n",
      "[math_placeholder | 176]\n",
      "\n",
      "Note that each basis function in our model should have its own weight vector, so weights [math_placeholder | 177] in the inner sum have two indices: [math_placeholder | 178] is the basis function index, while [math_placeholder | 179] is the index of the weight in the weight vector of basis function [math_placeholder | 180]. With superscripts [math_placeholder | 181] and [math_placeholder | 182] we indicated which weights are used first in the calculation of model prediction, and which are used second. The expression on the right is just a matrix notation of the model, where we managed to get rid of the sums and we combined the weights into a weight vector and weight matrix. Take some time to convince yourself that this notation is the same as the one with sums.\n",
      "\n",
      "Now that we have metabolized this, it's time for a big revelation. What kind of model is this actually? For each basis function there are weights from the matrix [math_placeholder | 183], which we multiply with all the input features and add them up. We then multiply these values again by weights [math_placeholder | 184] and add them up. We've built something most of you already know: a neural network!\n",
      "\n",
      "\n",
      "[image_placeholder | 5]\n",
      "\n",
      "\n",
      "This network of ours is two-layered, however nothing prevents us from going deeper: we can make the basis functions be combinations of other basis functions, and those again be combinations of yet another basis functions, etc.\n",
      "\n",
      "Obviously, neural networks are a more complex model than generalized linear models. This, of course, comes at a price: a more complex optimization procedure (due to non-convexity of\n",
      "the error function, since the loss in the output layer - which can still be the quadratic loss or cross entropy loss - now has a very complex dependence on the weights of the previous layers) and a greater possibility of model overfitting. Of course, various solutions have been proposed to tackle these issues, in particular within the now popular deep learning paradigm. We won't go any further, however, as this topic is receiving a rather comprehensive treatment in other courses.\n",
      "\n",
      "What matters here is to be aware of the connection: a neural network is an extension of a generalized linear model in which the basis functions are adaptive, i.e., the feature mapping function is also learned from the data.\n",
      "\n",
      "\n",
      "\n",
      "  Newton's method is a second-order optimization method that converges faster than the gradient descent, and is based on the calculation of the Hessian matrix. The logistic regression variant is called IRLS\n",
      "  The calculation of the Hessian matrix is expensive to compute in both time and space, so we may resort to a quasi-Newton method, such as L-BSFG\n",
      "  Multinomial logistic regression is a generalization of logistic regression to more than two classes, with softmax function as the activation function\n",
      "  Common to generalized linear models is that their outputs are variables from the exponential family distributions\n",
      "  Instead of using fixed basis function, we can use parameterized adaptive basis functions, which brings us to neural networks\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff65fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math_rag.infrastructure.utils import TemplateChunkerUtil\n",
    "\n",
    "\n",
    "max_window_size = 1000\n",
    "\n",
    "chunks = TemplateChunkerUtil.chunk(template, max_window_size)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in chunks:\n",
    "    print(x)\n",
    "    print()\n",
    "    print('-------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14717542",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATH_PLACEHOLDER_TEMPLATE = '[math_placeholder | {index}]'\n",
    "\n",
    "\n",
    "def inject_latex(chunk: str, placeholder_index_to_latex: dict[int, str]) -> str:\n",
    "    \"\"\"\n",
    "    Replace each [math_placeholder | i] in the chunk with [<latex> | i],\n",
    "    preserving the index.\n",
    "    \"\"\"\n",
    "    for index, latex in placeholder_index_to_latex.items():\n",
    "        original = MATH_PLACEHOLDER_TEMPLATE.format(index=index)\n",
    "        replacement = f'[{latex} | {index}]'\n",
    "        chunk = chunk.replace(original, replacement)\n",
    "\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf0e62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_index_to_latex = {i: math_node.latex for i, math_node in enumerate(math_nodes_)}\n",
    "formatted_chunks = [inject_latex(chunk, placeholder_index_to_latex) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "823f64fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[$$\n",
      "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathrm{x})\n",
      "$$ | 2]\n",
      "\n",
      "If we introduce an index for the iterations, then we can write this as an equation:\n",
      "\n",
      "[$$\n",
      "\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta \\nabla f\\left(\\mathbf{x}_{t}\\right)\n",
      "$$ | 3]\n",
      "\n",
      "The idea with Newton's method is to take the point [$\\mathbf{x}_{t}$ | 4] (the current minimum) and compute at it the quadratic approximation of the function [$f(\\mathrm{x})$ | 5], and then move to the minimizer of this quadratic approximation (which is known analytically). If [$f$ | 6] is a function of one variable, this would look like this:\n",
      "\n",
      "\n",
      "[image_placeholder | 1]\n",
      "\n",
      "\n",
      "The black curve is the function [$f(x)$ | 7] that we minimize. We start from point [$x_{0}$ | 8]. At this point we do a quadratic approximation of the function [$f$ | 9], thus obtaining a parabola that is tangential to the function [$f$ | 10] at the point [$x_{0}$ | 11]. The search then moves to a point that minimizes the quadratic approximation of [$f$ | 12]\n"
     ]
    }
   ],
   "source": [
    "print(formatted_chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make a \"preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e94979",
   "metadata": {},
   "source": [
    "## Mathpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a156903",
   "metadata": {},
   "outputs": [],
   "source": [
    "mathpix_client = infrastructure_container.mathpix_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb4ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mathpix_client.convert_image(url='https://iili.io/FKvvD0J.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "pdf_path = Path('../.tmp/test_formulas.pdf')\n",
    "mathpix_client.convert_pdf(file_path=Path('../.tmp/test_formulas.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e6121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = Path('../.tmp/mathpix/downloads/data.zip')\n",
    "target_path = Path('../.tmp/mathpix/data.zip')\n",
    "\n",
    "from math_rag.shared.utils import ZipExtractorUtil\n",
    "\n",
    "\n",
    "ZipExtractorUtil.extract(zip_path, target_path)\n",
    "\n",
    "# TODO find .tex file in extracted dir, convert to katex, save to minio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
