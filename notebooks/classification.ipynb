{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "\n",
    "RESET = False\n",
    "\n",
    "infrastructure_container = InfrastructureContainer()\n",
    "infrastructure_container.init_resources()\n",
    "\n",
    "math_article_seeder = infrastructure_container.math_article_seeder()\n",
    "math_expression_seeder = infrastructure_container.math_expression_seeder()\n",
    "math_expression_classification_seeder = (\n",
    "    infrastructure_container.math_expression_classification_seeder()\n",
    ")\n",
    "math_article_seeder.seed(reset=RESET)\n",
    "await math_expression_seeder.seed(reset=RESET)\n",
    "await math_expression_classification_seeder.seed(reset=RESET)\n",
    "\n",
    "math_expression_repository = infrastructure_container.math_expression_repository()\n",
    "math_expression_classification_repository = (\n",
    "    infrastructure_container.math_expression_classification_repository()\n",
    ")\n",
    "llm = infrastructure_container.openai_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - description for each class\n",
    "# - how to determine classes?\n",
    "# - do names need to take a single token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expressions = await math_expression_repository.get_math_expressions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 20:10:04,906 - INFO - HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 20:10:06,049 - INFO - HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 20:10:06,051 - INFO - Batch batch_67c9f30de7ac8190a781a743f42395c2 created with status validating\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'batch_67c9f30de7ac8190a781a743f42395c2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math_rag.application.models.inference import (\n",
    "    LLMConversation,\n",
    "    LLMMessage,\n",
    "    LLMParams,\n",
    "    LLMRequest,\n",
    "    LLMRequestBatch,\n",
    "    LLMResponse,\n",
    ")\n",
    "\n",
    "\n",
    "response_type = str\n",
    "prompts = ['what is 2 + 2', 'what is 2 + 3']\n",
    "\n",
    "request_batch = LLMRequestBatch(\n",
    "    requests=[\n",
    "        LLMRequest(\n",
    "            conversation=LLMConversation(\n",
    "                messages=[LLMMessage(role='user', content=prompt)]\n",
    "            ),\n",
    "            params=LLMParams[response_type](\n",
    "                model='gpt-4o-mini', temperature=0.0, response_type=response_type\n",
    "            ),\n",
    "        )\n",
    "        for prompt in prompts\n",
    "    ]\n",
    ")\n",
    "batch_id = await llm.batch_generate_init(request_batch)\n",
    "batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 20:12:03,931 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_67c9f30de7ac8190a781a743f42395c2 \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 20:12:03,932 - INFO - Batch batch_67c9f30de7ac8190a781a743f42395c2 status updated to validating\n"
     ]
    }
   ],
   "source": [
    "result = await llm.batch_generate_result(batch_id, response_type)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 22:33:14,496 - INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_67c9f30de7ac8190a781a743f42395c2 \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 22:33:14,498 - INFO - Batch batch_67c9f30de7ac8190a781a743f42395c2 status completed\n",
      "2025-03-06 22:33:14,967 - INFO - HTTP Request: GET https://api.openai.com/v1/files/file-1gUSmeCsE19eAXMD5s6nCC/content \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 22:33:15,303 - INFO - HTTP Request: GET https://api.openai.com/v1/files/file-16wwEDXWSeiBETu5n7AiGf/content \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 22:33:15,731 - INFO - HTTP Request: DELETE https://api.openai.com/v1/files/file-16wwEDXWSeiBETu5n7AiGf \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 22:33:16,286 - INFO - HTTP Request: DELETE https://api.openai.com/v1/files/file-1gUSmeCsE19eAXMD5s6nCC \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResponseBatch[str](request_batch=LLMRequestBatch[str](requests=[]), responses=[LLMResponse[str](content='2 + 2 equals 4.'), LLMResponse[str](content='2 + 3 equals 5.')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await llm.batch_generate_result('batch_67c9f30de7ac8190a781a743f42395c2', str)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 equals 4.\n",
      "2 + 3 equals 5.\n"
     ]
    }
   ],
   "source": [
    "for item in result.responses:\n",
    "    print(item.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pydantic._internal._model_construction.ModelMetaclass\n",
      "math_rag.application.models.inference.llm_conversation.LLMConversation\n"
     ]
    }
   ],
   "source": [
    "from math_rag.application.models.inference import LLMConversation\n",
    "\n",
    "\n",
    "print(f'{LLMConversation.__class__.__module__}.{LLMConversation.__class__.__name__}')\n",
    "print(f'{LLMConversation.__module__}.{LLMConversation.__qualname__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math_rag.application.models.inference.llm_conversation.LLMConversation\n"
     ]
    }
   ],
   "source": [
    "response_type = LLMConversation\n",
    "\n",
    "print(f'{response_type.__module__}.{response_type.__qualname__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generic\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from math_rag.application.types.inference import LLMResponseType\n",
    "\n",
    "\n",
    "class SomeRequest(BaseModel, Generic[LLMResponseType]):\n",
    "    response_type: type[LLMResponseType]\n",
    "\n",
    "\n",
    "class MyResponse(BaseModel):\n",
    "    name: str\n",
    "\n",
    "\n",
    "class SomeResponse(SomeRequest[MyResponse]):\n",
    "    pass\n",
    "\n",
    "\n",
    "some_request = SomeRequest(response_type=MyResponse)\n",
    "\n",
    "some_dict = some_request.model_dump()\n",
    "\n",
    "some_request_validated = SomeRequest.model_validate(some_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'math_rag.application.models.assistants.kc_assistant_output.KCAssistantOutput'>\n"
     ]
    }
   ],
   "source": [
    "from math_rag.application.models.assistants import KCAssistantOutput\n",
    "from math_rag.application.models.inference import LLMParams\n",
    "\n",
    "\n",
    "output = KCAssistantOutput(katex='some katex')\n",
    "\n",
    "params = LLMParams(model='some model', temperature=0.0, response_type=KCAssistantOutput)\n",
    "print(params.response_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.infrastructure.mappings.documents import LLMParamsMapping\n",
    "\n",
    "\n",
    "document_params = LLMParamsMapping[KCAssistantOutput].to_target(params)\n",
    "bson_dict = document_params.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=UUID('2fb3641b-09b1-48b7-8f78-3e23de2eee83') model='some model' temperature=0.0 top_logprobs=None reasoning_effort=None max_completion_tokens=None response_type='math_rag.application.models.assistants.KCAssistantOutput' metadata=None n=1\n"
     ]
    }
   ],
   "source": [
    "from math_rag.infrastructure.models.documents import LLMParamsDocument\n",
    "\n",
    "\n",
    "original_document_params = LLMParamsDocument.model_validate(bson_dict)\n",
    "original_params = LLMParamsMapping[KCAssistantOutput].to_source(\n",
    "    original_document_params\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math-rag-QVTscwFS-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
