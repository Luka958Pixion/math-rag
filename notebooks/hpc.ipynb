{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from huggingface_hub import AsyncInferenceClient\n",
    "\n",
    "\n",
    "HUGGINGFACE_TOKEN = config('HUGGINGFACE_TOKEN')\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "client = AsyncInferenceClient(\n",
    "    model=MODEL_HUB_ID,\n",
    "    provider='hf-inference',\n",
    "    timeout=None,\n",
    "    api_key=HUGGINGFACE_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.base.assistants import BaseAssistantInput, BaseAssistantOutput\n",
    "\n",
    "\n",
    "class SomeInput(BaseAssistantInput):\n",
    "    pass\n",
    "\n",
    "\n",
    "class SomeOutput(BaseAssistantOutput):\n",
    "    result: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from math_rag.application.models.inference import (\n",
    "    LLMBatchRequest,\n",
    "    LLMConversation,\n",
    "    LLMMessage,\n",
    "    LLMParams,\n",
    "    LLMRequest,\n",
    ")\n",
    "from math_rag.infrastructure.mappings.inference.huggingface import (\n",
    "    LLMRequestMapping,\n",
    "    LLMResponseListMapping,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "some_input = SomeInput()\n",
    "\n",
    "_requests = [\n",
    "    LLMRequest(\n",
    "        conversation=LLMConversation(\n",
    "            messages=[\n",
    "                LLMMessage(role='system', content='You are a helpful assistant.'),\n",
    "                LLMMessage(role='user', content=f'what is {i}+2'),\n",
    "            ]\n",
    "        ),\n",
    "        params=LLMParams(\n",
    "            model=MODEL_HUB_ID,\n",
    "            temperature=0,\n",
    "            response_type=SomeOutput,\n",
    "            max_completion_tokens=10,\n",
    "            metadata={'input_id': str(some_input.id)},\n",
    "        ),\n",
    "    )\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "batch_request: LLMBatchRequest = LLMBatchRequest(requests=_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [LLMRequestMapping.to_target(request) for request in batch_request.requests]\n",
    "lines = [json.dumps(request, separators=(',', ':')) for request in requests]\n",
    "jsonl_str = '\\n'.join(lines)\n",
    "jsonl_bytes = jsonl_str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_dict = json.loads(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='{ \"result\": 4 }', tool_call_id=None, tool_calls=None), logprobs=None)], created=1744125157, id='', model='microsoft/Phi-3-mini-4k-instruct', system_fingerprint='3.2.1-native', usage=ChatCompletionOutputUsage(completion_tokens=9, prompt_tokens=17, total_tokens=26), object='chat.completion')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await client.chat_completion(**request_dict)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResponseList(id=UUID('f7fd3b29-2772-468d-831f-b160bf1dfaa0'), request_id=UUID('cb02d5a3-bed8-4109-b06c-04d585081ef3'), responses=[LLMResponse(id=UUID('3976268b-adbf-494a-b88b-fda24781d477'), content=BoundAssistantOutput(id=UUID('bd7fdedd-f32a-4d7f-845d-0a21413828b6'), input_id=UUID('c882baa3-2047-4f65-b8df-bd176bba3b2c'), result=4), logprobs=None)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_list = LLMResponseListMapping.to_source(\n",
    "    result,\n",
    "    request_id=request.id,\n",
    "    input_id=request_dict['extra_body']['input_id'],\n",
    "    response_type=SomeOutput,\n",
    ")\n",
    "response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = Path(f'.tmp/input_{batch_request.id}.jsonl')\n",
    "\n",
    "with open(input_file_path, 'w') as input_file:\n",
    "    for line in lines:\n",
    "        input_file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.inference import (\n",
    "    EMBatchRequest,\n",
    "    EMParams,\n",
    "    EMRequest,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_HUB_ID = 'BAAI/bge-large-en-v1.5'\n",
    "\n",
    "_em_requests = [\n",
    "    EMRequest(\n",
    "        text=f'hello world {i}',\n",
    "        params=EMParams(model=MODEL_HUB_ID, dimensions=1024),\n",
    "    )\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "em_batch_request: EMBatchRequest = EMBatchRequest(requests=_em_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apptainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "\n",
    "infrastructure_container = InfrastructureContainer()\n",
    "infrastructure_container.init_resources()\n",
    "\n",
    "tgi_batch_llm = infrastructure_container.tgi_batch_llm()\n",
    "tei_batch_em = infrastructure_container.tei_batch_em()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await tgi_batch_llm.init_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await tgi_batch_llm.batch_generate(\n",
    "    batch_request=batch_request,\n",
    "    response_type=SomeOutput,\n",
    "    poll_interval=3 * 60,\n",
    "    max_tokens_per_day=None,\n",
    "    max_num_retries=0,\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await tei_batch_em.init_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await tei_batch_em.batch_embed(\n",
    "    batch_request=em_batch_request,\n",
    "    poll_interval=3 * 60,\n",
    "    max_tokens_per_day=None,\n",
    "    max_num_retries=0,\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "from decouple import config\n",
    "\n",
    "\n",
    "environ['PBS_O_WORKDIR'] = '../.tmp'\n",
    "environ['TGI_API_KEY'] = config('HUGGINGFACE_TOKEN')\n",
    "environ['MODEL_HUB_ID'] = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "%run ../assets/hpc/hf/tgi/tgi_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 12:36:43,783 - INFO - Host canonicalization disabled\n",
      "2025-05-10 12:36:43,783 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Thread(Thread-5 (runner), started 281472901378432)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 12:36:43,795 - INFO - [conn=6] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:43,796 - INFO - [conn=6]   Local address: 172.18.0.4, port 48184\n",
      "2025-05-10 12:36:43,796 - INFO - [conn=6]   Peer address: 161.53.2.37, port 22\n",
      "2025-05-10 12:36:43,823 - INFO - [conn=6] Beginning auth for user lpanic\n",
      "2025-05-10 12:36:44,095 - INFO - [conn=6] Auth for user lpanic succeeded\n",
      "2025-05-10 12:36:44,098 - INFO - [conn=6, chan=0] Requesting new SSH session\n",
      "2025-05-10 12:36:44,140 - INFO - [conn=6, chan=0]   Command: find tei_default_root -name \"snapshot_*.json\" -maxdepth 1 -print -quit\n",
      "2025-05-10 12:36:44,283 - INFO - [conn=6, chan=0] Received exit status 0\n",
      "2025-05-10 12:36:44,284 - INFO - [conn=6, chan=0] Received channel close\n",
      "2025-05-10 12:36:44,284 - INFO - [conn=6, chan=0] Channel closed\n",
      "2025-05-10 12:36:44,284 - INFO - Command `find tei_default_root -name \"snapshot_*.json\" -maxdepth 1 -print -quit` in `run` returned stdout: \n",
      "2025-05-10 12:36:44,285 - INFO - [conn=6] Closing connection\n",
      "2025-05-10 12:36:44,285 - INFO - [conn=6] Sending disconnect: Disconnected by application (11)\n",
      "2025-05-10 12:36:44,286 - INFO - [conn=6] Connection closed\n",
      "2025-05-10 12:36:44,287 - INFO - Host canonicalization disabled\n",
      "2025-05-10 12:36:44,287 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:44,294 - INFO - [conn=7] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:44,295 - INFO - [conn=7]   Local address: 172.18.0.4, port 48186\n",
      "2025-05-10 12:36:44,295 - INFO - [conn=7]   Peer address: 161.53.2.37, port 22\n",
      "2025-05-10 12:36:44,319 - INFO - [conn=7] Beginning auth for user lpanic\n",
      "2025-05-10 12:36:44,582 - INFO - [conn=7] Auth for user lpanic succeeded\n",
      "2025-05-10 12:36:44,585 - INFO - [conn=7, chan=0] Requesting new SSH session\n",
      "2025-05-10 12:36:44,628 - INFO - [conn=7, chan=0]   Command: find tgi_default_root -name \"snapshot_*.json\" -maxdepth 1 -print -quit\n",
      "2025-05-10 12:36:44,771 - INFO - [conn=7, chan=0] Received exit status 0\n",
      "2025-05-10 12:36:44,773 - INFO - [conn=7, chan=0] Received channel close\n",
      "2025-05-10 12:36:44,775 - INFO - [conn=7, chan=0] Channel closed\n",
      "2025-05-10 12:36:44,776 - INFO - Command `find tgi_default_root -name \"snapshot_*.json\" -maxdepth 1 -print -quit` in `run` returned stdout: tgi_default_root/snapshot_487724.x3000c0s25b0n0.hsn.hpc.srce.hr.json\n",
      "2025-05-10 12:36:44,777 - INFO - [conn=7] Closing connection\n",
      "2025-05-10 12:36:44,779 - INFO - [conn=7] Sending disconnect: Disconnected by application (11)\n",
      "2025-05-10 12:36:44,780 - INFO - [conn=7] Connection closed\n",
      "2025-05-10 12:36:44,782 - INFO - Host canonicalization disabled\n",
      "2025-05-10 12:36:44,783 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:44,795 - INFO - [conn=8] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:44,795 - INFO - [conn=8]   Local address: 172.18.0.4, port 48198\n",
      "2025-05-10 12:36:44,796 - INFO - [conn=8]   Peer address: 161.53.2.37, port 22\n",
      "2025-05-10 12:36:44,823 - INFO - [conn=8] Beginning auth for user lpanic\n",
      "2025-05-10 12:36:45,089 - INFO - [conn=8] Auth for user lpanic succeeded\n",
      "2025-05-10 12:36:45,090 - INFO - [conn=8, chan=0] Requesting new SSH session\n",
      "2025-05-10 12:36:45,137 - INFO - [conn=8, chan=0]   Command: test -f tgi_default_root/snapshot_487724.x3000c0s25b0n0.hsn.hpc.srce.hr.json && echo \"true\" || echo \"false\"\n",
      "2025-05-10 12:36:45,282 - INFO - [conn=8, chan=0] Received exit status 0\n",
      "2025-05-10 12:36:45,284 - INFO - [conn=8, chan=0] Received channel close\n",
      "2025-05-10 12:36:45,285 - INFO - [conn=8, chan=0] Channel closed\n",
      "2025-05-10 12:36:45,286 - INFO - Command `test -f tgi_default_root/snapshot_487724.x3000c0s25b0n0.hsn.hpc.srce.hr.json && echo \"true\" || echo \"false\"` in `run` returned stdout: true\n",
      "2025-05-10 12:36:45,286 - INFO - [conn=8] Closing connection\n",
      "2025-05-10 12:36:45,287 - INFO - [conn=8] Sending disconnect: Disconnected by application (11)\n",
      "2025-05-10 12:36:45,288 - INFO - [conn=8] Connection closed\n",
      "2025-05-10 12:36:45,290 - INFO - Host canonicalization disabled\n",
      "2025-05-10 12:36:45,291 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:45,302 - INFO - [conn=9] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:45,302 - INFO - [conn=9]   Local address: 172.18.0.4, port 48212\n",
      "2025-05-10 12:36:45,303 - INFO - [conn=9]   Peer address: 161.53.2.37, port 22\n",
      "2025-05-10 12:36:45,326 - INFO - [conn=9] Beginning auth for user lpanic\n",
      "2025-05-10 12:36:45,594 - INFO - [conn=9] Auth for user lpanic succeeded\n",
      "2025-05-10 12:36:45,595 - INFO - [conn=9, chan=0] Requesting new SSH session\n",
      "2025-05-10 12:36:45,638 - INFO - [conn=9, chan=0]   Subsystem: sftp\n",
      "2025-05-10 12:36:45,644 - INFO - [conn=9, chan=0] Starting SFTP client\n",
      "2025-05-10 12:36:45,862 - INFO - [conn=9, chan=0] SFTP client exited\n",
      "2025-05-10 12:36:45,862 - INFO - [conn=9, chan=0] Closing channel\n",
      "2025-05-10 12:36:45,863 - INFO - [conn=9, chan=0] Received exit status 0\n",
      "2025-05-10 12:36:45,863 - INFO - [conn=9, chan=0] Received channel close\n",
      "2025-05-10 12:36:45,863 - INFO - [conn=9, chan=0] Channel closed\n",
      "2025-05-10 12:36:45,864 - INFO - [conn=9] Closing connection\n",
      "2025-05-10 12:36:45,864 - INFO - [conn=9] Sending disconnect: Disconnected by application (11)\n",
      "2025-05-10 12:36:45,864 - INFO - [conn=9] Connection closed\n",
      "2025-05-10 12:36:45,866 - INFO - Host canonicalization disabled\n",
      "2025-05-10 12:36:45,866 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:45,875 - INFO - [conn=10] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:45,875 - INFO - [conn=10]   Local address: 172.18.0.4, port 48226\n",
      "2025-05-10 12:36:45,876 - INFO - [conn=10]   Peer address: 161.53.2.37, port 22\n",
      "2025-05-10 12:36:45,898 - INFO - [conn=10] Beginning auth for user lpanic\n",
      "2025-05-10 12:36:46,160 - INFO - [conn=10] Auth for user lpanic succeeded\n",
      "2025-05-10 12:36:46,162 - INFO - [conn=10, chan=0] Requesting new SSH session\n",
      "2025-05-10 12:36:46,204 - INFO - [conn=10, chan=0]   Command: tar -cvf tgi_default_root/snapshot_20250510T122826Z-17a07b35403dd62c.tar -C tgi_default_root/data/snapshots/20250510T122826Z-17a07b35403dd62c .\n",
      "2025-05-10 12:36:46,349 - INFO - [conn=10, chan=0] Received exit status 0\n",
      "2025-05-10 12:36:46,351 - INFO - [conn=10, chan=0] Received channel close\n",
      "2025-05-10 12:36:46,352 - INFO - [conn=10, chan=0] Channel closed\n",
      "2025-05-10 12:36:46,353 - INFO - Command `tar -cvf tgi_default_root/snapshot_20250510T122826Z-17a07b35403dd62c.tar -C tgi_default_root/data/snapshots/20250510T122826Z-17a07b35403dd62c .` in `run` returned stdout: ./\n",
      "./01JTX2H6WK23MEVZDWE7R8ZM7J/\n",
      "./01JTX2H6WK23MEVZDWE7R8ZM7J/index\n",
      "./01JTX2H6WK23MEVZDWE7R8ZM7J/meta.json\n",
      "./01JTX2H6WK23MEVZDWE7R8ZM7J/chunks/\n",
      "./01JTX2H6WK23MEVZDWE7R8ZM7J/chunks/000001\n",
      "./01JTX2H6WK23MEVZDWE7R8ZM7J/tombstones\n",
      "./01JTX2P40Z10PM2A05V7N1320E/\n",
      "./01JTX2P40Z10PM2A05V7N1320E/index\n",
      "./01JTX2P40Z10PM2A05V7N1320E/meta.json\n",
      "./01JTX2P40Z10PM2A05V7N1320E/chunks/\n",
      "./01JTX2P40Z10PM2A05V7N1320E/chunks/000001\n",
      "./01JTX2P40Z10PM2A05V7N1320E/tombstones\n",
      "./01JTV38GTJTXBFZK454F2K8PT5/\n",
      "./01JTV38GTJTXBFZK454F2K8PT5/index\n",
      "./01JTV38GTJTXBFZK454F2K8PT5/meta.json\n",
      "./01JTV38GTJTXBFZK454F2K8PT5/chunks/\n",
      "./01JTV38GTJTXBFZK454F2K8PT5/chunks/000001\n",
      "./01JTV38GTJTXBFZK454F2K8PT5/tombstones\n",
      "2025-05-10 12:36:46,353 - INFO - [conn=10] Closing connection\n",
      "2025-05-10 12:36:46,354 - INFO - [conn=10] Sending disconnect: Disconnected by application (11)\n",
      "2025-05-10 12:36:46,355 - INFO - [conn=10] Connection closed\n",
      "2025-05-10 12:36:46,357 - INFO - Host canonicalization disabled\n",
      "2025-05-10 12:36:46,358 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:46,368 - INFO - [conn=11] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:46,369 - INFO - [conn=11]   Local address: 172.18.0.4, port 48236\n",
      "2025-05-10 12:36:46,369 - INFO - [conn=11]   Peer address: 161.53.2.37, port 22\n",
      "2025-05-10 12:36:46,487 - INFO - [conn=11] Beginning auth for user lpanic\n",
      "2025-05-10 12:36:46,749 - INFO - [conn=11] Auth for user lpanic succeeded\n",
      "2025-05-10 12:36:46,751 - INFO - [conn=11, chan=0] Requesting new SSH session\n",
      "2025-05-10 12:36:46,794 - INFO - [conn=11, chan=0]   Subsystem: sftp\n",
      "2025-05-10 12:36:46,800 - INFO - [conn=11, chan=0] Starting SFTP client\n",
      "2025-05-10 12:36:47,135 - INFO - [conn=11, chan=0] Received exit status 0\n",
      "2025-05-10 12:36:47,135 - INFO - [conn=11, chan=0] Received channel close\n",
      "2025-05-10 12:36:47,136 - INFO - [conn=11, chan=0] SFTP client exited\n",
      "2025-05-10 12:36:47,136 - INFO - [conn=11, chan=0] Closing channel\n",
      "2025-05-10 12:36:47,136 - INFO - [conn=11, chan=0] Channel closed\n",
      "2025-05-10 12:36:47,137 - INFO - [conn=11] Closing connection\n",
      "2025-05-10 12:36:47,137 - INFO - [conn=11] Sending disconnect: Disconnected by application (11)\n",
      "2025-05-10 12:36:47,137 - INFO - [conn=11] Connection closed\n",
      "2025-05-10 12:36:47,157 - INFO - Host canonicalization disabled\n",
      "2025-05-10 12:36:47,157 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:47,165 - INFO - [conn=12] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-05-10 12:36:47,165 - INFO - [conn=12]   Local address: 172.18.0.4, port 50386\n",
      "2025-05-10 12:36:47,166 - INFO - [conn=12]   Peer address: 161.53.2.37, port 22\n",
      "2025-05-10 12:36:47,194 - INFO - [conn=12] Beginning auth for user lpanic\n",
      "2025-05-10 12:36:47,460 - INFO - [conn=12] Auth for user lpanic succeeded\n",
      "2025-05-10 12:36:47,463 - INFO - [conn=12, chan=0] Requesting new SSH session\n",
      "2025-05-10 12:36:47,506 - INFO - [conn=12, chan=0]   Command: rm -f tgi_default_root/snapshot_487724.x3000c0s25b0n0.hsn.hpc.srce.hr.json tgi_default_root/snapshot_20250510T122826Z-17a07b35403dd62c.tar\n",
      "2025-05-10 12:36:47,661 - INFO - [conn=12, chan=0] Received exit status 0\n",
      "2025-05-10 12:36:47,662 - INFO - [conn=12, chan=0] Received channel close\n",
      "2025-05-10 12:36:47,663 - INFO - [conn=12, chan=0] Channel closed\n",
      "2025-05-10 12:36:47,664 - INFO - Command `rm -f tgi_default_root/snapshot_487724.x3000c0s25b0n0.hsn.hpc.srce.hr.json tgi_default_root/snapshot_20250510T122826Z-17a07b35403dd62c.tar` in `run` returned stdout: \n",
      "2025-05-10 12:36:47,664 - INFO - [conn=12] Closing connection\n",
      "2025-05-10 12:36:47,665 - INFO - [conn=12] Sending disconnect: Disconnected by application (11)\n",
      "2025-05-10 12:36:47,666 - INFO - [conn=12] Connection closed\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "\n",
    "\n",
    "# NOTE: prometheus_snapshot_loader_service.load will be run as a background task in fast api\n",
    "def run_async_in_thread(coro):\n",
    "    def runner():\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.run_until_complete(coro)\n",
    "        loop.close()\n",
    "\n",
    "    thread = threading.Thread(target=runner)\n",
    "    thread.start()\n",
    "\n",
    "    return thread\n",
    "\n",
    "\n",
    "prometheus_snapshot_loader_service = (\n",
    "    infrastructure_container.prometheus_snapshot_loader_service()\n",
    ")\n",
    "run_async_in_thread(prometheus_snapshot_loader_service.load())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
