{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from huggingface_hub import AsyncInferenceClient\n",
    "\n",
    "\n",
    "HUGGINGFACE_TOKEN = config('HUGGINGFACE_TOKEN')\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "client = AsyncInferenceClient(\n",
    "    model=MODEL_HUB_ID,\n",
    "    provider='hf-inference',\n",
    "    timeout=None,\n",
    "    api_key=HUGGINGFACE_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.base.assistants import BaseAssistantInput, BaseAssistantOutput\n",
    "\n",
    "\n",
    "class SomeInput(BaseAssistantInput):\n",
    "    pass\n",
    "\n",
    "\n",
    "class SomeOutput(BaseAssistantOutput):\n",
    "    result: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from math_rag.application.models.inference import (\n",
    "    LLMBatchRequest,\n",
    "    LLMBatchResult,\n",
    "    LLMConversation,\n",
    "    LLMMessage,\n",
    "    LLMParams,\n",
    "    LLMRequest,\n",
    ")\n",
    "from math_rag.infrastructure.mappings.inference.huggingface import (\n",
    "    LLMRequestMapping,\n",
    "    LLMResponseListMapping,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "some_input = SomeInput()\n",
    "\n",
    "request = LLMRequest(\n",
    "    conversation=LLMConversation(\n",
    "        messages=[\n",
    "            LLMMessage(role='system', content='You are a helpful assistant.'),\n",
    "            LLMMessage(role='user', content='what is 2+2'),\n",
    "        ]\n",
    "    ),\n",
    "    params=LLMParams(\n",
    "        model=MODEL_HUB_ID,\n",
    "        temperature=0,\n",
    "        response_type=SomeOutput,\n",
    "        max_completion_tokens=10,\n",
    "        metadata={'input_id': str(some_input.id)},\n",
    "    ),\n",
    ")\n",
    "\n",
    "batch_request: LLMBatchRequest = LLMBatchRequest(requests=[request])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [LLMRequestMapping.to_target(request) for request in batch_request.requests]\n",
    "lines = [json.dumps(request, separators=(',', ':')) for request in requests]\n",
    "jsonl_str = '\\n'.join(lines)\n",
    "jsonl_bytes = jsonl_str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_dict = json.loads(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='{ \"result\": 4 }', tool_call_id=None, tool_calls=None), logprobs=None)], created=1744125157, id='', model='microsoft/Phi-3-mini-4k-instruct', system_fingerprint='3.2.1-native', usage=ChatCompletionOutputUsage(completion_tokens=9, prompt_tokens=17, total_tokens=26), object='chat.completion')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await client.chat_completion(**request_dict)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResponseList(id=UUID('f7fd3b29-2772-468d-831f-b160bf1dfaa0'), request_id=UUID('cb02d5a3-bed8-4109-b06c-04d585081ef3'), responses=[LLMResponse(id=UUID('3976268b-adbf-494a-b88b-fda24781d477'), content=BoundAssistantOutput(id=UUID('bd7fdedd-f32a-4d7f-845d-0a21413828b6'), input_id=UUID('c882baa3-2047-4f65-b8df-bd176bba3b2c'), result=4), logprobs=None)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_list = LLMResponseListMapping.to_source(\n",
    "    result,\n",
    "    request_id=request.id,\n",
    "    input_id=request_dict['extra_body']['input_id'],\n",
    "    response_type=SomeOutput,\n",
    ")\n",
    "response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = Path(f'.tmp/input_{batch_request.id}.jsonl')\n",
    "\n",
    "with open(input_file_path, 'w') as input_file:\n",
    "    for line in lines:\n",
    "        input_file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apptainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "\n",
    "infrastructure_container = InfrastructureContainer()\n",
    "infrastructure_container.init_resources()\n",
    "\n",
    "apptainer_client = infrastructure_container.apptainer_client()\n",
    "sftp_client = infrastructure_container.sftp_client()\n",
    "pbs_pro_client = infrastructure_container.pbs_pro_client()\n",
    "hpc_client = infrastructure_container.hpc_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 18:55:25,155 - INFO - HTTP Request: GET http://host.docker.internal:7015/health \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await apptainer_client.health()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.infrastructure.inference.huggingface import HuggingFaceBatchLLM\n",
    "\n",
    "\n",
    "llm = HuggingFaceBatchLLM(\n",
    "    remote_project_root=Path('tgi_default_root'),\n",
    "    hpc_client=hpc_client,\n",
    "    pbs_pro_client=pbs_pro_client,\n",
    "    sftp_client=sftp_client,\n",
    "    apptainer_client=apptainer_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.init_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 18:55:30,488 - INFO - Host canonicalization disabled\n",
      "2025-04-08 18:55:30,489 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-08 18:55:32,352 - INFO - [conn=0] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-08 18:55:32,354 - INFO - [conn=0]   Local address: 172.22.0.9, port 45012\n",
      "2025-04-08 18:55:32,355 - INFO - [conn=0]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-08 18:55:32,418 - INFO - [conn=0] Beginning auth for user lpanic\n",
      "2025-04-08 18:55:33,460 - INFO - [conn=0] Auth for user lpanic succeeded\n",
      "2025-04-08 18:55:33,462 - INFO - [conn=0, chan=0] Requesting new SSH session\n",
      "2025-04-08 18:55:33,474 - INFO - [conn=0, chan=0]   Subsystem: sftp\n",
      "2025-04-08 18:55:33,480 - INFO - [conn=0, chan=0] Starting SFTP client\n",
      "2025-04-08 18:55:33,684 - INFO - [conn=0, chan=0] SFTP client exited\n",
      "2025-04-08 18:55:33,685 - INFO - [conn=0, chan=0] Closing channel\n",
      "2025-04-08 18:55:33,685 - INFO - [conn=0, chan=0] Received exit status 0\n",
      "2025-04-08 18:55:33,685 - INFO - [conn=0, chan=0] Received channel close\n",
      "2025-04-08 18:55:33,686 - INFO - [conn=0, chan=0] Channel closed\n",
      "2025-04-08 18:55:33,686 - INFO - [conn=0] Closing connection\n",
      "2025-04-08 18:55:33,686 - INFO - [conn=0] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-08 18:55:33,687 - INFO - [conn=0] Connection closed\n",
      "2025-04-08 18:55:33,688 - INFO - Host canonicalization disabled\n",
      "2025-04-08 18:55:33,688 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-08 18:55:33,751 - INFO - [conn=1] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-08 18:55:33,752 - INFO - [conn=1]   Local address: 172.22.0.9, port 45066\n",
      "2025-04-08 18:55:33,752 - INFO - [conn=1]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-08 18:55:35,181 - INFO - [conn=1] Beginning auth for user lpanic\n",
      "2025-04-08 18:55:35,842 - INFO - [conn=1] Auth for user lpanic succeeded\n",
      "2025-04-08 18:55:35,846 - INFO - [conn=1, chan=0] Requesting new SSH session\n",
      "2025-04-08 18:55:36,765 - INFO - [conn=1, chan=0]   Command: cd tgi_default_root && qsub -v MODEL_HUB_ID=microsoft/Phi-3-mini-4k-instruct -l select=1:ncpus=8:mem=34359738368B:ngpus=1,walltime=1:00:00 tgi.sh\n",
      "2025-04-08 18:55:37,838 - INFO - [conn=1, chan=0] Received exit status 0\n",
      "2025-04-08 18:55:37,839 - INFO - [conn=1, chan=0] Received channel close\n",
      "2025-04-08 18:55:37,840 - INFO - [conn=1, chan=0] Channel closed\n",
      "2025-04-08 18:55:37,841 - INFO - Command `cd tgi_default_root && qsub -v MODEL_HUB_ID=microsoft/Phi-3-mini-4k-instruct -l select=1:ncpus=8:mem=34359738368B:ngpus=1,walltime=1:00:00 tgi.sh` in `run` returned stdout: 472320.x3000c0s25b0n0.hsn.hpc.srce.hr\n",
      "2025-04-08 18:55:37,843 - INFO - [conn=1] Closing connection\n",
      "2025-04-08 18:55:37,844 - INFO - [conn=1] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-08 18:55:37,845 - INFO - [conn=1] Connection closed\n",
      "2025-04-08 18:55:37,846 - INFO - Host canonicalization disabled\n",
      "2025-04-08 18:55:37,846 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-08 18:55:37,866 - INFO - [conn=2] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-08 18:55:37,866 - INFO - [conn=2]   Local address: 172.22.0.9, port 45082\n",
      "2025-04-08 18:55:37,867 - INFO - [conn=2]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-08 18:55:37,973 - INFO - [conn=2] Beginning auth for user lpanic\n",
      "2025-04-08 18:55:38,297 - INFO - [conn=2] Auth for user lpanic succeeded\n",
      "2025-04-08 18:55:38,300 - INFO - [conn=2, chan=0] Requesting new SSH session\n",
      "2025-04-08 18:55:38,334 - INFO - [conn=2, chan=0]   Command: qstat -x 472320.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'\n",
      "2025-04-08 18:55:38,493 - INFO - [conn=2, chan=0] Received exit status 0\n",
      "2025-04-08 18:55:38,495 - INFO - [conn=2, chan=0] Received channel close\n",
      "2025-04-08 18:55:38,498 - INFO - [conn=2, chan=0] Channel closed\n",
      "2025-04-08 18:55:38,498 - INFO - Command `qstat -x 472320.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'` in `run` returned stdout: 472320.x3000c0s25b0n0 tgi lpanic 0 R gpu\n",
      "2025-04-08 18:55:38,498 - INFO - [conn=2] Closing connection\n",
      "2025-04-08 18:55:38,499 - INFO - [conn=2] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-08 18:55:38,499 - INFO - [conn=2] Connection closed\n",
      "2025-04-08 18:55:38,500 - INFO - Batch 472320.x3000c0s25b0n0.hsn.hpc.srce.hr created with state PBSProJobState.RUNNING\n",
      "2025-04-08 18:55:38,501 - INFO - Host canonicalization disabled\n",
      "2025-04-08 18:55:38,501 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-08 18:55:38,510 - INFO - [conn=3] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-08 18:55:38,511 - INFO - [conn=3]   Local address: 172.22.0.9, port 45090\n",
      "2025-04-08 18:55:38,511 - INFO - [conn=3]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-08 18:55:38,752 - INFO - [conn=3] Beginning auth for user lpanic\n",
      "2025-04-08 18:55:39,087 - INFO - [conn=3] Auth for user lpanic succeeded\n",
      "2025-04-08 18:55:39,089 - INFO - [conn=3, chan=0] Requesting new SSH session\n",
      "2025-04-08 18:55:39,131 - INFO - [conn=3, chan=0]   Command: qstat -x 472320.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'\n",
      "2025-04-08 18:55:39,335 - INFO - [conn=3, chan=0] Received exit status 0\n",
      "2025-04-08 18:55:39,337 - INFO - [conn=3, chan=0] Received channel close\n",
      "2025-04-08 18:55:39,340 - INFO - [conn=3, chan=0] Channel closed\n",
      "2025-04-08 18:55:39,341 - INFO - Command `qstat -x 472320.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'` in `run` returned stdout: 472320.x3000c0s25b0n0 tgi lpanic 0 R gpu\n",
      "2025-04-08 18:55:39,342 - INFO - [conn=3] Closing connection\n",
      "2025-04-08 18:55:39,343 - INFO - [conn=3] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-08 18:55:39,343 - INFO - [conn=3] Connection closed\n",
      "2025-04-08 18:55:39,344 - INFO - Batch 472320.x3000c0s25b0n0.hsn.hpc.srce.hr state PBSProJobState.RUNNING\n"
     ]
    }
   ],
   "source": [
    "res = await llm.batch_generate(\n",
    "    batch_request=batch_request,\n",
    "    response_type=SomeOutput,\n",
    "    poll_interval=3 * 60,\n",
    "    max_num_retries=0,\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 15:22:16,598 - INFO - Reader thread started\n",
      "2025-04-08 15:22:16,599 - INFO - Processor thread started\n",
      "2025-04-08 15:22:16,599 - INFO - Writer thread started\n",
      "2025-04-08 15:22:16,600 - INFO - Finished reading input file\n",
      "2025-04-08 15:22:16,601 - INFO - Reader thread exiting\n",
      "2025-04-08 15:22:16,879 - INFO - Processor thread exiting\n",
      "2025-04-08 15:22:16,881 - INFO - Finished writing output file\n",
      "2025-04-08 15:22:16,882 - INFO - Writer thread exiting\n",
      "2025-04-08 15:22:16,883 - INFO - All threads have completed\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "\n",
    "from decouple import config\n",
    "\n",
    "\n",
    "environ['PBS_O_WORKDIR'] = '../.tmp'\n",
    "environ['TGI_API_KEY'] = config('HUGGINGFACE_TOKEN')\n",
    "environ['MODEL_HUB_ID'] = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "environ['MAX_RETRIES'] = '3'\n",
    "\n",
    "%run ../assets/hpc/hf/tgi/tgi.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
