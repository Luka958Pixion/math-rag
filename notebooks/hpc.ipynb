{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from math_rag.application.containers import ApplicationContainer\n",
    "    from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "    application_container: ApplicationContainer\n",
    "    infrastructure_container: InfrastructureContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESET = False\n",
    "%load_ext hooks.notebook_hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from huggingface_hub import AsyncInferenceClient\n",
    "\n",
    "\n",
    "HUGGINGFACE_TOKEN = config('HUGGINGFACE_TOKEN')\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "client = AsyncInferenceClient(\n",
    "    model=MODEL_HUB_ID,\n",
    "    provider='hf-inference',\n",
    "    timeout=None,\n",
    "    api_key=HUGGINGFACE_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.base.assistants import BaseAssistantInput, BaseAssistantOutput\n",
    "\n",
    "\n",
    "class SomeInput(BaseAssistantInput):\n",
    "    pass\n",
    "\n",
    "\n",
    "class SomeOutput(BaseAssistantOutput):\n",
    "    result: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.inference import (\n",
    "    LLMBatchRequest,\n",
    "    LLMConversation,\n",
    "    LLMMessage,\n",
    "    LLMParams,\n",
    "    LLMRequest,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "some_input = SomeInput()\n",
    "\n",
    "_requests = [\n",
    "    LLMRequest(\n",
    "        conversation=LLMConversation(\n",
    "            messages=[\n",
    "                LLMMessage(role='system', content='You are a helpful assistant.'),\n",
    "                LLMMessage(role='user', content=f'what is {i}+2'),\n",
    "            ]\n",
    "        ),\n",
    "        params=LLMParams(\n",
    "            model=MODEL_HUB_ID,\n",
    "            temperature=0,\n",
    "            response_type=SomeOutput,\n",
    "            max_completion_tokens=10,\n",
    "            metadata={'input_id': str(some_input.id)},\n",
    "        ),\n",
    "    )\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "batch_request: LLMBatchRequest = LLMBatchRequest(requests=_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.inference import (\n",
    "    EMBatchRequest,\n",
    "    EMParams,\n",
    "    EMRequest,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_HUB_ID = 'BAAI/bge-large-en-v1.5'\n",
    "\n",
    "_em_requests = [\n",
    "    EMRequest(\n",
    "        text=f'hello world {i}',\n",
    "        params=EMParams(model=MODEL_HUB_ID, dimensions=1024),\n",
    "    )\n",
    "    for i in range(50)\n",
    "]\n",
    "\n",
    "em_batch_request: EMBatchRequest = EMBatchRequest(requests=_em_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apptainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "\n",
    "infrastructure_container = InfrastructureContainer()\n",
    "infrastructure_container.init_resources()\n",
    "\n",
    "tgi_batch_llm = infrastructure_container.tgi_batch_llm()\n",
    "tei_batch_em = infrastructure_container.tei_batch_em()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await tgi_batch_llm.init_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await tgi_batch_llm.batch_generate(\n",
    "    batch_request=batch_request,\n",
    "    response_type=SomeOutput,\n",
    "    poll_interval=3 * 60,\n",
    "    max_tokens_per_day=None,\n",
    "    max_input_file_size=None,\n",
    "    max_num_retries=0,\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await tei_batch_em.init_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await tei_batch_em.batch_embed(\n",
    "    batch_request=em_batch_request,\n",
    "    poll_interval=3 * 60,\n",
    "    max_tokens_per_day=None,\n",
    "    max_input_file_size=None,\n",
    "    max_num_retries=0,\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "from decouple import config\n",
    "\n",
    "\n",
    "environ['PBS_O_WORKDIR'] = '../.tmp'\n",
    "environ['TGI_API_KEY'] = config('HUGGINGFACE_TOKEN')\n",
    "environ['MODEL_HUB_ID'] = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "%run ../assets/hpc/hf/tgi/tgi_client.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
