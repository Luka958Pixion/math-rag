{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from huggingface_hub import AsyncInferenceClient\n",
    "\n",
    "\n",
    "HUGGINGFACE_TOKEN = config('HUGGINGFACE_TOKEN')\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "client = AsyncInferenceClient(\n",
    "    model=MODEL_HUB_ID,\n",
    "    provider='hf-inference',\n",
    "    timeout=None,\n",
    "    api_key=HUGGINGFACE_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.base.assistants import BaseAssistantInput, BaseAssistantOutput\n",
    "\n",
    "\n",
    "class SomeInput(BaseAssistantInput):\n",
    "    pass\n",
    "\n",
    "\n",
    "class SomeOutput(BaseAssistantOutput):\n",
    "    result: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from math_rag.application.models.inference import (\n",
    "    LLMBatchRequest,\n",
    "    LLMBatchResult,\n",
    "    LLMConversation,\n",
    "    LLMMessage,\n",
    "    LLMParams,\n",
    "    LLMRequest,\n",
    ")\n",
    "from math_rag.infrastructure.mappings.inference.huggingface import (\n",
    "    LLMRequestMapping,\n",
    "    LLMResponseListMapping,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "some_input = SomeInput()\n",
    "\n",
    "request = LLMRequest(\n",
    "    conversation=LLMConversation(\n",
    "        messages=[\n",
    "            LLMMessage(role='system', content='You are a helpful assistant.'),\n",
    "            LLMMessage(role='user', content='what is 2+2'),\n",
    "        ]\n",
    "    ),\n",
    "    params=LLMParams(\n",
    "        model=MODEL_HUB_ID,\n",
    "        temperature=0,\n",
    "        response_type=SomeOutput,\n",
    "        max_completion_tokens=10,\n",
    "        metadata={'input_id': str(some_input.id)},\n",
    "    ),\n",
    ")\n",
    "\n",
    "batch_request: LLMBatchRequest = LLMBatchRequest(requests=[request])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [LLMRequestMapping.to_target(request) for request in batch_request.requests]\n",
    "lines = [json.dumps(request, separators=(',', ':')) for request in requests]\n",
    "jsonl_str = '\\n'.join(lines)\n",
    "jsonl_bytes = jsonl_str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_dict = json.loads(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='{ \"result\": 4 }', tool_call_id=None, tool_calls=None), logprobs=None)], created=1744125157, id='', model='microsoft/Phi-3-mini-4k-instruct', system_fingerprint='3.2.1-native', usage=ChatCompletionOutputUsage(completion_tokens=9, prompt_tokens=17, total_tokens=26), object='chat.completion')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await client.chat_completion(**request_dict)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResponseList(id=UUID('f7fd3b29-2772-468d-831f-b160bf1dfaa0'), request_id=UUID('cb02d5a3-bed8-4109-b06c-04d585081ef3'), responses=[LLMResponse(id=UUID('3976268b-adbf-494a-b88b-fda24781d477'), content=BoundAssistantOutput(id=UUID('bd7fdedd-f32a-4d7f-845d-0a21413828b6'), input_id=UUID('c882baa3-2047-4f65-b8df-bd176bba3b2c'), result=4), logprobs=None)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_list = LLMResponseListMapping.to_source(\n",
    "    result,\n",
    "    request_id=request.id,\n",
    "    input_id=request_dict['extra_body']['input_id'],\n",
    "    response_type=SomeOutput,\n",
    ")\n",
    "response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = Path(f'.tmp/input_{batch_request.id}.jsonl')\n",
    "\n",
    "with open(input_file_path, 'w') as input_file:\n",
    "    for line in lines:\n",
    "        input_file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apptainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "\n",
    "infrastructure_container = InfrastructureContainer()\n",
    "infrastructure_container.init_resources()\n",
    "\n",
    "apptainer_client = infrastructure_container.apptainer_client()\n",
    "sftp_client = infrastructure_container.sftp_client()\n",
    "pbs_pro_client = infrastructure_container.pbs_pro_client()\n",
    "file_system_client = infrastructure_container.file_system_client()\n",
    "hpc_client = infrastructure_container.hpc_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.infrastructure.inference.huggingface import TGIBatchLLM\n",
    "\n",
    "\n",
    "llm = TGIBatchLLM(\n",
    "    file_system_client=file_system_client,\n",
    "    pbs_pro_client=pbs_pro_client,\n",
    "    sftp_client=sftp_client,\n",
    "    apptainer_client=apptainer_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.init_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await llm.batch_generate(\n",
    "    batch_request=batch_request,\n",
    "    response_type=SomeOutput,\n",
    "    poll_interval=3 * 60,\n",
    "    max_num_retries=0,\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "from decouple import config\n",
    "\n",
    "\n",
    "environ['PBS_O_WORKDIR'] = '../.tmp'\n",
    "environ['TGI_API_KEY'] = config('HUGGINGFACE_TOKEN')\n",
    "environ['MODEL_HUB_ID'] = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "%run ../assets/hpc/hf/tgi/tgi_client.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
