{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "\n",
    "\n",
    "HPC_USER = config('HPC_USER')\n",
    "HPC_HOST = config('HPC_HOST')\n",
    "HPC_PASSPHRASE = config('HPC_PASSPHRASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from math_rag.infrastructure.clients import HPCClient, PBSProClient, SFTPClient\n",
    "\n",
    "\n",
    "sftp_client = SFTPClient(HPC_HOST, HPC_USER, HPC_PASSPHRASE)\n",
    "pbs_pro_client = PBSProClient(HPC_HOST, HPC_USER, HPC_PASSPHRASE)\n",
    "hpc_client = HPCClient(HPC_HOST, HPC_USER, HPC_PASSPHRASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove .. when not in /notebooks\n",
    "source = Path('../assets/huggingface/tgi/tgi.sh')\n",
    "target = Path('/lustre/home/lpanic/tgi_test.sh')\n",
    "\n",
    "await sftp_client.upload(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove .. when not in /notebooks\n",
    "source = Path('/lustre/home/lpanic/tgi_test.sh')\n",
    "target = Path('../tmp/tgi.sh')\n",
    "\n",
    "await sftp_client.download(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limited to 80 characters (79 + newline)\n",
    "len('    Error_Path = x3000c0s27b0n0.hsn.hpc.srce.hr:/lustre/home/lpanic/hello_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qstat -f 465317\n",
    "queue_status = \"\"\"\n",
    "Job Id: 465317.x3000c0s25b0n0.hsn.hpc.srce.hr\n",
    "    Job_Name = hello\n",
    "    Job_Owner = lpanic@x3000c0s27b0n0.hsn.hpc.srce.hr\n",
    "    resources_used.cpupercent = 0\n",
    "    resources_used.cput = 00:00:00\n",
    "    resources_used.mem = 0b\n",
    "    resources_used.ncpus = 1\n",
    "    resources_used.vmem = 0kb\n",
    "    resources_used.walltime = 00:00:00\n",
    "    job_state = R\n",
    "    queue = cpu-single\n",
    "    server = x3000c0s25b0n0.hsn.hpc.srce.hr\n",
    "    Checkpoint = u\n",
    "    ctime = Fri Mar 28 10:45:48 2025\n",
    "    Error_Path = x3000c0s27b0n0.hsn.hpc.srce.hr:/lustre/home/lpanic/hello_world\n",
    "\t/hello.e465317\n",
    "    exec_host = x8000c1s4b0n1/1\n",
    "    exec_vnode = (x8000c1s4b0n1:mem=1843200kb:ncpus=1:ngpus=0)\n",
    "    Hold_Types = n\n",
    "    Join_Path = n\n",
    "    Keep_Files = oed\n",
    "    Mail_Points = a\n",
    "    mtime = Fri Mar 28 10:45:53 2025\n",
    "    Output_Path = x3000c0s27b0n0.hsn.hpc.srce.hr:/lustre/home/lpanic/hello_worl\n",
    "\td/hello.o465317\n",
    "    Priority = 0\n",
    "    qtime = Fri Mar 28 10:45:48 2025\n",
    "    Rerunable = True\n",
    "    Resource_List.mem = 1800mb\n",
    "    Resource_List.ncpus = 1\n",
    "    Resource_List.ngpus = 0\n",
    "    Resource_List.nodect = 1\n",
    "    Resource_List.place = pack\n",
    "    Resource_List.select = 1:mem=1800mb:ncpus=1:ngpus=0\n",
    "    Resource_List.walltime = 48:00:00\n",
    "    stime = Fri Mar 28 10:45:48 2025\n",
    "    session_id = 3900810\n",
    "    jobdir = /lustre/home/lpanic\n",
    "    substate = 42\n",
    "    Variable_List = PBS_O_HOME=/lustre/home/lpanic,PBS_O_LANG=en_US.UTF-8,\n",
    "\tPBS_O_LOGNAME=lpanic,\n",
    "\tPBS_O_PATH=/lustre/home/lpanic/.local/bin:/lustre/home/lpanic/bin:/opt\n",
    "\t/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/share/Module\n",
    "\ts/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/op\n",
    "\tt/pbs/bin:/sbin:/bin,PBS_O_MAIL=/var/spool/mail/lpanic,\n",
    "\tPBS_O_SHELL=/bin/bash,PBS_O_HOST=x3000c0s27b0n0.hsn.hpc.srce.hr,\n",
    "\tPBS_O_WORKDIR=/lustre/home/lpanic/hello_world,PBS_O_SYSTEM=Linux,\n",
    "\tPBS_O_QUEUE=RouteQ\n",
    "         = Job run at Fri Mar 28 at 10:45 on (x8000c1s4b0n1:mem=1843200kb:nc\n",
    "\tpus=1:ngpus=0)\n",
    "    etime = Fri Mar 28 10:45:48 2025\n",
    "    run_count = 1\n",
    "    eligible_time = 00:00:05\n",
    "    Submit_arguments = -koed hello.sh\n",
    "    project = _pbs_project_default\n",
    "    Submit_Host = x3000c0s27b0n0.hsn.hpc.srce.hr\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.infrastructure.mappings.hpcs.pbs import PBSProJobFullMapping\n",
    "\n",
    "\n",
    "job = PBSProJobFullMapping.to_source(queue_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'469063.x3000c0s25b0n0.hsn.hpc.srce.hr'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbs_path = Path('hello_world/hello2b.sh')\n",
    "job_id = await pbs_pro_client.queue_submit(pbs_path)\n",
    "job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBSProJobAlternate(id='469063.x3000c0s25b0*', user='lpanic', queue=<HPCQueue.GPU: 'gpu'>, name='hello', session_id=None, num_chunks=1, num_cpus=1, requested_mem=120000000000, requested_time=datetime.timedelta(seconds=600), state=<PBSProJobState.QUEUED: 'Q'>, elapsed_time=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = await pbs_pro_client.queue_status(job_id, alternate=True, full=False)\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PBSProJobFull(id='469063.x3000c0s25b0n0.hsn.hpc.srce.hr', name='hello', owner='lpanic@x3000c0s27b0n0.hsn.hpc.srce.hr', state=<PBSProJobState.QUEUED: 'Q'>, queue=<HPCQueue.GPU: 'gpu'>, server='x3000c0s25b0n0.hsn.hpc.srce.hr', checkpoint='u', exec_host=None, exec_vnode=None, error_path='x3000c0s27b0n0.hsn.hpc.srce.hr:/lustre/home/lpanic/error.log', output_path='x3000c0s27b0n0.hsn.hpc.srce.hr:/lustre/home/lpanic/output.log', dir=None, hold_types=<PBSProHoldType.NONE: 'n'>, join_path=<PBSProJoinPath.NONE: 'n'>, keep_files=<PBSProKeepFiles.STDOUT_STDERR_DIRECT: 'oed'>, mail_points=<PBSProMailPoints.ABORT: 'a'>, substate=10, priority=0, session_id=None, rerunable=True, run_count=None, submit_arguments='-koed hello_world/hello2b.sh', project='_pbs_project_default', submit_host='x3000c0s27b0n0.hsn.hpc.srce.hr', created=datetime.datetime(2025, 3, 31, 14, 12, 5), queued=datetime.datetime(2025, 3, 31, 14, 12, 5), modified=datetime.datetime(2025, 3, 31, 14, 12, 5), started=None, eligible=datetime.datetime(2025, 3, 31, 14, 12, 5), eligible_delta=datetime.timedelta(seconds=23), resource_list=PBSProResourceList(mem=120000000000, num_cpus=1, num_gpus=2, num_nodes=1, place='pack', select='1:ncpus=1:ngpus=2', walltime=datetime.timedelta(seconds=600)), resources_used=None, variable_list=PBSProVariableList(home=PosixPath('/lustre/home/lpanic'), paths=[PosixPath('/lustre/home/lpanic/.local/bin'), PosixPath('/lustre/home/lpanic/bin'), PosixPath('/opt/clmgr/sbin'), PosixPath('/opt/clmgr/bin'), PosixPath('/opt/sgi/sbin'), PosixPath('/opt/sgi/bin'), PosixPath('/usr/share/Modules/bin'), PosixPath('/usr/local/bin'), PosixPath('/usr/bin'), PosixPath('/usr/local/sbin'), PosixPath('/usr/sbin'), PosixPath('/opt/c3/bin'), PosixPath('/opt/pbs/bin'), PosixPath('/sbin'), PosixPath('/bin')], mail=None, shell='/bin/bash', workdir=PosixPath('/lustre/home/lpanic'), host='x3000c0s27b0n0.hsn.hpc.srce.hr', lang='en_US.UTF-8', logname='lpanic', system='Linux', queue='gpu', tz=None, environment=None, jobdir=None, jobid=None, jobname=None, nodefile=None, execution_queue=None, tmpdir=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = await pbs_pro_client.queue_status(job_id, alternate=False, full=True)\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await pbs_pro_client.trace_job(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "await pbs_pro_client.queue_delete(job_id, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from huggingface_hub import AsyncInferenceClient\n",
    "\n",
    "\n",
    "HUGGINGFACE_TOKEN = config('HUGGINGFACE_TOKEN')\n",
    "\n",
    "TGI_BASE_URL = ''\n",
    "TGI_MODEL = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "client = AsyncInferenceClient(\n",
    "    # base_url=TGI_BASE_URL,\n",
    "    model=TGI_MODEL,\n",
    "    provider='hf-inference',\n",
    "    timeout=None,\n",
    "    api_key=HUGGINGFACE_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.base.assistants import BaseAssistantInput\n",
    "\n",
    "\n",
    "class Result(BaseAssistantInput):\n",
    "    result: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from math_rag.application.models.inference import (\n",
    "    LLMBatchRequest,\n",
    "    LLMBatchResult,\n",
    "    LLMConversation,\n",
    "    LLMMessage,\n",
    "    LLMParams,\n",
    "    LLMRequest,\n",
    ")\n",
    "from math_rag.infrastructure.mappings.inference.huggingface import (\n",
    "    LLMRequestMapping,\n",
    "    LLMResponseListMapping,\n",
    ")\n",
    "\n",
    "\n",
    "request = LLMRequest(\n",
    "    conversation=LLMConversation(\n",
    "        messages=[\n",
    "            LLMMessage(role='system', content='You are a helpful assistant.'),\n",
    "            LLMMessage(role='user', content='what is 2+2'),\n",
    "        ]\n",
    "    ),\n",
    "    params=LLMParams(\n",
    "        model=TGI_MODEL, temperature=0, response_type=Result, max_completion_tokens=10\n",
    "    ),\n",
    ")\n",
    "batch_request: LLMBatchRequest = LLMBatchRequest(requests=[request])\n",
    "requests = [LLMRequestMapping.to_target(request) for request in batch_request.requests]\n",
    "\n",
    "lines = [json.dumps(request, separators=(',', ':')) for request in requests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_dict = json.loads(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='{ \"result\": 4 }', tool_call_id=None, tool_calls=None), logprobs=None)], created=1743597261, id='', model='microsoft/Phi-3-mini-4k-instruct', system_fingerprint='3.2.1-native', usage=ChatCompletionOutputUsage(completion_tokens=9, prompt_tokens=17, total_tokens=26), object='chat.completion')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await client.chat_completion(**request_dict)  # TODO\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list = LLMResponseListMapping.to_source(\n",
    "    result, request_id=..., input_id=..., response_type=...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = Path(f'input_{batch_request.id}.jsonl')\n",
    "\n",
    "with open(input_file_path, 'w') as input_file:\n",
    "    for line in lines:\n",
    "        input_file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='length', index=0, message=ChatCompletionOutputMessage(role='assistant', content='The sum of 2 and 2 is ', tool_call_id=None, tool_calls=None), logprobs=None)], created=1743595756, id='', model='microsoft/Phi-3-mini-4k-instruct', system_fingerprint='3.2.1-native', usage=ChatCompletionOutputUsage(completion_tokens=10, prompt_tokens=17, total_tokens=27), object='chat.completion')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await client.chat_completion(\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': 'what is 2+2'},\n",
    "    ],\n",
    "    model=TGI_MODEL,\n",
    "    response_format=None,\n",
    "    temperature=0,\n",
    "    max_tokens=10,\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='{ \"result\": 4 }', tool_call_id=None, tool_calls=None), logprobs=None)], created=1743595874, id='', model='microsoft/Phi-3-mini-4k-instruct', system_fingerprint='3.2.1-native', usage=ChatCompletionOutputUsage(completion_tokens=9, prompt_tokens=17, total_tokens=26), object='chat.completion')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await client.chat_completion(\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': 'what is 2+2'},\n",
    "    ],\n",
    "    model=TGI_MODEL,\n",
    "    response_format={\n",
    "        'type': 'json',  # json or regex\n",
    "        'value': Result.model_json_schema(),  # json schema\n",
    "    },\n",
    "    temperature=0,\n",
    "    max_tokens=10,\n",
    ")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
