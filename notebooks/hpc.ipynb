{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from huggingface_hub import AsyncInferenceClient\n",
    "\n",
    "\n",
    "HUGGINGFACE_TOKEN = config('HUGGINGFACE_TOKEN')\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "\n",
    "client = AsyncInferenceClient(\n",
    "    model=MODEL_HUB_ID,\n",
    "    provider='hf-inference',\n",
    "    timeout=None,\n",
    "    api_key=HUGGINGFACE_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.base.assistants import BaseAssistantInput, BaseAssistantOutput\n",
    "\n",
    "\n",
    "class SomeInput(BaseAssistantInput):\n",
    "    pass\n",
    "\n",
    "\n",
    "class SomeOutput(BaseAssistantOutput):\n",
    "    result: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from math_rag.application.models.inference import (\n",
    "    LLMBatchRequest,\n",
    "    LLMBatchResult,\n",
    "    LLMConversation,\n",
    "    LLMMessage,\n",
    "    LLMParams,\n",
    "    LLMRequest,\n",
    ")\n",
    "from math_rag.infrastructure.mappings.inference.huggingface import (\n",
    "    LLMRequestMapping,\n",
    "    LLMResponseListMapping,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_HUB_ID = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "some_input = SomeInput()\n",
    "\n",
    "request = LLMRequest(\n",
    "    conversation=LLMConversation(\n",
    "        messages=[\n",
    "            LLMMessage(role='system', content='You are a helpful assistant.'),\n",
    "            LLMMessage(role='user', content='what is 2+2'),\n",
    "        ]\n",
    "    ),\n",
    "    params=LLMParams(\n",
    "        model=MODEL_HUB_ID,\n",
    "        temperature=0,\n",
    "        response_type=SomeOutput,\n",
    "        max_completion_tokens=10,\n",
    "        metadata={'input_id': str(some_input.id)},\n",
    "    ),\n",
    ")\n",
    "\n",
    "batch_request: LLMBatchRequest = LLMBatchRequest(requests=[request])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [LLMRequestMapping.to_target(request) for request in batch_request.requests]\n",
    "lines = [json.dumps(request, separators=(',', ':')) for request in requests]\n",
    "jsonl_str = '\\n'.join(lines)\n",
    "jsonl_bytes = jsonl_str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_dict = json.loads(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='{ \"result\": 4 }', tool_call_id=None, tool_calls=None), logprobs=None)], created=1744125157, id='', model='microsoft/Phi-3-mini-4k-instruct', system_fingerprint='3.2.1-native', usage=ChatCompletionOutputUsage(completion_tokens=9, prompt_tokens=17, total_tokens=26), object='chat.completion')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await client.chat_completion(**request_dict)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResponseList(id=UUID('f7fd3b29-2772-468d-831f-b160bf1dfaa0'), request_id=UUID('cb02d5a3-bed8-4109-b06c-04d585081ef3'), responses=[LLMResponse(id=UUID('3976268b-adbf-494a-b88b-fda24781d477'), content=BoundAssistantOutput(id=UUID('bd7fdedd-f32a-4d7f-845d-0a21413828b6'), input_id=UUID('c882baa3-2047-4f65-b8df-bd176bba3b2c'), result=4), logprobs=None)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_list = LLMResponseListMapping.to_source(\n",
    "    result,\n",
    "    request_id=request.id,\n",
    "    input_id=request_dict['extra_body']['input_id'],\n",
    "    response_type=SomeOutput,\n",
    ")\n",
    "response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = Path(f'.tmp/input_{batch_request.id}.jsonl')\n",
    "\n",
    "with open(input_file_path, 'w') as input_file:\n",
    "    for line in lines:\n",
    "        input_file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apptainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "\n",
    "infrastructure_container = InfrastructureContainer()\n",
    "infrastructure_container.init_resources()\n",
    "\n",
    "apptainer_client = infrastructure_container.apptainer_client()\n",
    "sftp_client = infrastructure_container.sftp_client()\n",
    "pbs_pro_client = infrastructure_container.pbs_pro_client()\n",
    "hpc_client = infrastructure_container.hpc_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 23:08:02,264 - INFO - HTTP Request: GET http://host.docker.internal:7015/health \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await apptainer_client.health()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.infrastructure.inference.huggingface import HuggingFaceBatchLLM\n",
    "\n",
    "\n",
    "llm = HuggingFaceBatchLLM(\n",
    "    remote_project_root=Path('tgi_default_root'),\n",
    "    hpc_client=hpc_client,\n",
    "    pbs_pro_client=pbs_pro_client,\n",
    "    sftp_client=sftp_client,\n",
    "    apptainer_client=apptainer_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.init_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 23:08:04,393 - INFO - Host canonicalization disabled\n",
      "2025-04-09 23:08:04,394 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 23:08:04,419 - INFO - [conn=0] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:08:04,419 - INFO - [conn=0]   Local address: 172.22.0.9, port 55338\n",
      "2025-04-09 23:08:04,420 - INFO - [conn=0]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-09 23:08:04,473 - INFO - [conn=0] Beginning auth for user lpanic\n",
      "2025-04-09 23:08:05,261 - INFO - [conn=0] Auth for user lpanic succeeded\n",
      "2025-04-09 23:08:05,264 - INFO - [conn=0, chan=0] Requesting new SSH session\n",
      "2025-04-09 23:08:05,275 - INFO - [conn=0, chan=0]   Subsystem: sftp\n",
      "2025-04-09 23:08:05,280 - INFO - [conn=0, chan=0] Starting SFTP client\n",
      "2025-04-09 23:08:05,468 - INFO - [conn=0, chan=0] Received exit status 0\n",
      "2025-04-09 23:08:05,470 - INFO - [conn=0, chan=0] Received channel close\n",
      "2025-04-09 23:08:05,472 - INFO - [conn=0, chan=0] SFTP client exited\n",
      "2025-04-09 23:08:05,473 - INFO - [conn=0, chan=0] Closing channel\n",
      "2025-04-09 23:08:05,475 - INFO - [conn=0, chan=0] Channel closed\n",
      "2025-04-09 23:08:05,476 - INFO - [conn=0] Closing connection\n",
      "2025-04-09 23:08:05,476 - INFO - [conn=0] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-09 23:08:05,477 - INFO - [conn=0] Connection closed\n",
      "2025-04-09 23:08:05,478 - INFO - Host canonicalization disabled\n",
      "2025-04-09 23:08:05,478 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:08:05,487 - INFO - [conn=1] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:08:05,488 - INFO - [conn=1]   Local address: 172.22.0.9, port 55352\n",
      "2025-04-09 23:08:05,489 - INFO - [conn=1]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-09 23:08:05,527 - INFO - [conn=1] Beginning auth for user lpanic\n",
      "2025-04-09 23:08:05,795 - INFO - [conn=1] Auth for user lpanic succeeded\n",
      "2025-04-09 23:08:05,798 - INFO - [conn=1, chan=0] Requesting new SSH session\n",
      "2025-04-09 23:08:05,850 - INFO - [conn=1, chan=0]   Command: cd tgi_default_root && qsub -v MODEL_HUB_ID=microsoft/Phi-3-mini-4k-instruct -l select=1:ncpus=8:mem=34359738368B:ngpus=1,walltime=1:00:00 tgi.sh\n",
      "2025-04-09 23:08:06,014 - INFO - [conn=1, chan=0] Received exit status 0\n",
      "2025-04-09 23:08:06,015 - INFO - [conn=1, chan=0] Received channel close\n",
      "2025-04-09 23:08:06,017 - INFO - [conn=1, chan=0] Channel closed\n",
      "2025-04-09 23:08:06,017 - INFO - Command `cd tgi_default_root && qsub -v MODEL_HUB_ID=microsoft/Phi-3-mini-4k-instruct -l select=1:ncpus=8:mem=34359738368B:ngpus=1,walltime=1:00:00 tgi.sh` in `run` returned stdout: 472838.x3000c0s25b0n0.hsn.hpc.srce.hr\n",
      "2025-04-09 23:08:06,017 - INFO - [conn=1] Closing connection\n",
      "2025-04-09 23:08:06,017 - INFO - [conn=1] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-09 23:08:06,018 - INFO - [conn=1] Connection closed\n",
      "2025-04-09 23:08:06,019 - INFO - Host canonicalization disabled\n",
      "2025-04-09 23:08:06,020 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:08:06,029 - INFO - [conn=2] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:08:06,030 - INFO - [conn=2]   Local address: 172.22.0.9, port 55364\n",
      "2025-04-09 23:08:06,030 - INFO - [conn=2]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-09 23:08:06,069 - INFO - [conn=2] Beginning auth for user lpanic\n",
      "2025-04-09 23:08:06,339 - INFO - [conn=2] Auth for user lpanic succeeded\n",
      "2025-04-09 23:08:06,342 - INFO - [conn=2, chan=0] Requesting new SSH session\n",
      "2025-04-09 23:08:06,390 - INFO - [conn=2, chan=0]   Command: qstat -x 472838.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'\n",
      "2025-04-09 23:08:06,546 - INFO - [conn=2, chan=0] Received exit status 0\n",
      "2025-04-09 23:08:06,546 - INFO - [conn=2, chan=0] Received channel close\n",
      "2025-04-09 23:08:06,547 - INFO - [conn=2, chan=0] Channel closed\n",
      "2025-04-09 23:08:06,547 - INFO - Command `qstat -x 472838.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'` in `run` returned stdout: 472838.x3000c0s25b0n0 tgi lpanic 0 Q gpu\n",
      "2025-04-09 23:08:06,547 - INFO - [conn=2] Closing connection\n",
      "2025-04-09 23:08:06,547 - INFO - [conn=2] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-09 23:08:06,548 - INFO - [conn=2] Connection closed\n",
      "2025-04-09 23:08:06,548 - INFO - Batch 472838.x3000c0s25b0n0.hsn.hpc.srce.hr created with state PBSProJobState.QUEUED\n",
      "2025-04-09 23:08:06,549 - INFO - Host canonicalization disabled\n",
      "2025-04-09 23:08:06,549 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:08:06,558 - INFO - [conn=3] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:08:06,558 - INFO - [conn=3]   Local address: 172.22.0.9, port 55380\n",
      "2025-04-09 23:08:06,559 - INFO - [conn=3]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-09 23:08:06,602 - INFO - [conn=3] Beginning auth for user lpanic\n",
      "2025-04-09 23:08:06,863 - INFO - [conn=3] Auth for user lpanic succeeded\n",
      "2025-04-09 23:08:06,867 - INFO - [conn=3, chan=0] Requesting new SSH session\n",
      "2025-04-09 23:08:06,915 - INFO - [conn=3, chan=0]   Command: qstat -x 472838.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'\n",
      "2025-04-09 23:08:07,071 - INFO - [conn=3, chan=0] Received exit status 0\n",
      "2025-04-09 23:08:07,073 - INFO - [conn=3, chan=0] Received channel close\n",
      "2025-04-09 23:08:07,075 - INFO - [conn=3, chan=0] Channel closed\n",
      "2025-04-09 23:08:07,077 - INFO - Command `qstat -x 472838.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'` in `run` returned stdout: 472838.x3000c0s25b0n0 tgi lpanic 0 Q gpu\n",
      "2025-04-09 23:08:07,077 - INFO - [conn=3] Closing connection\n",
      "2025-04-09 23:08:07,078 - INFO - [conn=3] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-09 23:08:07,078 - INFO - [conn=3] Connection closed\n",
      "2025-04-09 23:08:07,079 - INFO - Batch 472838.x3000c0s25b0n0.hsn.hpc.srce.hr state PBSProJobState.QUEUED\n",
      "2025-04-09 23:11:07,167 - INFO - Host canonicalization disabled\n",
      "2025-04-09 23:11:07,167 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:11:07,180 - INFO - [conn=4] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:11:07,181 - INFO - [conn=4]   Local address: 172.22.0.9, port 36298\n",
      "2025-04-09 23:11:07,181 - INFO - [conn=4]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-09 23:11:07,270 - INFO - [conn=4] Beginning auth for user lpanic\n",
      "2025-04-09 23:11:07,542 - INFO - [conn=4] Auth for user lpanic succeeded\n",
      "2025-04-09 23:11:07,545 - INFO - [conn=4, chan=0] Requesting new SSH session\n",
      "2025-04-09 23:11:07,595 - INFO - [conn=4, chan=0]   Command: qstat -x 472838.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'\n",
      "2025-04-09 23:11:07,755 - INFO - [conn=4, chan=0] Received exit status 0\n",
      "2025-04-09 23:11:07,757 - INFO - [conn=4, chan=0] Received channel close\n",
      "2025-04-09 23:11:07,759 - INFO - [conn=4, chan=0] Channel closed\n",
      "2025-04-09 23:11:07,760 - INFO - Command `qstat -x 472838.x3000c0s25b0n0.hsn.hpc.srce.hr | awk 'NR==3 {print $1, $2, $3, $4, $5, $6}'` in `run` returned stdout: 472838.x3000c0s25b0n0 tgi lpanic 00:00:46 F gpu\n",
      "2025-04-09 23:11:07,760 - INFO - [conn=4] Closing connection\n",
      "2025-04-09 23:11:07,762 - INFO - [conn=4] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-09 23:11:07,763 - INFO - [conn=4] Connection closed\n",
      "2025-04-09 23:11:07,763 - INFO - Batch 472838.x3000c0s25b0n0.hsn.hpc.srce.hr state PBSProJobState.FINISHED\n",
      "2025-04-09 23:11:07,765 - INFO - Host canonicalization disabled\n",
      "2025-04-09 23:11:07,765 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:11:07,775 - INFO - [conn=5] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:11:07,775 - INFO - [conn=5]   Local address: 172.22.0.9, port 36306\n",
      "2025-04-09 23:11:07,775 - INFO - [conn=5]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-09 23:11:07,803 - INFO - [conn=5] Beginning auth for user lpanic\n",
      "2025-04-09 23:11:08,073 - INFO - [conn=5] Auth for user lpanic succeeded\n",
      "2025-04-09 23:11:08,075 - INFO - [conn=5, chan=0] Requesting new SSH session\n",
      "2025-04-09 23:11:08,126 - INFO - [conn=5, chan=0]   Subsystem: sftp\n",
      "2025-04-09 23:11:08,139 - INFO - [conn=5, chan=0] Starting SFTP client\n",
      "2025-04-09 23:11:08,385 - INFO - [conn=5, chan=0] Received exit status 0\n",
      "2025-04-09 23:11:08,386 - INFO - [conn=5, chan=0] Received channel close\n",
      "2025-04-09 23:11:08,386 - INFO - [conn=5, chan=0] SFTP client exited\n",
      "2025-04-09 23:11:08,386 - INFO - [conn=5, chan=0] Closing channel\n",
      "2025-04-09 23:11:08,386 - INFO - [conn=5, chan=0] Channel closed\n",
      "2025-04-09 23:11:08,387 - INFO - [conn=5] Closing connection\n",
      "2025-04-09 23:11:08,387 - INFO - [conn=5] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-09 23:11:08,388 - INFO - [conn=5] Connection closed\n",
      "2025-04-09 23:11:08,391 - INFO - Host canonicalization disabled\n",
      "2025-04-09 23:11:08,392 - INFO - Opening SSH connection to login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:11:08,400 - INFO - [conn=6] Connected to SSH server at login-gpu.hpc.srce.hr, port 22\n",
      "2025-04-09 23:11:08,400 - INFO - [conn=6]   Local address: 172.22.0.9, port 36320\n",
      "2025-04-09 23:11:08,400 - INFO - [conn=6]   Peer address: 161.53.2.37, port 22\n",
      "2025-04-09 23:11:08,435 - INFO - [conn=6] Beginning auth for user lpanic\n",
      "2025-04-09 23:11:08,706 - INFO - [conn=6] Auth for user lpanic succeeded\n",
      "2025-04-09 23:11:08,707 - INFO - [conn=6, chan=0] Requesting new SSH session\n",
      "2025-04-09 23:11:08,837 - INFO - [conn=6, chan=0]   Command: rm -f tgi_default_root/output.jsonl tgi_default_root/output.jsonl\n",
      "2025-04-09 23:11:08,989 - INFO - [conn=6, chan=0] Received exit status 0\n",
      "2025-04-09 23:11:08,992 - INFO - [conn=6, chan=0] Received channel close\n",
      "2025-04-09 23:11:08,994 - INFO - [conn=6, chan=0] Channel closed\n",
      "2025-04-09 23:11:08,995 - INFO - Command `rm -f tgi_default_root/output.jsonl tgi_default_root/output.jsonl` in `run` returned stdout: \n",
      "2025-04-09 23:11:08,995 - INFO - [conn=6] Closing connection\n",
      "2025-04-09 23:11:08,996 - INFO - [conn=6] Sending disconnect: Disconnected by application (11)\n",
      "2025-04-09 23:11:08,996 - INFO - [conn=6] Connection closed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMBatchResult(id=UUID('0d2fe74d-4742-4ff8-af76-4b492a4df6ef'), response_lists=[LLMResponseList(id=UUID('1a4e5d53-06a0-465c-8deb-8009184ed348'), request_id=UUID('edcac871-b0d2-4853-b5fd-9a96740be189'), responses=[LLMResponse(id=UUID('f7f5933a-dd9a-4f5c-b28b-046de3bc81db'), content=BoundAssistantOutput(id=UUID('ac172dc5-e9d6-494c-a789-30cddaf8fb74'), input_id=UUID('ae419d0d-42f4-43e7-ad80-f5da2bdf871b'), result=4), logprobs=None)])], failed_requests=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await llm.batch_generate(\n",
    "    batch_request=batch_request,\n",
    "    response_type=SomeOutput,\n",
    "    poll_interval=3 * 60,\n",
    "    max_num_retries=0,\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 22:36:02,044 - INFO - Reader thread started\n",
      "2025-04-09 22:36:02,045 - INFO - Processor thread started\n",
      "2025-04-09 22:36:02,045 - INFO - Writer thread started\n",
      "2025-04-09 22:36:02,046 - INFO - Finished reading input file\n",
      "2025-04-09 22:36:02,046 - INFO - Reader thread exiting\n",
      "2025-04-09 22:36:03,281 - INFO - Processor thread exiting\n",
      "2025-04-09 22:36:03,282 - INFO - Finished writing output file\n",
      "2025-04-09 22:36:03,282 - INFO - Writer thread exiting\n",
      "2025-04-09 22:36:03,283 - INFO - All threads have completed\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "\n",
    "from decouple import config\n",
    "\n",
    "\n",
    "environ['PBS_O_WORKDIR'] = '../.tmp'\n",
    "environ['TGI_API_KEY'] = config('HUGGINGFACE_TOKEN')\n",
    "environ['MODEL_HUB_ID'] = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "environ['MAX_RETRIES'] = '3'\n",
    "\n",
    "%run ../assets/hpc/hf/tgi/tgi.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
