{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32ba622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from math_rag.application.containers import ApplicationContainer\n",
    "    from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "    application_container: ApplicationContainer\n",
    "    infrastructure_container: InfrastructureContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c020d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 10:02:39,802 - INFO - datasets - config.py:54 - PyTorch version 2.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "RESET = False\n",
    "%load_ext hooks.notebook_hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386180b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92db620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expression_description_writer_assistant = (\n",
    "    application_container.math_expression_description_writer_assistant()\n",
    ")\n",
    "math_expression_description_optimizer_assistant = (\n",
    "    application_container.math_expression_description_optimizer_assistant()\n",
    ")\n",
    "math_expression_comparator_assistant = application_container.math_expression_comparator_assistant()\n",
    "math_expression_relationship_description_writer_assistant = (\n",
    "    application_container.math_expression_relationship_description_writer_assistant()\n",
    ")\n",
    "math_expression_relationship_detector_assistant = (\n",
    "    application_container.math_expression_relationship_detector_assistant()\n",
    ")\n",
    "\n",
    "default_embedder = application_container.default_embedder()\n",
    "math_expression_description_opt_embedding_repository = (\n",
    "    infrastructure_container.math_expression_description_opt_embedding_repository()\n",
    ")\n",
    "math_expression_description_opt_repository = (\n",
    "    infrastructure_container.math_expression_description_opt_repository()\n",
    ")\n",
    "math_expression_description_repository = (\n",
    "    infrastructure_container.math_expression_description_repository()\n",
    ")\n",
    "math_expression_group_repository = infrastructure_container.math_expression_group_repository()\n",
    "math_expression_repository = infrastructure_container.math_expression_repository()\n",
    "grouper_service = application_container.grouper_service()\n",
    "\n",
    "math_expression_graph_repository = await infrastructure_container.math_expression_graph_repository()\n",
    "math_expression_relationship_repository = (\n",
    "    infrastructure_container.math_expression_relationship_repository()\n",
    ")\n",
    "math_expression_relationship_description_repository = (\n",
    "    infrastructure_container.math_expression_relationship_description_repository()\n",
    ")\n",
    "katex_corrector_service = application_container.katex_corrector_service()\n",
    "math_expression_context_repository = infrastructure_container.math_expression_context_repository()\n",
    "math_article_chunk_repository = infrastructure_container.math_article_chunk_repository()\n",
    "math_expression_index_repository = infrastructure_container.math_expression_index_repository()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb39d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 00:23:32,301 - INFO - googleapiclient.discovery_cache - __init__.py:49 - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from math_rag.core.models import MathArticle\n",
    "\n",
    "\n",
    "google_drive_repository = infrastructure_container.google_drive_repository()\n",
    "math_article_parser_service = infrastructure_container.math_article_parser_service()\n",
    "\n",
    "file_id = google_drive_repository.get_file_id(\n",
    "    Path('ml/lectures/L07-LogisticRegression2/2024_08_10_2174b40686820b4cb591g.tex')\n",
    ")\n",
    "\n",
    "if not file_id:\n",
    "    raise ValueError()\n",
    "\n",
    "file_content = google_drive_repository.get_file_by_id(file_id)\n",
    "\n",
    "math_article = MathArticle(\n",
    "    math_expression_dataset_id=None,\n",
    "    math_expression_index_id=None,\n",
    "    name='article',\n",
    "    bytes=file_content.getvalue(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081db754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "\n",
    "from math_rag.application.utils import InputCreatorUtil\n",
    "from math_rag.core.models import MathExpressionIndex\n",
    "from math_rag.infrastructure.utils import (\n",
    "    TemplateChunkerUtil,\n",
    "    TemplateContextChunkerUtil,\n",
    "    TemplateFormatterUtil,\n",
    "    TemplateIndexFinderUtil,\n",
    ")\n",
    "\n",
    "\n",
    "index = MathExpressionIndex()\n",
    "index.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81928c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('6cafc7fa-982b-4285-92d0-a53dd2a8453a')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = MathExpressionIndex(id=UUID('6cafc7fa-982b-4285-92d0-a53dd2a8453a'))\n",
    "index.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - dont chunk over image placeholder (use code like in context chunker)\n",
    "# rename payload \"description\" in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99cb323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_id_to_remove = UUID('01627335-ba50-4c7d-85f2-eb0b27027c6f')\n",
    "common_filter = dict(math_expression_index_id=index_id_to_remove)\n",
    "\n",
    "await math_expression_index_repository.delete_one(filter=dict(id=index_id_to_remove))\n",
    "\n",
    "await math_expression_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_context_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_description_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_description_opt_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_group_repository.delete_many(filter=common_filter.copy())\n",
    "await math_article_chunk_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_relationship_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_relationship_description_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_description_opt_embedding_repository.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0146e",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816adeb6",
   "metadata": {},
   "source": [
    "### 1. MathExpression, requires: MathArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aad9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathExpression\n",
    "\n",
    "\n",
    "math_nodes, _, template = math_article_parser_service.parse_for_index(math_article)\n",
    "math_nodes.sort(key=lambda x: x.position)\n",
    "\n",
    "katexes = [math_node.latex.strip('$') for math_node in math_nodes]\n",
    "valid_katexes = await katex_corrector_service.correct(katexes, max_num_retries=3)\n",
    "math_expressions = [\n",
    "    MathExpression(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_dataset_id=None,\n",
    "        math_expression_group_id=None,\n",
    "        math_expression_index_id=index.id,\n",
    "        latex=node.latex,\n",
    "        katex=katex.strip(),\n",
    "        index=i,\n",
    "        position=node.position,\n",
    "        is_inline=node.is_inline,\n",
    "    )\n",
    "    for i, (node, katex) in enumerate(zip(math_nodes, valid_katexes))\n",
    "]\n",
    "await math_expression_repository.insert_many(math_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff89ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jan Å najder, lectures, v2.0\n",
      "\n",
      "Last time we introduced the logistic regression algorithm. We defined the model and derived the cross-entropy error function as the negative probability of the labels in the training set. We established that minimizing that error had no solution in closed form, so we turned to iterative procedures. We have considered the simplest such procedure, the gradient descent algorithm, and we applied it to logistic regression, in standard (batch) and stochastic variant. In the end, we talked about regularization, specifically [math_placeholder | 0] regularization, which we incorporated quite straightforwardly into the optimization process.\n",
      "\n",
      "Today we'll talk a bit more about logistic regression. First, we'll consider some more efficient (read: faster) alternatives to gradient descent. Second, we'll consider the extension of binary logistic regression to multiclass logistic regression. Third, we'll look at all the models discussed thus far and see what they have in common and how they can be generalized. Finally, we'll talk about adaptive basis functions, which is a way to learn the feature mapping function directly from data, instead of defining it manually.\n",
      "\n",
      "Unlike previous lectures, this one won't go into details, because we do not have the time for this. My goal is to give you enough information to know where to look for more, should you feel motivated to do so or if you need it\n",
      "\n",
      "\n",
      "Last time we established that the gradient descent needs to be coupled with line search, otherwise there is no guarantee for global convergence. This means that, depending on where the descent initially starts, it can happen that the gradient descent does not converge but diverge (effectively it starts to ascend instead of descend). Line search prevents this from happening. However, line search can result in a zig-zag descent. Let's recall the figure we discussed last time:\n",
      "\n",
      "\n",
      "[image_placeholder | 0]\n",
      "\n",
      "\n",
      "The blue trajectory corresponds to the best scenario, and the red to the worst-case scenario for a gradient descent with line search. Which one will play out depends on the choice of the starting point of the search. As we can see, it can happen that the descent zig-zags quite a lot, which means that the optimization will consume a lot of iterations. Obviously, the problem arises because the descent direction is not as good as it could be. Imagine descending into a pit from the starting point for the red trajectory. It is hard to imagine that you will descend\n",
      "as pointed to by the line. It is more likely that the gravity would pull you and your descend direction would be steeper (i.e., at a smaller angle to the blue line). This reasoning of ours is based on the curvature of the isocontours (that is, the curvature of the surface down which we descend). In other words, the curvature of the surface gives us, in addition to the gradient, additional information about where the minimum is likely to be located (at least when it comes to convex functions).\n",
      "\n",
      "The above observations apply to the standard (batch) gradient. In stochastic gradient descent we typically do not use line search. But there the descent will zig-zag quite a lot anyway, since each step is taken based on the gradient calculated for a single example. In the following, we focus on non-stochastic, i.e., batch gradient descent.\n",
      "\n",
      "Based on the above consideration, we can conclude that the batch gradient descent could be improved if we take into account not only the slope (gradient) but also the curvature (the change in gradient, i.e., the second derivative) of the error function. Such optimization methods are referred to as second-order optimization, as opposed to first-order optimization methods, such as gradient descent. The basic second-order optimization method is the Newton's method.\n",
      "\n",
      "\n",
      "Consider minimization of function [math_placeholder | 1]. We know that the parameter update in gradient descent is as follows:\n",
      "\n",
      "[math_placeholder | 2]\n",
      "\n",
      "If we introduce an index for the iterations, then we can write this as an equation:\n",
      "\n",
      "[math_placeholder | 3]\n",
      "\n",
      "The idea with Newton's method is to take the point [math_placeholder | 4] (the current minimum) and compute at it the quadratic approximation of the function [math_placeholder | 5], and then move to the minimizer of this quadratic approximation (which is known analytically). If [math_placeholder | 6] is a function of one variable, this would look like this:\n",
      "\n",
      "\n",
      "[image_placeholder | 1]\n",
      "\n",
      "\n",
      "The black curve is the function [math_placeholder | 7] that we minimize. We start from point [math_placeholder | 8]. At this point we do a quadratic approximation of the function [math_placeholder | 9], thus obtaining a parabola that is tangential to the function [math_placeholder | 10] at the point [math_placeholder | 11]. The search then moves to a point that minimizes the quadratic approximation of [math_placeholder | 12]. In the picture it is the point [math_placeholder | 13]. At that point we again do a quadratic approximation of the function [math_placeholder | 14] and move to the point that minimizes that approximation. The procedure is repeated until the update is small enough.\n",
      "\n",
      "The methods works just the same for functions of several variables, i.e., in a multidimensional parameter space. In a two-dimensional space, it looks like this:\n",
      "\n",
      "\n",
      "[image_placeholder | 2]\n",
      "\n",
      "\n",
      "So, the idea is to take the step exactly so that we land in the minimum of the quadratic approximation. This will work well if [math_placeholder | 15] is a convex function, which happens to be the case with logistic regression.\n",
      "\n",
      "Let us now deal with the technical details. We need to find a quadratic approximation of a function at some point. How can we do this? Recall, from calculus, that every differentiable function [math_placeholder | 16] can be expressed at some given point [math_placeholder | 17] in the form of a power series. More precisely, every differentiable function can be expanded into a Taylor series about the point [math_placeholder | 18], as follows:\n",
      "\n",
      "[math_placeholder | 19]\n",
      "\n",
      "As we only need a quadratic approximation, we will take only the first three terms of the Taylor series. This then is the second-order Taylor expansion:\n",
      "\n",
      "[math_placeholder | 20]\n",
      "\n",
      "Note that this now is an approximation; the equality between [math_placeholder | 21] on the left-hand side and the series on the right-hand side is valid only when the series is infinite.\n",
      "\n",
      "Our error function, [math_placeholder | 22], is a function of multiple variables (i.e., vector w). For a multivariate function [math_placeholder | 23], the quadratic expansion about the point [math_placeholder | 24] is:\n",
      "\n",
      "[math_placeholder | 25]\n",
      "\n",
      "where [math_placeholder | 26] is the Hessian matrix (hrv. Hesseova matrica) of function [math_placeholder | 27] at point [math_placeholder | 28]. The Hessian matrix is a square [math_placeholder | 29] matrix of second-order partial derivatives of function [math_placeholder | 30], which is a function that maps [math_placeholder | 31]-dimensional vectors into scalars. The Hessian matrix is defined as follows:\n",
      "\n",
      "[math_placeholder | 32]\n",
      "\n",
      "that is, an element of the Hessian matrix is defined as:\n",
      "\n",
      "[math_placeholder | 33]\n",
      "\n",
      "Note that the Hessian matrix is a symmetric matrix, as the order of partial differentiation does not affect the result (commutativity).\n",
      "\n",
      "Using the expansion of the quadratic function [math_placeholder | 34], one can now derive its minimum, which us exactly where we want to step to when descending (we omit the derivation). The parameter update is then:\n",
      "\n",
      "[math_placeholder | 35]\n",
      "\n",
      "If the Hessian matrix is computed exactly (i.e., it is not an approximation), then we can set [math_placeholder | 36], because then the step will land exactly in the minimum of the quadratic approximation.\n",
      "\n",
      "We see that the Newton's optimization method requires us to compute the inverse of the Hessian matrix. In general, the Hessian matrix [math_placeholder | 37] is positive semi-definite ( [math_placeholder | 38] for every non-zero vector [math_placeholder | 39] ) if and only if [math_placeholder | 40] is a convex function. However, for its inverse to exist, the matrix [math_placeholder | 41] has to be positive definite ( [math_placeholder | 42] for each non-zero vector [math_placeholder | 43] ). If [math_placeholder | 44] is positive semi-definite, but not positive definite, then [math_placeholder | 45] has no inverse and we cannot apply the Newton's method.\n",
      "\n",
      "\n",
      "Let us now apply Newton's procedure to logistic regression. The weights update rule is as follows:\n",
      "\n",
      "[math_placeholder | 46]\n",
      "\n",
      "One can easily derive that the Hessian matrix for the cross-entropy error equals:\n",
      "\n",
      "[math_placeholder | 47]\n",
      "\n",
      "where [math_placeholder | 48], i.e., a diagonal matrix that on its diagonal has the firstorder derivatives of the logistic output [math_placeholder | 49] for each of the [math_placeholder | 50] examples from the training set. Just in case, let's check the compatibility of the matrices we multiply here: [math_placeholder | 51] [math_placeholder | 52]. Everything is fine: the dimension of the matrix [math_placeholder | 53] is [math_placeholder | 54], where [math_placeholder | 55] is the number of features in the feature space (i.e., after mapping).\n",
      "\n",
      "Let's go back quickly to the problem of computing the inverse of the Hessian matrix [math_placeholder | 56]. As we already said, [math_placeholder | 57] is positive semi-definite if and only if the function is convex. The crossentropy error is convex, so the [math_placeholder | 58] matrix for logistic regression will be positive semi-definite. But this alone does not guarantee it is invertible. However, one can show that the Hessian matrix [math_placeholder | 59] of cross-entropy, which can be decomposed as [math_placeholder | 60], must be positive definite. In other words, the Hessian matrix of the cross entropy error always has an inverse, so we can always use the Newton's method to optimize the parameters of logistic regression. However, due to multicollinearity, [math_placeholder | 61] may be ill-conditioned and in this case the solution will be unstable, which means we have to either select a linearly independent subset of features or apply regularization (we'll see how soon).\n",
      "\n",
      "If we were now to plug in [math_placeholder | 62] for [math_placeholder | 63] in the weights update rule above, and rearrange the expression a bit, we would obtain the iteratively reweighted least squares (IRLS) algorithm (hrv. algoritam najmanjih kvadrata s iterativnim aÅ¾uriranjem teÅ¾ina). We won't go into the details; it suffices for you to know that this algorithm is used for faster optimization for logistic regression, and that it is in fact an application of the Newton's method, which is a second-order optimization procedure.\n",
      "\n",
      "\n",
      "The problem with Newton's method (and thus with IRLS) is that the computation of the Hessian (and especially its inverse) can be expensive (especially if [math_placeholder | 64], the number of dimensions of the feature space, is large). Note that one has to compute the Hessian matrix and its inverse in every step of the optimization procedure. It might well be the case that doing gradient descent is less taxing computationally, even if the descent zig-zags a lot!\n",
      "\n",
      "The alternative is that, instead of computing the exact Hessian matrix, we compute its approximation. This is what the so-called quasi-Newton methods do. These procedures using the gradient vectors (from the current and previous step) to approximate the Hessian matrix (or its inverse) in each step of the descent. The most common such method is the BFSG algorithm. Again, we won't go into details, we just want you to be aware that the method exists.\n",
      "\n",
      "Another problem is that, even if we approximate the Hessian matrix, storing it in memory can be problematic because its dimensions are [math_placeholder | 65]. In that case, the matrix can be \"compressed\" using a low-rank approximation method. The algorithm that works that way is the limited BFSG (L-BFSG) (hrv. ograniÄen BFSG). Again, it suffices for you to know that this algorithm exists.\n",
      "\n",
      "\n",
      "We already know about the benefits of regularization, and last time we talked about the additional benefit of regularization in logistic regression (when optimizing the parameters for linearly separable problems, we have [math_placeholder | 66] and logistic regression overfits easily). Extending the Newton's method to regularized cross-entropy error is simple. Let's look at [math_placeholder | 67] regulation. We already considered adding the [math_placeholder | 68] regularization term to the gradient:\n",
      "\n",
      "[math_placeholder | 69]\n",
      "\n",
      "We have the same for the Hessian matrix:\n",
      "\n",
      "[math_placeholder | 70]\n",
      "\n",
      "where [math_placeholder | 71] is a diagonal matrix with a regularization factor [math_placeholder | 72] on the diagonal (except for the upper-left element, because the weight [math_placeholder | 73] is not regularized).\n",
      "\n",
      "We see, thus, that [math_placeholder | 74] regularization is easily incorporated into the gradient descent and in the Newton's method, and it was just as easily incorporated into the least squares optimization method for the linear regression. All in all, [math_placeholder | 75] regularization is a tame beast.\n",
      "\n",
      "What about [math_placeholder | 76] regularization? So far, we haven't dealt with it at all, and we won't deal with it now, either. Let it be said that things get complicated here, which is expected because [math_placeholder | 77] norm is not differentiable, so we cannot calculate the gradient. We instead compute the subgradient (engl. podgradijent). We then typically use the coordinate descent (engl. koordinatni spust), where we optimize by each variable in term (dimension after dimension), or we use proximal or projection optimization methods. For now, you only need to know that optimization for [math_placeholder | 78] regularized logistic regression is possible, that there are algorithms for it, and that they are implemented in standard tools.\n",
      "\n",
      "\n",
      "Last time we talked about binary logistic regression: we classified into classes [math_placeholder | 79] and [math_placeholder | 80]. To get the probabilities, we used the sigmoid for the activation function:\n",
      "\n",
      "[math_placeholder | 81]\n",
      "\n",
      "But what if we have more than two classes, i.e. [math_placeholder | 82] ? We could apply the OVO or OVR decomposition scheme, but the problem is that the probabilities for the individual classes would not add up to 1 . Moreover, in statistical sense the estimates for the parameters for the individual models are less reliable than estimates for a model that considers all classes at once. Instead, it's better to use the multinomial logistic regression (MNR, occasionally MLR), also referred to as maximum entropy classifier (hrv. klasifikator maksimalne entropije).\n",
      "\n",
      "\n",
      "The idea is actually very simple: use a separate weight vector [math_placeholder | 83] for each of the [math_placeholder | 84] classes, but then pass the scalar product [math_placeholder | 85] through an appropriate activation function to be make sure that the probabilities of all classes add up to 1. A function that does exactly this is the softmax\n",
      "\n",
      "function. For some input example [math_placeholder | 86], the softmax function takes the values [math_placeholder | 87] for each of the [math_placeholder | 88] classes, i.e., a [math_placeholder | 89]-dimensional vector, and maps them to a [math_placeholder | 90]-dimensional vector whose components sum to 1 . Formally, softmax : [math_placeholder | 91], where [math_placeholder | 92] is the component of the output vector equal to:\n",
      "\n",
      "[math_placeholder | 93]\n",
      "\n",
      "The softmax function accomplishes two things: it normalizes all values to a total of 1 , but it also amplifies the larger values and attenuates the smaller values. The function is called softmax because it corresponds a \"soft\" variant of the max function (in the sense that, unlike the max function, it is continuous and differentiable). Let's look at an example.\n",
      "\n",
      "[math_placeholder | 94] EXAMPLE\n",
      "\n",
      "\n",
      "[image_placeholder | 3]\n",
      "\n",
      "\n",
      "[math_placeholder | 95]\n",
      "\n",
      "\n",
      "[image_placeholder | 4]\n",
      "\n",
      "\n",
      "[math_placeholder | 96]\n",
      "\n",
      "We will define the model [math_placeholder | 97] of multinomial logistic regression as a set of models [math_placeholder | 98], where each model [math_placeholder | 99] is responsible for class [math_placeholder | 100] out of [math_placeholder | 101] classes. We define each model [math_placeholder | 102] so that it outputs the probability of example [math_placeholder | 103] belonging to class [math_placeholder | 104], using the softmax function:\n",
      "\n",
      "[math_placeholder | 105]\n",
      "\n",
      "where [math_placeholder | 106] is a matrix [math_placeholder | 107] weight vectors [math_placeholder | 108]. Note that, by virtue of the softmax function, the model [math_placeholder | 109] for class [math_placeholder | 110] takes into account the outputs of the other [math_placeholder | 111] models for the remaining classes.\n",
      "\n",
      "This defines the model. Let us now derive the error function.\n",
      "\n",
      "\n",
      "In binary logistic regression, we defined derived the error function staring from the negative logarithm of the probability of the labels. The labels were binary, [math_placeholder | 112], that is, they were Bernoulli variables. Now, since the output of multiclass regression can take more than two [math_placeholder | 113] values, then we move to categorical variable, which is also also called multinomial, or, perhaps better, the multinoulli variable. We represent such a variable as a vector of indicator (binary) variables:\n",
      "\n",
      "[math_placeholder | 114]\n",
      "\n",
      "where [math_placeholder | 115] if the outcome of the variable is [math_placeholder | 116], otherwise [math_placeholder | 117]. For example, [math_placeholder | 118] indicates that the multinomial variable has taken the third state out of four possible states. [math_placeholder | 119] is valid (outcomes are mutually exclusive and complete). Let's denote the probability [math_placeholder | 120] as [math_placeholder | 121].\n",
      "\n",
      "We will now define the distribution of this variable. Recall, the distribution of the Bernoulli variable, which has only two values, [math_placeholder | 122] and [math_placeholder | 123], is defined via the [math_placeholder | 124] parameter as follows:\n",
      "\n",
      "[math_placeholder | 125]\n",
      "\n",
      "We can generalize this to [math_placeholder | 126] values as follows. First, we need [math_placeholder | 127] parameters, so we will define a parameter vector:\n",
      "\n",
      "[math_placeholder | 128]\n",
      "\n",
      "where parameters [math_placeholder | 129] satisfy [math_placeholder | 130] and [math_placeholder | 131], as they represent probabilities.\n",
      "\n",
      "Now, by analogy with the Bernoulli distribution, the distribution of a categorical variable can be defined as:\n",
      "\n",
      "[math_placeholder | 132]\n",
      "\n",
      "Note: as with binary logistic regression, the probability that example [math_placeholder | 133] belongs to class [math_placeholder | 134] is exactly what we are given by the model:\n",
      "\n",
      "[math_placeholder | 135]\n",
      "\n",
      "Now we can finally write the logarithm of the probability of the labels from [math_placeholder | 136] as:\n",
      "\n",
      "[math_placeholder | 137]\n",
      "\n",
      "The error function we wish to minimize is the negative logarithm of the probability of the labels:\n",
      "\n",
      "[math_placeholder | 138]\n",
      "\n",
      "We see that we arrived at the generalization of the cross-entropy error to [math_placeholder | 139] classes. Also, from this we can read off the loss function as:\n",
      "\n",
      "[math_placeholder | 140]\n",
      "\n",
      "The logic is the same as with binary logistic regression: if the label [math_placeholder | 141] of some example [math_placeholder | 142] for class [math_placeholder | 143] is equal to 1 , then we want the model prediction (softmax output) to be a high probability close to 1 , because then [math_placeholder | 144] and the loss will be zero. Otherwise, if the model for the example whose label is 1 gives a value close to 0 , then the logarithm will be a large negative number, its negation will be a large positive number, and consequently the loss will be large.\n",
      "\n",
      "\n",
      "As with binary logistic regression, we cannot minimize [math_placeholder | 145] in closed form, so we need to rely on iterative optimization. For gradient descent, one can show (although it is a bit clumsy) that the gradient of the error function is equal to:\n",
      "\n",
      "[math_placeholder | 146]\n",
      "\n",
      "This is the gradient of the weights specifically for class [math_placeholder | 147]. The idea is that we can update the weights for each class separately. From this we can directly derive the stochastic gradient descent (we update the weights for each example, for each class). We can also derive the standard (batch) gradient descent, where we accumulate updates for all examples, for each class separately. We can also derive the Newton method (the Hessian matrix), but we will skip that.\n",
      "\n",
      "\n",
      "You may have noticed that the gradient of the loss we derived for multinomial logistic regression is actually the same as that for binary logistic regression: \"model's error times the example vector\". When we use this for gradient descent, we update the weights as follows:\n",
      "\n",
      "[math_placeholder | 148]\n",
      "\n",
      "We have already (in the context of the perceptron, if you remember) said this rule is called the Widrow-Hoff rule, while the alternative name is least-mean-squares (LMS) algorithm (not to be confused with least squares, although there is obviously a connection).\n",
      "\n",
      "The LMS algorithm, that is, this kind of learning where we use stochastic gradient descent to minimize the error by updating the model's weights in the way defined above, allows for online learning. We mentioned online learning the last time. Recall: online learning is a type of learning where not all learning examples need to be available up front, rather they can become available one after the other, and the model weights will be updated as new examples arrive.\n",
      "\n",
      "Let's go back a few weeks, to linear regression. When we introduced linear regression, we were actually talking only about batch optimization, which is achieved by calculating the pseudoinverse of the design matrix. However, already then we could have resorted to a stochastic (i.e., online) weight update. Namely, instead of searching analytically for the minimum of the error function, we can calculate the gradient of the error function, and then apply the gradient descent. (We didn't do that at that at that time, probably because we were blinded by the sheer elegance of the closed-form solution). Well, let's do it now. The quadratic error function of linear regression is:\n",
      "\n",
      "[math_placeholder | 149]\n",
      "\n",
      "The gradient (for one example [math_placeholder | 150] ) is:\n",
      "\n",
      "[math_placeholder | 151]\n",
      "\n",
      "So, the rule for updating the weights is:\n",
      "\n",
      "[math_placeholder | 152]\n",
      "\n",
      "which, again, is the LMS rule! So, there seems to be some underlying principle at play here all three regression algorithms use the same rule (LMS) for online learning (i.e., for stochastic gradient descent). How can that be? Well, this is because all these models - linear regression, logistic regression, and multinomial regression - belong to the generalized linear models family.\n",
      "\n",
      "Now it's time to round out the story and give a unified perspective on the three algorithms.\n",
      "\n",
      "\n",
      "For starters, let's recall from last lecture that generalized linear models are models that wrap the scalar product of the weight vector and example vector into an activation function [math_placeholder | 153]. So:\n",
      "\n",
      "[math_placeholder | 154]\n",
      "\n",
      "Let's look at three generalized linear models we've considered so far. For each of them, let's consider four points: (1) how the model is defined, (2) what is the probability distribution to which their output corresponds, (3) how is the loss is defined, and (4) what is the gradient of the loss, which we need for gradient descent.\n",
      "\n",
      "First, let's look at the linear regression algorithm:\n",
      "\n",
      "[math_placeholder | 155]\n",
      "\n",
      "For batch learning we use the least squares method (the pseudoinverse), and for online learning we use the LMS rule.\n",
      "\n",
      "Let's look at the logistic regression algorithm:\n",
      "\n",
      "[math_placeholder | 156]\n",
      "\n",
      "For batch learning we use gradient descent, the Newton's (IRLS) or the quasi-Newton method (BFSG, L-BFSG). We use the LMS rule for online learning.\n",
      "\n",
      "Finally, let's look at the multinomial logistic regression:\n",
      "\n",
      "[math_placeholder | 157]\n",
      "\n",
      "We use the same (mutatis mutandis) optimization procedures for model learning as for logistic regression.\n",
      "\n",
      "Notice the commonalities. For all three algorithms, we derived the loss function from the negative logarithm of the probability of labels of the examples from the dataset. We did this by using the normal, Bernoulli, and multinoulli distribution distribution for linear, binary logistic, and multinoulli logistic regression, respectively. Furthermore, for all three algorithms we derived an identical rule (the LMS) for online weights update.\n",
      "\n",
      "The question is: how come we always get the same weights update rule? Also, what is the relationship between the logistic function and the Bernoulli variable, and between the softmax function and the multinoulli distribution? There seems to be a connection, because in both cases we obtained the cross-entropy error. The answer lies in the properties of the distributions we used to model the labels [math_placeholder | 158].\n",
      "\n",
      "\n",
      "The distributions we have encountered so far (Gaussian, Bernoulli, multinoulli), but also some others that are often used in machine learning (binomial, multinomial, Student's t-distribution, uniform, beta distribution, gamma distribution, Dirichlet's) belong to the so-called exponential family (hrv. eksponencijalna familija). What is an exponential family? The exponential family is a broad group of distributions that can be written in the following form:\n",
      "\n",
      "[math_placeholder | 159]\n",
      "\n",
      "The exponential family distributions have many properties that are important for machine learning, but mostly for probabilistic approaches to machine learning, so we won't go into further into that here.\n",
      "\n",
      "What is of interesting to us here is that it is that the exponential family is crucial for for generalized linear models. Specifically, for the distributions belonging to the exponential family (including Gaussian, Bernoulli, and multinoulli) there is a relationship between the distribution and its (possibly nonlinear) activation function [math_placeholder | 160]. This function is in this context is referred to as the mean function (hrv. funkcija sredine), because it defines the [math_placeholder | 161] parameter of a distribution, i.e., the distribution's mean. Thus, the activation function [math_placeholder | 162] therefore [math_placeholder | 163] as a function of [math_placeholder | 164]. You probably already guessed that for the Gaussian distribution the activation function is an identity function, since [math_placeholder | 165]. For the Bernoulli's distribution it is the logistic function, and for the multinoulli distribution it is the softmax function.\n",
      "\n",
      "After this inspiring topic, let's take a look at another no less inspiring thing ...\n",
      "\n",
      "\n",
      "In generalized linear models (for both regression and classification) we had the option to map examples to the feature space using a feature mapping function:\n",
      "\n",
      "[math_placeholder | 166]\n",
      "\n",
      "where [math_placeholder | 167] is a set of [math_placeholder | 168] basis functions (nonlinear functions of the input variables): [math_placeholder | 169]. For example, polynomial mapping for [math_placeholder | 170] and [math_placeholder | 171] :\n",
      "\n",
      "[math_placeholder | 172]\n",
      "\n",
      "We then easily incorporated such a mapping into any generalized linear model:\n",
      "\n",
      "[math_placeholder | 173]\n",
      "\n",
      "where [math_placeholder | 174] is a chosen activation function (i.e., the mean function, to make use of the term we just introduced).\n",
      "\n",
      "Although we haven't tried it, the basis functions [math_placeholder | 175] need not necessarily be potencies or factors of the input features, but rather these can really be any functions. One interesting possibility is to use functions that measure the similarity of an example with some prototype\n",
      "examples in the input space. This then is called a kernel machine, and we'll talk about that in two weeks.\n",
      "\n",
      "At any rate, the limiting factor is that these are fixed basis functions: their number and shape is predetermined. This is a problem because in most cases we do not know in advance which basis functions are good for our problem. In other words, we generally don't know which feature mapping will make our problem linearly separable in feature space.\n",
      "\n",
      "We can solve this problem by letting the basis functions adapt to our data (examples from the training set). Here we have two options: the first, used by the aforementioned kernel machines, is to select some examples from the training set as prototypes, and then make the basis functions measure the similarity between an input example and these prototypes. This adjusts the total number of basis functions depending on the data. Another possibility is to use a fixed number of basis functions, but let each of them adapt to the data. Let's look into that in a bit more detail.\n",
      "\n",
      "The idea of adaptive basis functions is to define them up to some parameters, which we can then adjust to the data. That is, we will define parameterized basis functions. Does that sound familiar? Of course. It is exactly the same as training machine learning models: we define a function up to some parameters, and the parameters are determined by optimizing the empirical error on the training set. But let's look first at how one could go about parameterizing the basis functions. One possibility is to say that each basis function is a small generalized linear model on its own! So, we will have a generalized linear model and within it we'll have generalized linear models as its basis functions:\n",
      "\n",
      "[math_placeholder | 176]\n",
      "\n",
      "Note that each basis function in our model should have its own weight vector, so weights [math_placeholder | 177] in the inner sum have two indices: [math_placeholder | 178] is the basis function index, while [math_placeholder | 179] is the index of the weight in the weight vector of basis function [math_placeholder | 180]. With superscripts [math_placeholder | 181] and [math_placeholder | 182] we indicated which weights are used first in the calculation of model prediction, and which are used second. The expression on the right is just a matrix notation of the model, where we managed to get rid of the sums and we combined the weights into a weight vector and weight matrix. Take some time to convince yourself that this notation is the same as the one with sums.\n",
      "\n",
      "Now that we have metabolized this, it's time for a big revelation. What kind of model is this actually? For each basis function there are weights from the matrix [math_placeholder | 183], which we multiply with all the input features and add them up. We then multiply these values again by weights [math_placeholder | 184] and add them up. We've built something most of you already know: a neural network!\n",
      "\n",
      "\n",
      "[image_placeholder | 5]\n",
      "\n",
      "\n",
      "This network of ours is two-layered, however nothing prevents us from going deeper: we can make the basis functions be combinations of other basis functions, and those again be combinations of yet another basis functions, etc.\n",
      "\n",
      "Obviously, neural networks are a more complex model than generalized linear models. This, of course, comes at a price: a more complex optimization procedure (due to non-convexity of\n",
      "the error function, since the loss in the output layer - which can still be the quadratic loss or cross entropy loss - now has a very complex dependence on the weights of the previous layers) and a greater possibility of model overfitting. Of course, various solutions have been proposed to tackle these issues, in particular within the now popular deep learning paradigm. We won't go any further, however, as this topic is receiving a rather comprehensive treatment in other courses.\n",
      "\n",
      "What matters here is to be aware of the connection: a neural network is an extension of a generalized linear model in which the basis functions are adaptive, i.e., the feature mapping function is also learned from the data.\n",
      "\n",
      "\n",
      "\n",
      "  Newton's method is a second-order optimization method that converges faster than the gradient descent, and is based on the calculation of the Hessian matrix. The logistic regression variant is called IRLS\n",
      "  The calculation of the Hessian matrix is expensive to compute in both time and space, so we may resort to a quasi-Newton method, such as L-BSFG\n",
      "  Multinomial logistic regression is a generalization of logistic regression to more than two classes, with softmax function as the activation function\n",
      "  Common to generalized linear models is that their outputs are variables from the exponential family distributions\n",
      "  Instead of using fixed basis function, we can use parameterized adaptive basis functions, which brings us to neural networks\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd377fa0",
   "metadata": {},
   "source": [
    "### 2. MathExpressionContext, requires MathExpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6a6eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "above consideration, we can conclude that the batch gradient descent could be improved if we take into account not only the slope (gradient) but also the curvature (the change in gradient, i.e., the second derivative) of the error function. Such optimization methods are referred to as second-order optimization, as opposed to first-order optimization methods, such as gradient descent. The basic second-order optimization method is the Newton's method.\n",
      "\n",
      "\n",
      "Consider minimization of function [math_placeholder | 1]. We know that the parameter update in gradient descent is as follows:\n",
      "\n",
      "[math_placeholder | 2]\n",
      "\n",
      "If we introduce an index for the iterations, then we can write this as an equation:\n",
      "\n",
      "[math_placeholder | 3]\n",
      "\n",
      "The idea with Newton's method is to take the point [math_placeholder | 4] (the current minimum) and compute at it the quadratic approximation of the function [math_placeholder | 5], and then move to the minimizer of this quadratic approximation (which is known analytically). If [math_placeholder | 6]\n"
     ]
    }
   ],
   "source": [
    "context_templates = TemplateContextChunkerUtil.chunk(template, max_context_size=1000)\n",
    "assert len(context_templates) == len(math_expressions)\n",
    "\n",
    "print(context_templates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e5d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathExpressionContext\n",
    "\n",
    "\n",
    "index_to_katex = {\n",
    "    math_expression.index: math_expression.katex for math_expression in math_expressions\n",
    "}\n",
    "math_expression_contexts: list[MathExpressionContext] = []\n",
    "\n",
    "for math_expression, context_template in zip(math_expressions, context_templates):\n",
    "    formatted_context, _ = TemplateFormatterUtil.format(\n",
    "        context_template, index_to_katex, omit_wrapper=False\n",
    "    )\n",
    "    math_expression_context = MathExpressionContext(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_id=math_expression.id,\n",
    "        math_expression_index_id=index.id,\n",
    "        text=formatted_context,\n",
    "    )\n",
    "    math_expression_contexts.append(math_expression_context)\n",
    "\n",
    "await math_expression_context_repository.insert_many(math_expression_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315a5bb",
   "metadata": {},
   "source": [
    "### 3. MathExpressionDescription, requires MathExpression, MathExpressionContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c764cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionDescriptionWriter as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionDescription\n",
    "from math_rag.infrastructure.constants.services import MATH_TEMPLATE\n",
    "\n",
    "\n",
    "inputs: list[AssistantInput] = []\n",
    "input_id_to_math_expression_id: dict[UUID, UUID] = {}\n",
    "\n",
    "for math_expression, math_expression_context in zip(math_expressions, math_expression_contexts):\n",
    "    input = AssistantInput(\n",
    "        katex=MATH_TEMPLATE.format(katex=math_expression.katex, index=math_expression.index),\n",
    "        context=math_expression_context.text,\n",
    "    )\n",
    "    inputs.append(input)\n",
    "    input_id_to_math_expression_id[input.id] = math_expression.id\n",
    "\n",
    "outputs = await math_expression_description_writer_assistant.concurrent_assist(inputs)\n",
    "math_expression_descriptions = [\n",
    "    MathExpressionDescription(\n",
    "        math_expression_index_id=index.id,\n",
    "        math_expression_id=input_id_to_math_expression_id[output.input_id],\n",
    "        text=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]\n",
    "await math_expression_description_repository.insert_many(math_expression_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "605301ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0f273a6e-37ce-482e-9d30-60e722f61113\n",
      "The number of classes in a classification problem is greater than two.\n",
      "-----\n",
      "\n",
      "688ca751-615d-4e3c-93c5-aff8fffb7fbf\n",
      "The error function expressed as a function of the vector variable w, conditioned on the dataset D.\n",
      "-----\n",
      "\n",
      "d9873aaa-b2f3-4436-8800-d61b2bb00329\n",
      "the dimension of the output vector produced by the softmax function, which matches the number of classes\n",
      "-----\n",
      "\n",
      "db1c0de7-de27-4af7-906a-344e35354250\n",
      "the dimensionality of the vector produced by the softmax function, corresponding to the number of classes\n",
      "-----\n",
      "\n",
      "afa448f5-9adb-4b16-a288-5773e6707376\n",
      "the point at which the quadratic approximation of the function is constructed and is tangential to the function during the minimization process\n",
      "-----\n",
      "\n",
      "20c561d7-0d6c-4c10-ba35-ad688ee04188\n",
      "The variable representing the number of features in the feature space after mapping.\n",
      "-----\n",
      "\n",
      "73463273-b6ea-47ad-83ab-5727406a30c5\n",
      "A general differentiable function of a single variable, referenced in the context of Taylor series expansion and quadratic approximation.\n",
      "-----\n",
      "\n",
      "d5ffda03-055c-4ea1-b389-422bda5be37c\n",
      "A general function evaluated at a vector variable, representing the objective to be minimized in the context of optimization methods such as gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "361bc0a1-4337-46c0-b029-1b3dae087e82\n",
      "The target expression defines the element in the i-th row and j-th column of the Hessian matrix of a function f as the second partial derivative of f with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "f891481f-75ed-47af-9fde-63464ccafee4\n",
      "the Hessian matrix of the function of a vector variable evaluated at the point denoted by the subscript t\n",
      "-----\n",
      "\n",
      "496eb4aa-5c58-40b8-bd79-a8650e3223fb\n",
      "the Hessian matrix, which is referenced in the context of Newton's optimization method and is discussed in terms of its positive semi-definiteness and positive definiteness properties\n",
      "-----\n",
      "\n",
      "0fd7653c-1f59-4022-948c-6d770121e193\n",
      "The number of dimensions of the feature space.\n",
      "-----\n",
      "\n",
      "4169742a-47bc-46dc-8c02-3699764eb94c\n",
      "The variable representing a specific class label, used as an index to distinguish among multiple classes in a multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "57ba572b-71ba-452d-b7fe-43e620b9d372\n",
      "The target expression states that the mean parameter of a distribution is given by the dot product of a weight vector and a feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "96c71a16-2739-4b4a-94f9-5bd172b80d8e\n",
      "The function being approximated by a quadratic at the point x sub 1 during the iterative minimization procedure.\n",
      "-----\n",
      "\n",
      "e7f1238f-3fff-4b58-9c97-8c2fe0478302\n",
      "The k-th parameter in the parameter vector representing probabilities for a categorical distribution is constrained to be greater than or equal to zero.\n",
      "-----\n",
      "\n",
      "dfde6aab-4cb3-4fe5-95fe-3cef6812c0f2\n",
      "The probability that the k-th indicator variable equals one, representing the probability that the multinomial variable takes its k-th state.\n",
      "-----\n",
      "\n",
      "050f1a3e-5806-492f-a990-78b7bdcf679e\n",
      "the point at which the function is being expanded in its Taylor series representation\n",
      "-----\n",
      "\n",
      "909aa4d3-8ff8-41f3-86a2-363b4058b26b\n",
      "the point about which the Taylor series expansion of a function is performed\n",
      "-----\n",
      "\n",
      "18df1c73-82c5-420a-9e2c-ff3b37518c86\n",
      "A linear combination of a weight vector and a feature mapping of an input vector, where the weight vector is transposed and multiplied by the feature vector of the input.\n",
      "-----\n",
      "\n",
      "6b700c3c-a402-41f3-aa0b-21a305a4513f\n",
      "the number of possible values or categories for the variable being modeled, which also determines the number of parameters needed in the parameter vector\n",
      "-----\n",
      "\n",
      "2141cefe-3c28-4bb5-b2f0-4ee10215382b\n",
      "the function being minimized, which depends on a single variable and is the subject of the quadratic approximation at the starting point\n",
      "-----\n",
      "\n",
      "c4c1f0f2-4a39-410c-bfd0-1d1927ae9c32\n",
      "This denotes the index of a specific class in a multiclass classification setting, used to indicate which class's weights or parameters are being referenced or updated.\n",
      "-----\n",
      "\n",
      "b37872a0-e0fb-4e2e-bda5-0d33dd3ff1ef\n",
      "the function whose convexity is being discussed in relation to the positive semi-definiteness of the Hessian matrix\n",
      "-----\n",
      "\n",
      "cf4956ac-d57e-4f59-bfe4-dc0d5cc867c6\n",
      "the point in n-dimensional space at which the function f is evaluated and at which the Hessian matrix is computed\n",
      "-----\n",
      "\n",
      "e3c05985-b221-428e-bc2f-1ac4ff5360da\n",
      "the norm of the parameter vector w approaches infinity\n",
      "-----\n",
      "\n",
      "bc5d4263-0c0e-478c-bcb1-4ae02f547ea1\n",
      "the same non-zero vector referenced in the condition for positive semi-definiteness of the Hessian matrix, used to test whether the quadratic form with the Hessian is non-negative\n",
      "-----\n",
      "\n",
      "eddd27c3-018b-48ff-8061-28fca4814bcc\n",
      "The target expression refers to the Hessian matrix used in the context of optimizing the cross entropy error for logistic regression, specifically as it appears in the weights update rule for Newton's method.\n",
      "-----\n",
      "\n",
      "504b0e94-b840-4ff7-bc2f-4bc219689896\n",
      "The norm associated with L1 regularization, which is not differentiable and thus requires the use of subgradients or specialized optimization methods such as coordinate descent or proximal algorithms.\n",
      "-----\n",
      "\n",
      "065031d4-1f11-4b4e-bbd4-e9316579de5b\n",
      "the quadratic function used to approximate the original function in the context of Newton's optimization method\n",
      "-----\n",
      "\n",
      "096deb8e-4453-497e-8e42-68c741007c2d\n",
      "the total number of classes considered in the multinomial logistic regression model\n",
      "-----\n",
      "\n",
      "99ab2b03-069f-4ef5-81b8-0fc005191cbd\n",
      "The Hessian matrix is expressed as the product of the transpose of a matrix denoted by Phi, a matrix S, and Phi itself.\n",
      "-----\n",
      "\n",
      "598293f6-868c-424b-b239-246b789e6a0e\n",
      "A mapping from n-dimensional real vectors to n-dimensional real vectors.\n",
      "-----\n",
      "\n",
      "fdcc8b99-3d8a-4fe6-beaf-96c5d9bd42d4\n",
      "the dimension of the input vector space for the function that the Hessian matrix is associated with, indicating that the function maps vectors with this many components to scalars\n",
      "-----\n",
      "\n",
      "a17bfd4b-7c48-454f-b510-bf9c0ed6695a\n",
      "The expression refers to the L2 regularization term, a penalty added to the loss function in optimization problems, which is based on the sum of the squared values of the model parameters.\n",
      "-----\n",
      "\n",
      "ed387666-cf13-49e3-a97e-7b57543b6f80\n",
      "the diagonal matrix whose diagonal entries are equal to the regularization factor, except possibly for the upper-left element, which may be zero because the corresponding weight is not regularized\n",
      "-----\n",
      "\n",
      "f38796d0-dd0f-4557-aee3-9a8a7933efa2\n",
      "The target expression refers to the type of regularization known as L2 regularization, which is being discussed in the context of its incorporation into optimization methods such as gradient descent, Newton's method, and least squares for linear regression.\n",
      "-----\n",
      "\n",
      "d6e38806-06f5-498d-91da-1fb9cb21c532\n",
      "the variable representing a non-zero vector used in the condition that a matrix is positive definite, specifically in the inequality involving the transpose of the vector, the matrix, and the vector itself\n",
      "-----\n",
      "\n",
      "212d1334-e4e5-4146-ae8a-2bfe55ca48f5\n",
      "The target expression denotes the model corresponding to class k in the context of multinomial logistic regression, where each such model is responsible for predicting the probability that an input belongs to class k.\n",
      "-----\n",
      "\n",
      "31f4a47b-aedd-4068-b246-9ed6f239f999\n",
      "The variable representing the current value of the parameter vector at iteration t in an iterative optimization algorithm.\n",
      "-----\n",
      "\n",
      "8e55cce3-8de3-4ea8-bde9-c4f0e9829d00\n",
      "the activation function that is applied to the scalar product of the weight vector and the feature vector in the definition of a generalized linear model\n",
      "-----\n",
      "\n",
      "54a1da96-2e9f-4f05-bc40-b584eb0f511d\n",
      "A function of a vector variable x, representing a general multivariate function that is being considered for a quadratic (second-order) Taylor expansion about a specific point.\n",
      "-----\n",
      "\n",
      "a4802e86-0db2-4942-9bcd-f7c55a6b7db1\n",
      "the total number of examples in the training set\n",
      "-----\n",
      "\n",
      "59571f9a-355c-482a-9fad-5ffc21525e4f\n",
      "The variable y is a binary label that can take the value 0 or 1, indicating it is a Bernoulli random variable used in the context of binary logistic regression.\n",
      "-----\n",
      "\n",
      "9a68b12c-b715-4664-8dc2-83a8e3de4348\n",
      "the total number of classes considered in the generalization of the cross-entropy error and loss function\n",
      "-----\n",
      "\n",
      "6472cf1b-2216-436a-b670-60903d22caf1\n",
      "The probability that the k-th indicator variable in the vector representation of a multinoulli variable equals one, meaning the probability that the multinoulli variable takes the k-th outcome.\n",
      "-----\n",
      "\n",
      "0c2cedea-8ae5-4803-9214-04ebdeaaafec\n",
      "The target expression represents the function of a single variable, depicted as the black curve in the referenced image, which is being minimized using Newton's method.\n",
      "-----\n",
      "\n",
      "86dc5c0d-e597-43d3-b304-74aef14c9ac5\n",
      "The set of models indexed by k, where each model corresponds to a specific class in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "bd9be618-caeb-4389-85d0-e930051d4c41\n",
      "The total number of classes in the multinomial logistic regression model being discussed.\n",
      "-----\n",
      "\n",
      "e9788431-949f-465c-9a6c-b32a973c8c75\n",
      "the feature vector representing a single example whose class membership probability is being modeled\n",
      "-----\n",
      "\n",
      "fbe3860d-31d7-457e-94d9-18d8909f365e\n",
      "the function being minimized, which is approximated quadratically at a given point in the optimization process\n",
      "-----\n",
      "\n",
      "46216354-0560-4066-b086-d693a8e7a613\n",
      "The target expression denotes the activation function used in the context of generalized linear models, which serves as the mean function mapping the linear predictor to the mean parameter of a distribution from the exponential family.\n",
      "-----\n",
      "\n",
      "f53a96ea-a5ce-4aaa-89e4-acb731161916\n",
      "This expression denotes a function that takes as input a vector from n-dimensional real space and outputs a real number; in other words, it is a real-valued function defined on n-dimensional real vectors.\n",
      "-----\n",
      "\n",
      "f5e40c6d-eae1-46da-a00b-6c98ca4d6e5c\n",
      "A rightwards arrow symbol used to indicate the start of an example section in the text.\n",
      "-----\n",
      "\n",
      "72765d41-20bc-4b44-b48d-36325a0bee02\n",
      "The model corresponding to a specific class index, which outputs the probability that an input belongs to that class, as defined by the softmax function and parameterized by the associated weight vector.\n",
      "-----\n",
      "\n",
      "4894ad47-2237-490d-a05c-2c766e2789a7\n",
      "the index of a component in the output vector of the softmax function, representing a particular class among K possible classes\n",
      "-----\n",
      "\n",
      "a4681cf1-062a-4c2c-85a1-7ee320f1f692\n",
      "the class index for which the probability of example x is being computed in the multinomial logistic regression model\n",
      "-----\n",
      "\n",
      "6b7d3c5f-21bb-4a92-9a53-64c9cd124d43\n",
      "the output of the logistic function evaluated at the input vector x, representing the predicted probability for a given example in logistic regression\n",
      "-----\n",
      "\n",
      "c78a8f17-fdc4-424b-826f-e238949d8214\n",
      "the index that identifies a specific basis function within the model, used to distinguish between different basis functions and their associated weight vectors\n",
      "-----\n",
      "\n",
      "f1fa44ab-907a-45df-a375-57015abccae2\n",
      "The sum over all components of the indicator vector representing the categorical variable equals one, reflecting that exactly one outcome occurs among the mutually exclusive and collectively exhaustive possible states.\n",
      "-----\n",
      "\n",
      "dce9db8c-413d-4408-a8c0-4329b8c90825\n",
      "the parameter representing the probability of success in the Bernoulli distribution, used to define the probability of a binary outcome variable\n",
      "-----\n",
      "\n",
      "a0e3c236-dec2-447a-904e-d935d0820aaf\n",
      "the dimensions of the Hessian matrix, where the matrix is square and each side has a length equal to m plus one\n",
      "-----\n",
      "\n",
      "3aeaa1c4-8ad4-43ac-82fa-76d67b9ca64e\n",
      "the index used to denote a specific example in a dataset, typically ranging from 1 to N, where N is the total number of examples\n",
      "-----\n",
      "\n",
      "81a9745b-17f7-4908-81be-8311ffb94f57\n",
      "The matrix referenced, which is the Hessian matrix, is positive semi-definite but not positive definite, and therefore does not have an inverse, making it unsuitable for use in Newton's method as described in the context.\n",
      "-----\n",
      "\n",
      "c4aa5a12-2b5f-4acb-9136-980aae70aa0f\n",
      "the input example vector to the softmax function, representing a data point in the context of multiclass classification\n",
      "-----\n",
      "\n",
      "fddf82a7-f315-4641-ad69-74a0df07c433\n",
      "the index that specifies the position of a weight within the weight vector associated with a particular basis function\n",
      "-----\n",
      "\n",
      "1f75e399-fab3-4a13-b9e5-08b9ed116ea3\n",
      "The target expression denotes the collection of m basis functions, each labeled with a subscript from 1 to m, which are used in the feature mapping of input variables in generalized linear models.\n",
      "-----\n",
      "\n",
      "00c97125-cb15-45b6-ab95-ace8d47284c4\n",
      "the input vector corresponding to the i-th training example, as used in the context of linear regression and stochastic gradient descent\n",
      "-----\n",
      "\n",
      "9b510b66-07c2-473e-a777-065221ea672e\n",
      "the dimensions of a matrix with m plus one rows and N columns, intended to be multiplied by a matrix with N rows\n",
      "-----\n",
      "\n",
      "d596a030-d07c-4074-a786-e240a9cf2cbf\n",
      "The variable representing the index of a specific class in a multiclass classification setting, where there are a total of K classes.\n",
      "-----\n",
      "\n",
      "645b4d87-68de-4d9a-9fc2-bd0e87a26080\n",
      "The target expression denotes the number of weight vectors in the matrix, which corresponds to the number of classes in the multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "6bc45875-009e-4a4e-a4e9-df87a4eafc7a\n",
      "The number of input variables or features in the context of the feature mapping example, specifically set to two.\n",
      "-----\n",
      "\n",
      "37e02e27-fa43-4d36-a3a8-4c492ff17ed1\n",
      "The target expression denotes the dimensions of a matrix multiplication, where a matrix with N rows is multiplied by a matrix with N rows and m plus one columns.\n",
      "-----\n",
      "\n",
      "01305079-5e86-4f4c-a8da-436472550abf\n",
      "the total number of classes considered in the context of the softmax function, representing the dimensionality of the class score vector\n",
      "-----\n",
      "\n",
      "d455a5c9-be59-46b4-a224-3a380fd00f8d\n",
      "The target expression denotes the j-th basis function used in the mapping of input variables, which can be any function (not necessarily a polynomial or product of input features) applied to the input, as part of a generalized linear model.\n",
      "-----\n",
      "\n",
      "99b5f55a-62da-40a9-acb9-466bc48dcc91\n",
      "The target expression represents a matrix product where the transpose of a matrix, denoted as Phi, is multiplied on the left by a matrix S and on the right by the original matrix Phi. This product is used as the Hessian matrix in the context of logistic regression and Newton's method optimization.\n",
      "-----\n",
      "\n",
      "9617dd7f-b27a-4d10-8138-7a39809e261d\n",
      "The point reached after the first step of minimizing the quadratic approximation of the function, following the initial point, as illustrated in the described iterative minimization process.\n",
      "-----\n",
      "\n",
      "596ed236-5651-43ac-93db-22428c7efac9\n",
      "the integer immediately less than the class index k, representing the number of other models corresponding to the remaining classes except class k\n",
      "-----\n",
      "\n",
      "ff8ec75f-7c82-4d0f-a71d-296807e59033\n",
      "a square matrix with both the number of rows and columns equal to n\n",
      "-----\n",
      "\n",
      "f7def4aa-81d0-40c6-a52b-b4e6d5dc2d7b\n",
      "the weight parameter corresponding to the upper-left element of the weight vector, which is excluded from regularization in the context of L2 regularization\n",
      "-----\n",
      "\n",
      "749d819b-bc5e-4606-bc86-1fb3eec78fae\n",
      "the index that identifies a specific basis function within the model, used to distinguish between different basis functions and their associated weights\n",
      "-----\n",
      "\n",
      "193b3918-c4aa-4ad1-856e-12e02544a1fc\n",
      "The value representing the negative class label in binary logistic regression, used to distinguish it from the positive class label.\n",
      "-----\n",
      "\n",
      "7b910d83-03b6-4291-822f-1b13ff72f01f\n",
      "The expression indicates that when the model's predicted probability for a given class is close to one, the natural logarithm of this probability is approximately zero.\n",
      "-----\n",
      "\n",
      "57045e07-da5d-4e89-861f-1ac41494e59c\n",
      "The dimension of the Hessian matrix, which is a square matrix with both its number of rows and columns equal to one more than the number of features in the feature space.\n",
      "-----\n",
      "\n",
      "f900c292-2ff9-460c-aa9f-695f84ebacef\n",
      "The target expression denotes the mean parameter of a distribution, expressed as a function of the linear combination of the weight vector transpose and the feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "360a8917-f1c0-4d37-913b-6fb24f598e2a\n",
      "The target expression denotes a matrix composed of K column vectors, where each column is a weight vector associated with one of the K classes in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "3c500691-30d9-40bc-a74f-2bcd0aaab850\n",
      "the activation function that serves as the mean function in generalized linear models, relating the mean parameter of an exponential family distribution to the linear predictor\n",
      "-----\n",
      "\n",
      "7d6f6e4f-142a-4890-9c31-3f221524a1a4\n",
      "the Hessian matrix associated with the cross-entropy error function, whose dimension is (m+1) by (m+1), where m is the number of features in the feature space after mapping\n",
      "-----\n",
      "\n",
      "53483643-1f68-4d62-9021-dd177afc820f\n",
      "The matrix denoted by this symbol is the Hessian matrix for the cross-entropy error in logistic regression, and it has dimensions equal to the number of features plus one, squared; that is, it is a square matrix of size (m+1) by (m+1), where m is the number of features in the feature space after mapping.\n",
      "-----\n",
      "\n",
      "a896a52b-1d54-434c-9cb0-379a2f5ed0b5\n",
      "The label indicator for example i and class k, which takes the value 1 if example i belongs to class k and 0 otherwise.\n",
      "-----\n",
      "\n",
      "976cf911-4611-467b-9186-5537673a5a3d\n",
      "the mean parameter of a distribution in the context of the exponential family, as defined by the mean function (activation function)\n",
      "-----\n",
      "\n",
      "19e5dd00-ac49-427b-96c3-f1b4e2dd61b2\n",
      "The target expression denotes the class label corresponding to the positive class in binary logistic regression, as contrasted with the class label for the negative class, which is also mentioned in the context.\n",
      "-----\n",
      "\n",
      "5ca84e79-c4e1-466f-988d-191884864317\n",
      "The sum over all components of the parameter vector mu equals one, indicating that the parameters represent a probability distribution over K categories.\n",
      "-----\n",
      "\n",
      "82ae5d07-a024-48cd-88ee-a2ed84bbaabc\n",
      "The Hessian matrix of the cross-entropy error function, specifically in the context of logistic regression, which is discussed in terms of its positive definiteness and invertibility.\n",
      "-----\n",
      "\n",
      "8b0b5581-03dd-4636-aade-b4025b0d2d81\n",
      "the value corresponding to one of the two possible outcomes of a Bernoulli random variable, specifically the outcome that is not the 'success' case\n",
      "-----\n",
      "\n",
      "a8b76fdc-1104-4764-8b24-437283f4bab9\n",
      "the function being minimized in the context of Newton's method, evaluated at a variable x\n",
      "-----\n",
      "\n",
      "0f972452-a92d-4252-9516-6a032bbf88db\n",
      "The label corresponding to the i-th data point in a dataset, as referenced in the context of probabilistic modeling and distributions used for labels.\n",
      "-----\n",
      "\n",
      "6c0ed2b1-06ac-47f9-b959-ef55658eaada\n",
      "the index corresponding to the outcome of a multinomial variable, such that the k-th component of the indicator vector is one if the outcome is this value\n",
      "-----\n",
      "\n",
      "71de3bc9-54b7-4fa6-8921-04d821a3d418\n",
      "the weight vector associated with the k-th class in a multinomial logistic regression model, where a separate such vector is used for each class\n",
      "-----\n",
      "\n",
      "b7aa922c-f344-470f-a782-e92ad56548c9\n",
      "the Hessian matrix whose invertibility is required for applying Newton's optimization method, specifically in the context where it is positive semi-definite but not positive definite, resulting in the absence of an inverse\n",
      "-----\n",
      "\n",
      "d8f1d384-7b73-4cf6-8786-40cde0d6b88c\n",
      "The model referenced, used in multinomial logistic regression, is defined as a collection of models indexed by class, where each individual model outputs the probability of an input belonging to a specific class using the softmax function.\n",
      "-----\n",
      "\n",
      "01b66cce-343d-4679-9d2b-55c78f3debd4\n",
      "the index representing a particular class in a multi-class classification setting, where the total number of classes is denoted by an uppercase variable\n",
      "-----\n",
      "\n",
      "538143a4-852b-4960-ac78-7c7cb897e38c\n",
      "the regularization factor that appears on the diagonal of the regularization matrix, except for the upper-left element, in the context of L2 regularization for logistic regression\n",
      "-----\n",
      "\n",
      "92780f98-7af7-4aa9-9eee-7aedb96697fb\n",
      "The function of interest is a function of a vector variable, representing a function defined on a multidimensional space, and is referenced in the context of optimization methods and convexity, specifically in relation to logistic regression.\n",
      "-----\n",
      "\n",
      "2b0ad44c-c066-4326-af76-54cef755e7c3\n",
      "A function, denoted by a single letter, referenced in the context of optimization methods such as gradient descent and Newton's method, and described as potentially being a function of one variable.\n",
      "-----\n",
      "\n",
      "b304b291-67bf-412e-bbc7-71c8f3d9659c\n",
      "The variable represents the number of basis functions used in the feature mapping, as referenced in the context of mapping input variables to a higher-dimensional feature space in generalized linear models.\n",
      "-----\n",
      "\n",
      "f2989b32-d95a-478f-b75a-0779f11ad5d5\n",
      "the function being minimized, which is approximated quadratically at each iteration to determine the next search point\n",
      "-----\n",
      "\n",
      "3267f67e-ab1c-4746-9192-2d6b7afc1e34\n",
      "The target expression denotes the model corresponding to class k in a multinomial logistic regression framework, where each such model is responsible for predicting the probability that a given input belongs to class k.\n",
      "-----\n",
      "\n",
      "b8c337f7-f3fb-409b-b2da-18c43d20a98c\n",
      "The target expression represents the result of taking the dot product between the weight vector associated with class k and the input example vector, producing a scalar value that serves as the input to the softmax function for class k.\n",
      "-----\n",
      "\n",
      "80e127aa-49d0-4d9a-b8f1-4e1eeccdae73\n",
      "The target expression denotes the weight vector associated with class k, which is one of the K weight vectors that together form the weight matrix used in the softmax regression model.\n",
      "-----\n",
      "\n",
      "314025f5-8d62-433e-8289-866edbd895c4\n",
      "A four-dimensional indicator vector representing a multinomial variable in which only the third component is one and all others are zero, signifying that the third state out of four possible states has been selected.\n",
      "-----\n",
      "\n",
      "3a95e54c-f995-4841-ba7c-6d2baf83ea7e\n",
      "The expression defines the k-th component of the softmax function applied to a vector of n real numbers, where the output is the exponential of the k-th input divided by the sum of exponentials of all inputs.\n",
      "-----\n",
      "\n",
      "5a408f7d-d37d-4315-990c-a509fec111e2\n",
      "A function that takes an n-dimensional real vector as input and outputs a real number, representing the j-th basis function in a set of m basis functions used for feature mapping in generalized linear models.\n",
      "-----\n",
      "\n",
      "84ad5faf-2555-463b-9aa8-0415c7e7a3e5\n",
      "A column vector whose entries are indicator variables for a categorical variable with K possible outcomes, where each entry corresponds to one outcome and is set to one if that outcome occurs and zero otherwise.\n",
      "-----\n",
      "\n",
      "3a546563-936e-4a4a-8dc3-c6f6f39bc818\n",
      "The matrix referred to as the Hessian matrix of the cross-entropy error function in logistic regression, which can be decomposed as the product of the transpose of a feature matrix, a diagonal matrix, and the feature matrix itself; it may be ill-conditioned due to multicollinearity.\n",
      "-----\n",
      "\n",
      "1c65f181-9d0c-41ca-8c79-6b089eefadeb\n",
      "A vector containing K parameters, each denoted by a subscripted mu, representing the probabilities associated with each of K possible outcomes in a categorical distribution.\n",
      "-----\n",
      "\n",
      "ad327783-49e5-4d68-a4ee-0ef20184252d\n",
      "the initial point from which the minimization process starts in the context of minimizing a function of one variable using Newton's method\n",
      "-----\n",
      "\n",
      "658b514f-eb6f-4f95-a4e3-2b352072e37e\n",
      "The target expression defines a general model output as the result of applying an activation function to the dot product of a weight vector and a feature transformation of the input vector, parameterized by the weights.\n",
      "-----\n",
      "\n",
      "297e7394-92d0-44c8-8510-82e3c9a38350\n",
      "The expression gives the probability of a categorical outcome vector, where the probability is computed as the product over all classes of the class probability parameter raised to the power of the corresponding indicator variable for that class. The parameters are non-negative and sum to one, representing class probabilities, and the outcome vector uses one-hot encoding.\n",
      "-----\n",
      "\n",
      "41e78d64-caf5-4b75-8f7c-7b70a63cfe3a\n",
      "The k-th component of the parameter vector representing the probability assigned to the k-th category in a categorical distribution, where all such components are non-negative and sum to one.\n",
      "-----\n",
      "\n",
      "0be8bf93-c0c1-4660-aba0-2ab6aacf886f\n",
      "A five-dimensional vector whose components are 1, 2, 0.5, 5, and 3, used as an input example for the softmax function in the given context.\n",
      "-----\n",
      "\n",
      "43047fea-a562-4213-8d82-610d20a62d1d\n",
      "The condition that, for every non-zero vector, the quadratic form defined by the Hessian matrix is strictly greater than zero, which characterizes the matrix as positive definite.\n",
      "-----\n",
      "\n",
      "1201f373-0765-4541-8569-6fde165d24f7\n",
      "The target expression asserts that the number of possible values, denoted by the variable K, is greater than two, indicating a generalization from the binary case to a case with more than two categories.\n",
      "-----\n",
      "\n",
      "7ac240af-7e6c-4181-8179-3cdeae6cde54\n",
      "The value representing the second possible outcome of a Bernoulli random variable, where the variable can take on either zero or one.\n",
      "-----\n",
      "\n",
      "0bc9c24a-e01e-4981-aa04-115b5c029f14\n",
      "This expression represents the second-order Taylor approximation of a differentiable function at a point, using the value of the function, its first derivative, and its second derivative at that point, and is obtained by truncating the Taylor series after the quadratic term.\n",
      "-----\n",
      "\n",
      "ed796e75-bb5f-4340-a2fe-7aeb610ac667\n",
      "This expression denotes that the k-th component of the indicator vector representing a categorical (multinoulli) variable is zero, which occurs when the outcome of the variable is not the k-th category.\n",
      "-----\n",
      "\n",
      "3c2c99e9-00e2-4c2c-98e0-b674c4feb0a4\n",
      "The regularized Hessian matrix is defined as the sum of the original Hessian matrix and a scaled identity matrix, where the scaling factor is the regularization parameter.\n",
      "-----\n",
      "\n",
      "5738ff44-0894-486a-afc3-eacbda1101e0\n",
      "The target expression refers to the regularization term based on the squared L2 norm, which is added to the loss function or gradient in the context of regularized logistic regression.\n",
      "-----\n",
      "\n",
      "0b636160-e323-4900-b60a-e768810bfa90\n",
      "The target expression denotes the Hessian matrix referenced in the discussion of Newton's optimization method, specifically the matrix whose positive definiteness is required for the existence of its inverse and the applicability of Newton's method.\n",
      "-----\n",
      "\n",
      "219f980b-65a5-4200-b5ab-5af48c4c56c4\n",
      "the weight parameter associated with the i-th input variable in the weight vector of the j-th basis function within the inner sum of a generalized linear model that uses basis functions, where each basis function has its own set of weights\n",
      "-----\n",
      "\n",
      "57840bc5-c4b6-4418-aaa3-cc4eb3f0d4b8\n",
      "The target expression denotes a class label index, representing one of the possible classes in a classification problem with K classes. It is used to indicate the specific class to which an example may belong.\n",
      "-----\n",
      "\n",
      "54357f82-2224-4432-97c1-c55a27a41123\n",
      "The expression represents the gradient of the error function with respect to the weight vector for class k, where the error function depends on the weight matrix and the dataset. The gradient is given as a sum over all N data points, where for each data point, the difference between the model's output for class k and the true label for class k is multiplied by the feature vector of the data point.\n",
      "-----\n",
      "\n",
      "64da4006-ef51-4e0e-bfd8-5c62d7bb8be4\n",
      "The target expression defines the quadratic error function for linear regression, representing the sum over all N data points of the squared difference between the predicted value (obtained by applying a weight vector to a feature transformation of the input) and the actual target value, scaled by one half.\n",
      "-----\n",
      "\n",
      "03fd3242-c70d-4fc0-a4f8-31525a0db677\n",
      "This denotes a superscript label used to distinguish the first set of weights in the model, specifically those used earlier in the calculation of the model prediction.\n",
      "-----\n",
      "\n",
      "484dc562-8b8b-4d22-b88a-d743a95be2d8\n",
      "The expression refers to the type of regularization known for involving the sum of the absolute values of the model parameters, commonly associated with promoting sparsity in solutions and known for its non-differentiability, which complicates gradient-based optimization methods.\n",
      "-----\n",
      "\n",
      "55bbaa25-7f7f-4da8-ae79-73745124a0cd\n",
      "This expression states that the number of possible output classes, denoted by an uppercase variable, is greater than two, indicating a multiclass scenario rather than a binary one.\n",
      "-----\n",
      "\n",
      "f4624147-f06a-4d8b-a630-a49e2acb7e9c\n",
      "The target expression denotes the input example vector whose class membership probability is being modeled by the multinomial logistic regression model. It is the same vector for which the softmax function is computed and is used as the argument to the model functions.\n",
      "-----\n",
      "\n",
      "681df0a3-2761-4259-81ac-2a6a2cbb6447\n",
      "This expression represents the parameter update rule in gradient descent, where the current value of the parameter vector is updated by subtracting the product of a learning rate and the gradient of a function evaluated at the current parameter value.\n",
      "-----\n",
      "\n",
      "6824dd86-1ee4-4a6f-ae00-1260b52616f4\n",
      "The scalar product of the weight vector associated with the k-th class and the input vector, representing the unnormalized score for that class in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "abb8404b-df9c-4fe1-ae5c-e25435ad57d6\n",
      "The k-th component of the indicator vector representing the categorical variable is equal to one, indicating that the outcome corresponds to the k-th category.\n",
      "-----\n",
      "\n",
      "025032ca-174b-47a8-b002-3a769c8f1d3b\n",
      "This expression defines the Hessian matrix of a scalar-valued function of n variables as a square matrix whose entries are the second-order partial derivatives of the function with respect to each pair of variables, arranged so that the entry in the i-th row and j-th column corresponds to the second partial derivative with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "90ecfc70-bd28-4290-89b2-b70ea8ffba11\n",
      "This expression defines the general form of a probability distribution that belongs to the exponential family, where the probability of a variable given parameters is expressed as a product of a base measure and an exponential function involving the parameters, sufficient statistics, and a log-partition function.\n",
      "-----\n",
      "\n",
      "33693440-1596-4e1d-9bd2-1f02c513fb3b\n",
      "A diagonal matrix whose diagonal entries are given by the product of the logistic function evaluated at each training example and one minus that value, for all examples in the training set.\n",
      "-----\n",
      "\n",
      "e1db75f8-0ec8-41cc-813e-037cae07d78d\n",
      "The function of interest is a real-valued function that takes an n-dimensional vector as input and is used in the context of defining its Hessian matrix, which consists of all second-order partial derivatives of the function with respect to its input vector components.\n",
      "-----\n",
      "\n",
      "89e30aa1-c47a-4ebb-8e22-905a2690fc4f\n",
      "The gradient of the regularized error function with respect to the parameter vector, given the dataset, is equal to the gradient of the unregularized error function with respect to the parameter vector, given the dataset, plus the product of the regularization parameter and the parameter vector.\n",
      "-----\n",
      "\n",
      "071aab3a-d78d-4a9c-953b-f3161a917e21\n",
      "The target expression gives the update rule for the parameter vector at iteration t plus one, where the new value is obtained by subtracting from the current value a scaled product of the inverse of the Hessian matrix at iteration t and the gradient of the function evaluated at the current parameter vector. The scaling factor is denoted by eta.\n",
      "-----\n",
      "\n",
      "033bff2a-ddad-417c-9281-e7b51164d08f\n",
      "The symbol in question denotes the Hessian matrix, which in this context is associated with the cross-entropy error function, and is being discussed in terms of its positive semi-definiteness and invertibility properties in logistic regression.\n",
      "-----\n",
      "\n",
      "daffb72d-c861-4ae6-ae6f-cabefa790cd6\n",
      "The target expression specifies the degree parameter, set to two, for the polynomial mapping example where the input space has two variables.\n",
      "-----\n",
      "\n",
      "2d453a41-42ed-4cd3-9e79-197ef96dc2e2\n",
      "This expression represents the weight update rule in stochastic gradient descent for logistic regression, where the current weight vector is adjusted by subtracting the product of the learning rate, the difference between the model's prediction and the true label for a specific example, and the feature vector of that example.\n",
      "-----\n",
      "\n",
      "a674b09e-49b2-4064-a712-1b39d5f6648c\n",
      "The expression asserts that, for every non-zero vector, the quadratic form defined by the Hessian matrix is greater than or equal to zero, which is the condition for the Hessian matrix to be positive semi-definite.\n",
      "-----\n",
      "\n",
      "39634994-f563-43b8-9639-bd441c0ace96\n",
      "The parameter is set to the value one, indicating a full step size in the context of the parameter update rule for Newton's method when the Hessian matrix is computed exactly.\n",
      "-----\n",
      "\n",
      "32edba48-8bdc-445b-b20c-d3944b72e578\n",
      "The target expression gives the result of applying the softmax function to the vector with components one, two, zero point five, five, and three, yielding a five-dimensional vector of probabilities: approximately one point five percent, four point one percent, zero point nine percent, eighty-two point three percent, and eleven point two percent.\n",
      "-----\n",
      "\n",
      "f8785169-9b49-4d8a-b9af-30d3567daf4b\n",
      "The target expression denotes a type of regularization, specifically the one commonly referred to as L2 regularization, which is being discussed in the context of modifying optimization algorithms such as Newton's method for logistic regression. It is mentioned as a form of regularization that can be added to the error function and its derivatives to help prevent overfitting.\n",
      "-----\n",
      "\n",
      "40d67f52-34f1-46b8-89a1-ad04f21d1505\n",
      "This superscript is used to distinguish between two different sets of weights in the model, specifically indicating the set of weights that are used second in the calculation of the model prediction.\n",
      "-----\n",
      "\n",
      "f8def64a-7231-41b3-8e9c-660c202d4908\n",
      "This expression presents the key components of the logistic regression algorithm: the model output as the sigmoid of a linear combination of features, the probability distribution for the label given the input and weights, the cross-entropy loss function, and the gradient of the loss with respect to the weights.\n",
      "-----\n",
      "\n",
      "258283b5-3f62-43bd-bf12-71ce1ff8d8cf\n",
      "This expression defines the output of a binary logistic regression model as the probability that the class label equals one, given an input vector and a weight vector. It shows that this probability is computed by applying the sigmoid function to the inner product of the weight vector and a feature transformation of the input.\n",
      "-----\n",
      "\n",
      "07057317-9c6e-4c6d-b2a0-25b0656dbaec\n",
      "The target expression denotes the activation function used in a generalized linear model, which is applied to the linear combination of weights and feature-mapped inputs to produce the model's output.\n",
      "-----\n",
      "\n",
      "7a58773b-2493-4ec6-8fd6-40c99d0ca5c5\n",
      "The target expression gives the log-likelihood of the observed labels given the inputs, expressed as the sum over all examples and classes of the product of the label indicator and the logarithm of the predicted class probability, where the predicted probability is parameterized by the model weights.\n",
      "-----\n",
      "\n",
      "bbad5832-a923-4718-bc39-7f78e92ebcb9\n",
      "The term refers to a type of regularization mentioned in contrast to another type, specifically in the context of modifying optimization methods such as gradient descent, Newton's method, and least squares for linear regression. It is associated with a regularization term that is added to the loss function and affects the gradient and Hessian, as described in the surrounding context.\n",
      "-----\n",
      "\n",
      "66eefc4b-066c-46cc-abd7-750df54e681b\n",
      "the weight matrix associated with the first layer of the model, where each basis function has a corresponding set of weights that are multiplied with all input features and summed\n",
      "-----\n",
      "\n",
      "311f4660-ccbe-4e7f-811f-d4d845f9bad3\n",
      "The matrix denoted by the bold capital H at this index refers to the Hessian matrix associated with the cross-entropy error in logistic regression, which is stated to be positive semi-definite in this context.\n",
      "-----\n",
      "\n",
      "61510db0-ead7-4ed5-b3d8-69fa29183ffa\n",
      "This expression defines the loss function for a single example in a multi-class classification setting, where the loss is computed as the negative sum over all classes of the product of the true label indicator for each class and the logarithm of the predicted probability for that class, with the predicted probabilities parameterized by a model with weights.\n",
      "-----\n",
      "\n",
      "a42708ec-f790-4bb6-b0d2-16e54c30e188\n",
      "The target expression denotes the dataset consisting of input-output pairs, specifically the collection of all feature vectors and their corresponding labels, which are used to compute the probability of the labels and to define the error function in the context of multiclass classification.\n",
      "-----\n",
      "\n",
      "be4713b7-2c6e-4a42-b81a-a3f5d6ff8bd8\n",
      "the point in the domain of the multivariate function f, about which the quadratic (second-order) Taylor expansion is performed; it is a vector and serves as the expansion point in the context of the Taylor series for f of a vector variable\n",
      "-----\n",
      "\n",
      "d64be2d2-35cc-44ef-9bf8-8447d4b09c01\n",
      "The expression represents the iterative update rule for gradient descent, where the value of the variable at the next iteration is obtained by subtracting the product of a step size and the gradient of the function evaluated at the current value from the current value itself. The subscript denotes the iteration index.\n",
      "-----\n",
      "\n",
      "e4c7cb9f-6686-4630-a8e3-18cc51042a46\n",
      "The target expression defines a generalized linear model output as a function that applies an activation function to the inner product of a weight vector and a feature-mapped input vector, which is equivalently expressed as the activation function applied to the sum over all weights multiplied by their corresponding basis functions evaluated at the input.\n",
      "-----\n",
      "\n",
      "72bb00cf-cf57-4c83-98c6-a92a43dd59f2\n",
      "The target expression represents the value of a function evaluated at a variable x, which is the subject of the Taylor series expansion and its quadratic approximation about the point a, as discussed in the surrounding context.\n",
      "-----\n",
      "\n",
      "13cd319a-1a1f-4a7c-97f2-4ac8596fdedd\n",
      "This expression defines a specific feature mapping for a two-dimensional input vector, where the output is a six-dimensional vector consisting of the constant one, the two input components, their product, and the squares of each component. This mapping corresponds to a polynomial feature expansion of degree two for two input variables.\n",
      "-----\n",
      "\n",
      "af1eca0b-3f8c-41cb-a499-a4b698ef8edd\n",
      "The target expression defines a feature mapping function that takes an n-dimensional real vector as input and outputs an (m+1)-dimensional real vector, where the first component is 1 and the remaining m components are given by applying m basis functions to the input vector.\n",
      "-----\n",
      "\n",
      "08ecb7e8-ce97-4ae6-b222-f10c68edb9fe\n",
      "The target expression denotes the L one norm, which is referenced in the context as a type of regularization used in logistic regression. It is mentioned in relation to its non-differentiability and the need for specialized optimization methods such as coordinate descent or proximal algorithms.\n",
      "-----\n",
      "\n",
      "b454c06d-4df1-4e8d-8480-70f3e97c82d5\n",
      "The target expression defines the Hessian matrix for the cross-entropy error in logistic regression as the product of the transpose of a matrix of features, a diagonal matrix whose diagonal entries are the first derivatives of the logistic output for each training example, and the matrix of features itself.\n",
      "-----\n",
      "\n",
      "b044e98c-89da-4b4c-a397-d703e981bf09\n",
      "The target expression defines the probability that an input example belongs to a particular class in multinomial logistic regression, computed as the exponential of a class-specific linear function of a feature transformation of the input, divided by the sum of such exponentials over all classes, and is also interpreted as the softmax output for that class.\n",
      "-----\n",
      "\n",
      "c453729b-2113-48d1-af1d-026e7610805b\n",
      "This expression specifies the update rule for the weight vector in Newton's method as applied to logistic regression, where the new weight vector is obtained by subtracting the product of the inverse of the Hessian matrix of the weights and the gradient of the error function (with respect to the weights, given the dataset) from the current weight vector, with the step size set to one.\n",
      "-----\n",
      "\n",
      "bae03497-1d70-449d-87ca-9d12a2c889fe\n",
      "The target expression represents the gradient of the quadratic error function for linear regression, evaluated with respect to the weight vector, for a single training example. It is given as the product of the difference between the model's prediction and the true target value for that example, and the feature vector corresponding to the example.\n",
      "-----\n",
      "\n",
      "f233d82d-9787-4633-87cb-b43c1dafa488\n",
      "The target expression represents the weight update rule for stochastic gradient descent applied to the quadratic error function in linear regression. It updates the weight vector by subtracting the product of the learning rate, the prediction error for a single data point, and the feature vector of that data point.\n",
      "-----\n",
      "\n",
      "3ec01d4f-858d-486e-a817-2c02b6f248d4\n",
      "The target expression represents the error function to be minimized in a multi-class classification setting. It is defined as the negative sum, over all training examples and all classes, of the product of the label indicator for each class and the logarithm of the predicted probability for that class. This is the negative log-likelihood (or cross-entropy loss) for a model that predicts class probabilities using a softmax function parameterized by a weight matrix, given a dataset.\n",
      "-----\n",
      "\n",
      "30dbb160-d563-4892-8973-ecc0d0763ff9\n",
      "The expected value of a function E, evaluated at the parameter matrix W given the dataset D; this is the error or loss function for a model with parameters W conditioned on the data D, which is minimized during training but cannot be minimized in closed form, thus requiring iterative optimization methods.\n",
      "-----\n",
      "\n",
      "332486bd-278b-41a3-9aa6-381297cd51db\n",
      "This expression gives the probability mass function for a Bernoulli random variable, where the outcome variable can be either zero or one, and the probability of the outcome being one is parameterized by a probability parameter. The probability of observing a particular outcome is expressed as a function of this parameter and the observed value.\n",
      "-----\n",
      "\n",
      "6c98760f-1a80-4554-a1c7-e21eb839d17c\n",
      "The target expression defines a function of an input vector and a parameter vector, where the function is constructed by first applying a linear transformation to the input using a set of weights indexed by two indices, then applying a nonlinear function to the result to form a set of basis functions, then linearly combining these basis functions with another set of weights, and finally applying the same nonlinear function to this linear combination. The expression also provides an equivalent compact vector-matrix notation for this computation.\n",
      "-----\n",
      "\n",
      "cd0ec03f-99ba-4487-b0d2-f16a330687dd\n",
      "This expression gives the Taylor series expansion of a differentiable function about a point, showing both the explicit sum of terms involving derivatives of the function at that point and the equivalent summation notation, representing the function as an infinite sum of its derivatives evaluated at the point, each divided by the corresponding factorial and multiplied by the appropriate power of the difference between the variable and the expansion point.\n",
      "-----\n",
      "\n",
      "b859519a-49f9-4c93-89b6-b162939a400d\n",
      "This expression gives the second-order Taylor (quadratic) approximation of a multivariate function at a point, representing the function as the sum of its value at a reference point, the inner product of its gradient at that point with the displacement from the reference, and one half the quadratic form involving the Hessian matrix at that point and the displacement vector.\n",
      "-----\n",
      "\n",
      "72a2a7eb-c243-4ec8-8193-1497b9c52a06\n",
      "This expression presents four key components of the linear regression model: the model output as a linear function of the input features and weights, the conditional probability of the target given the input and weights as a normal distribution with mean equal to the model output and fixed variance, the loss function as the squared difference between the model output and the target, and the gradient of the loss with respect to the weights as the product of the prediction error and the feature vector.\n",
      "-----\n",
      "\n",
      "77b51860-5391-4712-942d-67518e21fe87\n",
      "The target expression denotes a vector of weights that are used to combine the outputs of the basis functions (or the first layer) in a two-layer neural network model. These weights are applied after the input features have been processed by the first layer and its associated weight matrix.\n",
      "-----\n",
      "\n",
      "da25e8bb-7965-4bef-b8cb-c24646c1717b\n",
      "This is a set of four expressions related to multinomial logistic regression: (1) the predicted probability for class k given an input and weight matrix, defined via the softmax function and also interpreted as the conditional probability of class k; (2) the probability of a one-hot encoded label vector given the input and weights, expressed as a product over classes of predicted probabilities raised to the power of the corresponding label indicator; (3) the loss function, given as the negative sum over classes of the label indicator times the logarithm of the predicted probability; and (4) the gradient of the loss with respect to the weight vector for class k, given as the difference between the predicted probability and the label indicator, times the feature vector.\n",
      "-----\n",
      "\n",
      "470de08f-679b-4141-8188-f209dff72737\n",
      "This expression defines the probability that an input vector belongs to class k, given the input, a set of model parameters, and a feature transformation. It equates this probability to the k-th output of the model, which is computed by exponentiating the inner product of the parameter vector for class k and the feature vector, and then normalizing by the sum of such exponentials over all classes. This is the softmax function used in multiclass classification.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in math_expression_descriptions:\n",
    "    print(x.id)\n",
    "    print(x.text)\n",
    "    print('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ebf8e",
   "metadata": {},
   "source": [
    "### 4. MathExpressionDescriptionOpt, requires: MathExpressionDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55912beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionDescriptionOptimizer as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionDescriptionOpt\n",
    "\n",
    "\n",
    "inputs, input_id_to_math_expression_description = InputCreatorUtil.create(\n",
    "    math_expression_descriptions, lambda x: AssistantInput(description=x.text)\n",
    ")\n",
    "outputs = await math_expression_description_optimizer_assistant.concurrent_assist(inputs)\n",
    "math_expression_descriptions_opt = [\n",
    "    MathExpressionDescriptionOpt(\n",
    "        math_expression_id=input_id_to_math_expression_description[\n",
    "            output.input_id\n",
    "        ].math_expression_id,\n",
    "        math_expression_description_id=input_id_to_math_expression_description[output.input_id].id,\n",
    "        math_expression_index_id=index.id,\n",
    "        text=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf0aa5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e3c05985-b221-428e-bc2f-1ac4ff5360da\n",
      "The norm of the parameter vector w approaches infinity.\n",
      "-----\n",
      "\n",
      "688ca751-615d-4e3c-93c5-aff8fffb7fbf\n",
      "Error function as a function of vector variable w, conditioned on dataset D.\n",
      "-----\n",
      "\n",
      "20c561d7-0d6c-4c10-ba35-ad688ee04188\n",
      "Variable denoting the number of features in the feature space after mapping.\n",
      "-----\n",
      "\n",
      "cf4956ac-d57e-4f59-bfe4-dc0d5cc867c6\n",
      "Point in n-dimensional space where function f is evaluated and Hessian matrix is computed.\n",
      "-----\n",
      "\n",
      "73463273-b6ea-47ad-83ab-5727406a30c5\n",
      "Differentiable single-variable function used in Taylor series expansion and quadratic approximation.\n",
      "-----\n",
      "\n",
      "4169742a-47bc-46dc-8c02-3699764eb94c\n",
      "Variable representing a specific class label, used as an index to distinguish among multiple classes in a multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "598293f6-868c-424b-b239-246b789e6a0e\n",
      "Mapping from n-dimensional real vectors to n-dimensional real vectors.\n",
      "-----\n",
      "\n",
      "f891481f-75ed-47af-9fde-63464ccafee4\n",
      "Hessian matrix of a vector-valued function evaluated at point t\n",
      "-----\n",
      "\n",
      "0fd7653c-1f59-4022-948c-6d770121e193\n",
      "Number of feature space dimensions.\n",
      "-----\n",
      "\n",
      "909aa4d3-8ff8-41f3-86a2-363b4058b26b\n",
      "the point at which a function's Taylor series is expanded\n",
      "-----\n",
      "\n",
      "050f1a3e-5806-492f-a990-78b7bdcf679e\n",
      "the point where a function is expanded in its Taylor series\n",
      "-----\n",
      "\n",
      "afa448f5-9adb-4b16-a288-5773e6707376\n",
      "Point where the quadratic approximation of a function is constructed and tangential to the function during minimization.\n",
      "-----\n",
      "\n",
      "db1c0de7-de27-4af7-906a-344e35354250\n",
      "Dimensionality of the vector output by the softmax function, equal to the number of classes.\n",
      "-----\n",
      "\n",
      "d5ffda03-055c-4ea1-b389-422bda5be37c\n",
      "General function evaluated at a vector variable, representing the objective to be minimized in optimization methods including gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "e7f1238f-3fff-4b58-9c97-8c2fe0478302\n",
      "The k-th parameter in a probability vector for a categorical distribution is constrained to be greater than or equal to zero.\n",
      "-----\n",
      "\n",
      "2141cefe-3c28-4bb5-b2f0-4ee10215382b\n",
      "Function of a single variable minimized using quadratic approximation at the starting point.\n",
      "-----\n",
      "\n",
      "96c71a16-2739-4b4a-94f9-5bd172b80d8e\n",
      "Function approximated by a quadratic at point xâ during iterative minimization.\n",
      "-----\n",
      "\n",
      "065031d4-1f11-4b4e-bbd4-e9316579de5b\n",
      "Quadratic function approximating the original function in Newton's optimization method.\n",
      "-----\n",
      "\n",
      "6b700c3c-a402-41f3-aa0b-21a305a4513f\n",
      "Number of possible values or categories for the modeled variable, determining the number of parameters in the parameter vector.\n",
      "-----\n",
      "\n",
      "18df1c73-82c5-420a-9e2c-ff3b37518c86\n",
      "A linear combination where a transposed weight vector is multiplied by the feature mapping of an input vector.\n",
      "-----\n",
      "\n",
      "dfde6aab-4cb3-4fe5-95fe-3cef6812c0f2\n",
      "Probability that the k-th indicator variable equals one, corresponding to the probability that the multinomial variable is in its k-th state.\n",
      "-----\n",
      "\n",
      "c4c1f0f2-4a39-410c-bfd0-1d1927ae9c32\n",
      "Index of a specific class in multiclass classification, indicating which class's weights or parameters are referenced or updated.\n",
      "-----\n",
      "\n",
      "361bc0a1-4337-46c0-b029-1b3dae087e82\n",
      "The element in the i-th row and j-th column of the Hessian matrix of a function f is the second partial derivative of f with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "0f273a6e-37ce-482e-9d30-60e722f61113\n",
      "A classification problem with more than two classes.\n",
      "-----\n",
      "\n",
      "ed387666-cf13-49e3-a97e-7b57543b6f80\n",
      "A diagonal matrix with diagonal entries equal to the regularization factor, except the upper-left element, which may be zero if the corresponding weight is not regularized.\n",
      "-----\n",
      "\n",
      "bc5d4263-0c0e-478c-bcb1-4ae02f547ea1\n",
      "Non-zero vector used in the condition for positive semi-definiteness of the Hessian matrix to test if the quadratic form with the Hessian is non-negative.\n",
      "-----\n",
      "\n",
      "86dc5c0d-e597-43d3-b304-74aef14c9ac5\n",
      "Models indexed by k, each corresponding to a specific class in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "6472cf1b-2216-436a-b670-60903d22caf1\n",
      "Probability that the k-th indicator variable in a multinoulli vector equals one, representing the probability that the multinoulli variable takes the k-th outcome.\n",
      "-----\n",
      "\n",
      "9a68b12c-b715-4664-8dc2-83a8e3de4348\n",
      "total number of classes in the generalization of the cross-entropy error and loss function\n",
      "-----\n",
      "\n",
      "bd9be618-caeb-4389-85d0-e930051d4c41\n",
      "Total number of classes in the multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "f5e40c6d-eae1-46da-a00b-6c98ca4d6e5c\n",
      "Rightwards arrow symbol indicating the start of an example section in text.\n",
      "-----\n",
      "\n",
      "00c97125-cb15-45b6-ab95-ace8d47284c4\n",
      "Input vector for the i-th training example in linear regression and stochastic gradient descent.\n",
      "-----\n",
      "\n",
      "fdcc8b99-3d8a-4fe6-beaf-96c5d9bd42d4\n",
      "Number of components in the input vector space for a function whose Hessian matrix is considered; the function maps vectors of this dimension to scalars.\n",
      "-----\n",
      "\n",
      "57ba572b-71ba-452d-b7fe-43e620b9d372\n",
      "The mean parameter of a distribution equals the dot product of a weight vector and a feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "a17bfd4b-7c48-454f-b510-bf9c0ed6695a\n",
      "L2 regularization term is a penalty added to the loss function in optimization problems, based on the sum of the squared values of model parameters.\n",
      "-----\n",
      "\n",
      "eddd27c3-018b-48ff-8061-28fca4814bcc\n",
      "Hessian matrix in logistic regression optimization, representing the second derivatives of the cross entropy error, used in the weights update rule for Newton's method.\n",
      "-----\n",
      "\n",
      "ff8ec75f-7c82-4d0f-a71d-296807e59033\n",
      "square matrix with n rows and n columns\n",
      "-----\n",
      "\n",
      "0c2cedea-8ae5-4803-9214-04ebdeaaafec\n",
      "Function of a single variable, shown as a black curve, minimized using Newton's method.\n",
      "-----\n",
      "\n",
      "4894ad47-2237-490d-a05c-2c766e2789a7\n",
      "Index of a component in the output vector of the softmax function, representing a specific class among K possible classes.\n",
      "-----\n",
      "\n",
      "fbe3860d-31d7-457e-94d9-18d8909f365e\n",
      "Function minimized during optimization, approximated quadratically at a specific point in the process.\n",
      "-----\n",
      "\n",
      "e9788431-949f-465c-9a6c-b32a973c8c75\n",
      "Feature vector representing a single example for which class membership probability is modeled.\n",
      "-----\n",
      "\n",
      "fddf82a7-f315-4641-ad69-74a0df07c433\n",
      "Index specifying the position of a weight in the weight vector for a given basis function.\n",
      "-----\n",
      "\n",
      "496eb4aa-5c58-40b8-bd79-a8650e3223fb\n",
      "Hessian matrix in Newton's optimization method, characterized by positive semi-definiteness and positive definiteness properties.\n",
      "-----\n",
      "\n",
      "b37872a0-e0fb-4e2e-bda5-0d33dd3ff1ef\n",
      "function analyzed for convexity based on the positive semi-definiteness of its Hessian matrix\n",
      "-----\n",
      "\n",
      "9b510b66-07c2-473e-a777-065221ea672e\n",
      "Matrix with m+1 rows and N columns, designed for multiplication with a matrix of N rows.\n",
      "-----\n",
      "\n",
      "096deb8e-4453-497e-8e42-68c741007c2d\n",
      "Total number of classes in the multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "6bc45875-009e-4a4e-a4e9-df87a4eafc7a\n",
      "Number of input variables or features in the feature mapping example is two.\n",
      "-----\n",
      "\n",
      "a8b76fdc-1104-4764-8b24-437283f4bab9\n",
      "Function minimized using Newton's method, evaluated at variable x.\n",
      "-----\n",
      "\n",
      "504b0e94-b840-4ff7-bc2f-4bc219689896\n",
      "L1 regularization norm, non-differentiable, requires subgradients or specialized optimization methods like coordinate descent or proximal algorithms.\n",
      "-----\n",
      "\n",
      "f7def4aa-81d0-40c6-a52b-b4e6d5dc2d7b\n",
      "Weight parameter for the upper-left element of the weight vector, excluded from L2 regularization.\n",
      "-----\n",
      "\n",
      "9617dd7f-b27a-4d10-8138-7a39809e261d\n",
      "Point obtained after the first step of minimizing the quadratic approximation of the function, starting from the initial point, in the iterative minimization process.\n",
      "-----\n",
      "\n",
      "f2989b32-d95a-478f-b75a-0779f11ad5d5\n",
      "Function minimized by quadratic approximation at each iteration to determine the next search point.\n",
      "-----\n",
      "\n",
      "c4aa5a12-2b5f-4acb-9136-980aae70aa0f\n",
      "Input example vector to the softmax function, representing a data point in multiclass classification.\n",
      "-----\n",
      "\n",
      "645b4d87-68de-4d9a-9fc2-bd0e87a26080\n",
      "The target expression indicates the number of weight vectors in the matrix, equal to the number of classes in the multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "f53a96ea-a5ce-4aaa-89e4-acb731161916\n",
      "A function mapping n-dimensional real vectors to real numbers; a real-valued function defined on R^n.\n",
      "-----\n",
      "\n",
      "3aeaa1c4-8ad4-43ac-82fa-76d67b9ca64e\n",
      "Index denoting a specific example in a dataset, typically ranging from 1 to N, where N is the total number of examples.\n",
      "-----\n",
      "\n",
      "f1fa44ab-907a-45df-a375-57015abccae2\n",
      "The indicator vector for a categorical variable has components that sum to one, indicating exactly one outcome among mutually exclusive and collectively exhaustive states.\n",
      "-----\n",
      "\n",
      "a0e3c236-dec2-447a-904e-d935d0820aaf\n",
      "Square Hessian matrix with dimensions (m + 1) Ã (m + 1).\n",
      "-----\n",
      "\n",
      "46216354-0560-4066-b086-d693a8e7a613\n",
      "Activation function in generalized linear models that maps the linear predictor to the mean parameter of an exponential family distribution.\n",
      "-----\n",
      "\n",
      "193b3918-c4aa-4ad1-856e-12e02544a1fc\n",
      "Value assigned to the negative class label in binary logistic regression, distinguishing it from the positive class label.\n",
      "-----\n",
      "\n",
      "596ed236-5651-43ac-93db-22428c7efac9\n",
      "The integer immediately less than class index k, representing the number of models corresponding to all classes except class k.\n",
      "-----\n",
      "\n",
      "d596a030-d07c-4074-a786-e240a9cf2cbf\n",
      "Variable denoting the index of a specific class among K total classes in multiclass classification.\n",
      "-----\n",
      "\n",
      "37e02e27-fa43-4d36-a3a8-4c492ff17ed1\n",
      "Describes matrix multiplication between a matrix with N rows and a matrix with N rows and m+1 columns.\n",
      "-----\n",
      "\n",
      "f900c292-2ff9-460c-aa9f-695f84ebacef\n",
      "Mean parameter of a distribution as a function of the linear combination of the weight vector transpose and the feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "01305079-5e86-4f4c-a8da-436472550abf\n",
      "Total number of classes in the softmax function, defining the dimensionality of the class score vector.\n",
      "-----\n",
      "\n",
      "82ae5d07-a024-48cd-88ee-a2ed84bbaabc\n",
      "Hessian matrix of the cross-entropy error function in logistic regression, with focus on positive definiteness and invertibility.\n",
      "-----\n",
      "\n",
      "6b7d3c5f-21bb-4a92-9a53-64c9cd124d43\n",
      "Predicted probability for an example in logistic regression, calculated as the output of the logistic function evaluated at input vector x.\n",
      "-----\n",
      "\n",
      "ad327783-49e5-4d68-a4ee-0ef20184252d\n",
      "initial point for minimizing a single-variable function using Newton's method\n",
      "-----\n",
      "\n",
      "0f972452-a92d-4252-9516-6a032bbf88db\n",
      "Label of the i-th data point in a dataset, as used in probabilistic modeling and label distributions.\n",
      "-----\n",
      "\n",
      "7b910d83-03b6-4291-822f-1b13ff72f01f\n",
      "When a model's predicted probability for a class approaches one, the natural logarithm of that probability approaches zero.\n",
      "-----\n",
      "\n",
      "8b0b5581-03dd-4636-aade-b4025b0d2d81\n",
      "The value representing the outcome of a Bernoulli random variable that is not the 'success' case.\n",
      "-----\n",
      "\n",
      "19e5dd00-ac49-427b-96c3-f1b4e2dd61b2\n",
      "The target expression denotes the class label for the positive class in binary logistic regression, contrasted with the class label for the negative class.\n",
      "-----\n",
      "\n",
      "3c500691-30d9-40bc-a74f-2bcd0aaab850\n",
      "Activation function used as the mean function in generalized linear models, linking the mean parameter of an exponential family distribution to the linear predictor.\n",
      "-----\n",
      "\n",
      "749d819b-bc5e-4606-bc86-1fb3eec78fae\n",
      "Index identifying a specific basis function within a model, distinguishing between different basis functions and their associated weights.\n",
      "-----\n",
      "\n",
      "01b66cce-343d-4679-9d2b-55c78f3debd4\n",
      "Index corresponding to a specific class in multi-class classification, with total classes represented by an uppercase variable.\n",
      "-----\n",
      "\n",
      "31f4a47b-aedd-4068-b246-9ed6f239f999\n",
      "Variable denoting the current value of the parameter vector at iteration t in an iterative optimization algorithm.\n",
      "-----\n",
      "\n",
      "360a8917-f1c0-4d37-913b-6fb24f598e2a\n",
      "Matrix of K column vectors, each representing a weight vector for one of K classes in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "976cf911-4611-467b-9186-5537673a5a3d\n",
      "Mean parameter of a distribution in the exponential family, defined by the mean function (activation function).\n",
      "-----\n",
      "\n",
      "80e127aa-49d0-4d9a-b8f1-4e1eeccdae73\n",
      "Weight vector for class k, one of K weight vectors comprising the weight matrix in the softmax regression model.\n",
      "-----\n",
      "\n",
      "81a9745b-17f7-4908-81be-8311ffb94f57\n",
      "The Hessian matrix is positive semi-definite but not positive definite, lacks an inverse, and is unsuitable for Newton's method in this context.\n",
      "-----\n",
      "\n",
      "a4681cf1-062a-4c2c-85a1-7ee320f1f692\n",
      "Class index used to compute the probability of example x in a multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "57840bc5-c4b6-4418-aaa3-cc4eb3f0d4b8\n",
      "Class label index representing one of K possible classes in a classification problem, indicating the specific class assignment for an example.\n",
      "-----\n",
      "\n",
      "b304b291-67bf-412e-bbc7-71c8f3d9659c\n",
      "Number of basis functions used in feature mapping for transforming input variables to a higher-dimensional feature space in generalized linear models.\n",
      "-----\n",
      "\n",
      "538143a4-852b-4960-ac78-7c7cb897e38c\n",
      "Regularization factor on the diagonal of the regularization matrix, excluding the upper-left element, in L2 regularization for logistic regression.\n",
      "-----\n",
      "\n",
      "0be8bf93-c0c1-4660-aba0-2ab6aacf886f\n",
      "Five-dimensional vector with components 1, 2, 0.5, 5, and 3, used as input for the softmax function.\n",
      "-----\n",
      "\n",
      "1f75e399-fab3-4a13-b9e5-08b9ed116ea3\n",
      "Collection of m basis functions, indexed from 1 to m, used for feature mapping of input variables in generalized linear models.\n",
      "-----\n",
      "\n",
      "c78a8f17-fdc4-424b-826f-e238949d8214\n",
      "Index identifying a specific basis function within a model, distinguishing between different basis functions and their associated weight vectors.\n",
      "-----\n",
      "\n",
      "7ac240af-7e6c-4181-8179-3cdeae6cde54\n",
      "The value representing the second outcome of a Bernoulli random variable, which can be zero or one.\n",
      "-----\n",
      "\n",
      "43047fea-a562-4213-8d82-610d20a62d1d\n",
      "For every non-zero vector, the quadratic form defined by the Hessian matrix is strictly greater than zero, characterizing the matrix as positive definite.\n",
      "-----\n",
      "\n",
      "212d1334-e4e5-4146-ae8a-2bfe55ca48f5\n",
      "The target expression represents the model for class k in multinomial logistic regression, which predicts the probability that an input belongs to class k.\n",
      "-----\n",
      "\n",
      "dce9db8c-413d-4408-a8c0-4329b8c90825\n",
      "Parameter indicating the probability of success in a Bernoulli distribution, defining the probability of a binary outcome variable.\n",
      "-----\n",
      "\n",
      "6c0ed2b1-06ac-47f9-b959-ef55658eaada\n",
      "Index of a multinomial variable's outcome; the k-th component of the indicator vector is one if the outcome equals this value.\n",
      "-----\n",
      "\n",
      "99ab2b03-069f-4ef5-81b8-0fc005191cbd\n",
      "The Hessian matrix equals the product of the transpose of matrix Phi, matrix S, and matrix Phi.\n",
      "-----\n",
      "\n",
      "1c65f181-9d0c-41ca-8c79-6b089eefadeb\n",
      "Vector of K parameters, each subscripted mu, representing probabilities for each of K outcomes in a categorical distribution.\n",
      "-----\n",
      "\n",
      "f38796d0-dd0f-4557-aee3-9a8a7933efa2\n",
      "L2 regularization, a regularization technique, is incorporated into optimization methods including gradient descent, Newton's method, and least squares for linear regression.\n",
      "-----\n",
      "\n",
      "71de3bc9-54b7-4fa6-8921-04d821a3d418\n",
      "Weight vector for the k-th class in multinomial logistic regression; each class has a distinct weight vector.\n",
      "-----\n",
      "\n",
      "3a95e54c-f995-4841-ba7c-6d2baf83ea7e\n",
      "The k-th component of the softmax function for a vector of n real numbers is the exponential of the k-th input divided by the sum of exponentials of all inputs.\n",
      "-----\n",
      "\n",
      "658b514f-eb6f-4f95-a4e3-2b352072e37e\n",
      "A general model output is computed by applying an activation function to the dot product of a weight vector and a feature transformation of the input vector, with the transformation parameterized by the weights.\n",
      "-----\n",
      "\n",
      "2b0ad44c-c066-4326-af76-54cef755e7c3\n",
      "A function represented by a single letter, relevant to optimization methods like gradient descent and Newton's method, and possibly defined for one variable.\n",
      "-----\n",
      "\n",
      "3c2c99e9-00e2-4c2c-98e0-b674c4feb0a4\n",
      "The regularized Hessian matrix equals the original Hessian matrix plus a regularization parameter multiplied by the identity matrix.\n",
      "-----\n",
      "\n",
      "92780f98-7af7-4aa9-9eee-7aedb96697fb\n",
      "Function of a vector variable defined on a multidimensional space, relevant to optimization methods and convexity, specifically in logistic regression.\n",
      "-----\n",
      "\n",
      "3267f67e-ab1c-4746-9192-2d6b7afc1e34\n",
      "The target expression represents the model for class k in multinomial logistic regression, which predicts the probability that an input belongs to class k.\n",
      "-----\n",
      "\n",
      "5a408f7d-d37d-4315-990c-a509fec111e2\n",
      "Function mapping an n-dimensional real vector to a real number, representing the j-th basis function among m basis functions for feature mapping in generalized linear models.\n",
      "-----\n",
      "\n",
      "72765d41-20bc-4b44-b48d-36325a0bee02\n",
      "Model for a specific class index that outputs the probability of input membership in that class, defined by the softmax function and parameterized by the class's weight vector.\n",
      "-----\n",
      "\n",
      "d455a5c9-be59-46b4-a224-3a380fd00f8d\n",
      "The j-th basis function in a generalized linear model mapping applies any function, not limited to polynomials or input feature products, to input variables.\n",
      "-----\n",
      "\n",
      "ed796e75-bb5f-4340-a2fe-7aeb610ac667\n",
      "The k-th component of the indicator vector for a categorical (multinoulli) variable is zero when the variable's outcome is not the k-th category.\n",
      "-----\n",
      "\n",
      "53483643-1f68-4d62-9021-dd177afc820f\n",
      "The symbol denotes the Hessian matrix for the cross-entropy error in logistic regression, a square matrix of size (m+1) by (m+1), where m is the number of features after mapping.\n",
      "-----\n",
      "\n",
      "55bbaa25-7f7f-4da8-ae79-73745124a0cd\n",
      "The number of possible output classes, denoted by an uppercase variable, is greater than two, indicating a multiclass scenario rather than a binary one.\n",
      "-----\n",
      "\n",
      "5ca84e79-c4e1-466f-988d-191884864317\n",
      "The parameter vector mu sums to one across all components, representing a probability distribution over K categories.\n",
      "-----\n",
      "\n",
      "b7aa922c-f344-470f-a782-e92ad56548c9\n",
      "Hessian matrix that is positive semi-definite but not positive definite, lacking an inverse and preventing application of Newton's optimization method.\n",
      "-----\n",
      "\n",
      "d8f1d384-7b73-4cf6-8786-40cde0d6b88c\n",
      "In multinomial logistic regression, the model consists of multiple class-indexed models, each outputting the probability that an input belongs to a specific class via the softmax function.\n",
      "-----\n",
      "\n",
      "1201f373-0765-4541-8569-6fde165d24f7\n",
      "The expression asserts that the number of possible values K is greater than two, generalizing from the binary case to more than two categories.\n",
      "-----\n",
      "\n",
      "57045e07-da5d-4e89-861f-1ac41494e59c\n",
      "The Hessian matrix is a square matrix with dimensions equal to one more than the number of features in the feature space.\n",
      "-----\n",
      "\n",
      "b8c337f7-f3fb-409b-b2da-18c43d20a98c\n",
      "Dot product of class k's weight vector and input example vector, yielding a scalar input to the softmax function for class k.\n",
      "-----\n",
      "\n",
      "0b636160-e323-4900-b60a-e768810bfa90\n",
      "Hessian matrix in Newton's optimization method, required to be positive definite for invertibility and applicability of the method.\n",
      "-----\n",
      "\n",
      "64da4006-ef51-4e0e-bfd8-5c62d7bb8be4\n",
      "Quadratic error function for linear regression: sum over N data points of squared difference between predicted value (weight vector applied to feature transformation of input) and actual target value, scaled by one half.\n",
      "-----\n",
      "\n",
      "41e78d64-caf5-4b75-8f7c-7b70a63cfe3a\n",
      "The k-th component of a parameter vector representing the probability assigned to the k-th category in a categorical distribution, with all components non-negative and summing to one.\n",
      "-----\n",
      "\n",
      "03fd3242-c70d-4fc0-a4f8-31525a0db677\n",
      "Superscript label identifying the first set of weights in the model, specifically those used earlier in the model prediction calculation.\n",
      "-----\n",
      "\n",
      "5738ff44-0894-486a-afc3-eacbda1101e0\n",
      "Regularization term based on the squared L2 norm added to the loss function or gradient in regularized logistic regression.\n",
      "-----\n",
      "\n",
      "7d6f6e4f-142a-4890-9c31-3f221524a1a4\n",
      "Hessian matrix of the cross-entropy error function with dimensions (m+1) by (m+1), where m is the number of features after mapping.\n",
      "-----\n",
      "\n",
      "84ad5faf-2555-463b-9aa8-0415c7e7a3e5\n",
      "A column vector with K entries, each an indicator variable for a categorical variable with K possible outcomes; each entry is one if its corresponding outcome occurs, zero otherwise.\n",
      "-----\n",
      "\n",
      "72bb00cf-cf57-4c83-98c6-a92a43dd59f2\n",
      "Value of a function evaluated at variable x, subject to Taylor series expansion and quadratic approximation about point a.\n",
      "-----\n",
      "\n",
      "abb8404b-df9c-4fe1-ae5c-e25435ad57d6\n",
      "The k-th component of the indicator vector equals one, indicating the outcome corresponds to the k-th category.\n",
      "-----\n",
      "\n",
      "39634994-f563-43b8-9639-bd441c0ace96\n",
      "Parameter set to one, indicating full step size in Newton's method parameter update when Hessian matrix is computed exactly.\n",
      "-----\n",
      "\n",
      "314025f5-8d62-433e-8289-866edbd895c4\n",
      "Four-dimensional indicator vector for a multinomial variable with the third component as one and all others zero, indicating selection of the third state out of four possible states.\n",
      "-----\n",
      "\n",
      "258283b5-3f62-43bd-bf12-71ce1ff8d8cf\n",
      "Defines the output of a binary logistic regression model as the probability that the class label equals one, given an input vector and a weight vector, computed by applying the sigmoid function to the inner product of the weight vector and a feature transformation of the input.\n",
      "-----\n",
      "\n",
      "311f4660-ccbe-4e7f-811f-d4d845f9bad3\n",
      "Bold capital H at this index denotes the Hessian matrix of the cross-entropy error in logistic regression, which is positive semi-definite in this context.\n",
      "-----\n",
      "\n",
      "daffb72d-c861-4ae6-ae6f-cabefa790cd6\n",
      "Degree parameter set to two for polynomial mapping example with input space of two variables.\n",
      "-----\n",
      "\n",
      "99b5f55a-62da-40a9-acb9-466bc48dcc91\n",
      "Matrix product where S multiplies the transpose of Phi on the left and Phi on the right, forming the Hessian matrix for logistic regression and Newton's method optimization.\n",
      "-----\n",
      "\n",
      "3a546563-936e-4a4a-8dc3-c6f6f39bc818\n",
      "Hessian matrix of the cross-entropy error function in logistic regression, decomposable as the product of the transpose of the feature matrix, a diagonal matrix, and the feature matrix; may be ill-conditioned due to multicollinearity.\n",
      "-----\n",
      "\n",
      "681df0a3-2761-4259-81ac-2a6a2cbb6447\n",
      "Parameter update rule in gradient descent: the parameter vector is updated by subtracting the product of the learning rate and the gradient of a function evaluated at the current parameter value.\n",
      "-----\n",
      "\n",
      "484dc562-8b8b-4d22-b88a-d743a95be2d8\n",
      "Regularization involving the sum of absolute values of model parameters, promotes sparsity in solutions, and is non-differentiable, complicating gradient-based optimization.\n",
      "-----\n",
      "\n",
      "a42708ec-f790-4bb6-b0d2-16e54c30e188\n",
      "Dataset of input-output pairs comprising all feature vectors and corresponding labels, used to compute label probabilities and define the error function in multiclass classification.\n",
      "-----\n",
      "\n",
      "40d67f52-34f1-46b8-89a1-ad04f21d1505\n",
      "Superscript distinguishes between two sets of weights in the model, indicating the set used second in model prediction calculations.\n",
      "-----\n",
      "\n",
      "6824dd86-1ee4-4a6f-ae00-1260b52616f4\n",
      "Scalar product of the k-th class weight vector and input vector, representing the unnormalized score for that class in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "0bc9c24a-e01e-4981-aa04-115b5c029f14\n",
      "Second-order Taylor approximation of a differentiable function at a point, using the function value, first derivative, and second derivative at that point, obtained by truncating the Taylor series after the quadratic term.\n",
      "-----\n",
      "\n",
      "219f980b-65a5-4200-b5ab-5af48c4c56c4\n",
      "Weight parameter for the i-th input variable in the weight vector of the j-th basis function within the inner sum of a generalized linear model using basis functions, where each basis function has a distinct set of weights.\n",
      "-----\n",
      "\n",
      "08ecb7e8-ce97-4ae6-b222-f10c68edb9fe\n",
      "L1 norm regularization in logistic regression is non-differentiable and requires specialized optimization methods such as coordinate descent or proximal algorithms.\n",
      "-----\n",
      "\n",
      "297e7394-92d0-44c8-8510-82e3c9a38350\n",
      "Probability of a categorical outcome vector computed as the product over all classes of the class probability parameter raised to the power of the corresponding one-hot encoded indicator variable. Parameters are non-negative, sum to one, and represent class probabilities; outcome vector uses one-hot encoding.\n",
      "-----\n",
      "\n",
      "f4624147-f06a-4d8b-a630-a49e2acb7e9c\n",
      "The target expression refers to the input example vector whose class membership probability is modeled by the multinomial logistic regression model. This vector is used as the argument to the model functions and for computing the softmax function.\n",
      "-----\n",
      "\n",
      "f8def64a-7231-41b3-8e9c-660c202d4908\n",
      "Key components of logistic regression: model output as sigmoid of linear feature combination, probability distribution for label given input and weights, cross-entropy loss function, gradient of loss with respect to weights.\n",
      "-----\n",
      "\n",
      "d9873aaa-b2f3-4436-8800-d61b2bb00329\n",
      "Output vector dimension from the softmax function equals the number of classes.\n",
      "-----\n",
      "\n",
      "a674b09e-49b2-4064-a712-1b39d5f6648c\n",
      "For every non-zero vector, the quadratic form defined by the Hessian matrix is greater than or equal to zero, indicating the Hessian matrix is positive semi-definite.\n",
      "-----\n",
      "\n",
      "07057317-9c6e-4c6d-b2a0-25b0656dbaec\n",
      "Activation function in a generalized linear model applied to the linear combination of weights and feature-mapped inputs to generate the model output.\n",
      "-----\n",
      "\n",
      "025032ca-174b-47a8-b002-3a769c8f1d3b\n",
      "The Hessian matrix of a scalar-valued function of n variables is a square matrix with entries given by the second-order partial derivatives of the function with respect to each pair of variables; the entry in the i-th row and j-th column is the second partial derivative with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "90ecfc70-bd28-4290-89b2-b70ea8ffba11\n",
      "General form of an exponential family probability distribution: probability of a variable given parameters equals a base measure times an exponential function of parameters, sufficient statistics, and a log-partition function.\n",
      "-----\n",
      "\n",
      "2d453a41-42ed-4cd3-9e79-197ef96dc2e2\n",
      "Weight update rule in stochastic gradient descent for logistic regression: current weight vector minus learning rate times the difference between model prediction and true label for a specific example, multiplied by that example's feature vector.\n",
      "-----\n",
      "\n",
      "77b51860-5391-4712-942d-67518e21fe87\n",
      "A vector of weights used to combine outputs of basis functions or the first layer in a two-layer neural network model, applied after input features are processed by the first layer and its weight matrix.\n",
      "-----\n",
      "\n",
      "bbad5832-a923-4718-bc39-7f78e92ebcb9\n",
      "A type of regularization used in optimization methods such as gradient descent, Newton's method, and least squares for linear regression, involving a regularization term added to the loss function that modifies the gradient and Hessian.\n",
      "-----\n",
      "\n",
      "66eefc4b-066c-46cc-abd7-750df54e681b\n",
      "Weight matrix for the model's first layer; each basis function has a set of weights applied to all input features and summed.\n",
      "-----\n",
      "\n",
      "89e30aa1-c47a-4ebb-8e22-905a2690fc4f\n",
      "The gradient of the regularized error function with respect to the parameter vector equals the gradient of the unregularized error function with respect to the parameter vector plus the product of the regularization parameter and the parameter vector.\n",
      "-----\n",
      "\n",
      "32edba48-8bdc-445b-b20c-d3944b72e578\n",
      "Applying the softmax function to the vector [1, 2, 0.5, 5, 3] yields the probability vector [1.5%, 4.1%, 0.9%, 82.3%, 11.2%].\n",
      "-----\n",
      "\n",
      "f233d82d-9787-4633-87cb-b43c1dafa488\n",
      "Weight update rule for stochastic gradient descent on quadratic error in linear regression: updates weight vector by subtracting learning rate multiplied by prediction error for a single data point and its feature vector.\n",
      "-----\n",
      "\n",
      "e1db75f8-0ec8-41cc-813e-037cae07d78d\n",
      "A real-valued function mapping an n-dimensional vector to a real number, used to define its Hessian matrix composed of all second-order partial derivatives with respect to the input vector components.\n",
      "-----\n",
      "\n",
      "61510db0-ead7-4ed5-b3d8-69fa29183ffa\n",
      "Defines the loss function for a single example in multi-class classification as the negative sum over all classes of the product of the true label indicator for each class and the logarithm of the predicted probability for that class, with predicted probabilities parameterized by model weights.\n",
      "-----\n",
      "\n",
      "54357f82-2224-4432-97c1-c55a27a41123\n",
      "Gradient of the error function with respect to the weight vector for class k, where the error function depends on the weight matrix and dataset. The gradient is a sum over all N data points; for each data point, the difference between the model's output for class k and the true label for class k is multiplied by the data point's feature vector.\n",
      "-----\n",
      "\n",
      "071aab3a-d78d-4a9c-953b-f3161a917e21\n",
      "The update rule for the parameter vector at iteration t+1 subtracts from the current value a scaled product of the inverse Hessian matrix at iteration t and the gradient of the function at the current parameter vector, with scaling factor eta.\n",
      "-----\n",
      "\n",
      "bae03497-1d70-449d-87ca-9d12a2c889fe\n",
      "Gradient of the quadratic error function for linear regression with respect to the weight vector for a single training example, equal to the product of the difference between the model prediction and true target value and the example's feature vector.\n",
      "-----\n",
      "\n",
      "332486bd-278b-41a3-9aa6-381297cd51db\n",
      "Probability mass function for a Bernoulli random variable with outcomes 0 or 1, where the probability of outcome 1 is given by a parameter; the probability of each outcome is expressed as a function of this parameter and the observed value.\n",
      "-----\n",
      "\n",
      "d64be2d2-35cc-44ef-9bf8-8447d4b09c01\n",
      "Iterative update rule for gradient descent: the next value is the current value minus the step size times the gradient of the function at the current value; the subscript indicates the iteration index.\n",
      "-----\n",
      "\n",
      "b454c06d-4df1-4e8d-8480-70f3e97c82d5\n",
      "The Hessian matrix for the cross-entropy error in logistic regression equals the product of the transpose of the feature matrix, a diagonal matrix with diagonal entries as the first derivatives of the logistic output for each training example, and the feature matrix.\n",
      "-----\n",
      "\n",
      "e4c7cb9f-6686-4630-a8e3-18cc51042a46\n",
      "A generalized linear model output is the activation function applied to the inner product of a weight vector and a feature-mapped input vector, equivalently the activation function applied to the sum of weights times their corresponding basis functions evaluated at the input.\n",
      "-----\n",
      "\n",
      "33693440-1596-4e1d-9bd2-1f02c513fb3b\n",
      "Diagonal matrix with entries equal to the product of the logistic function evaluated at each training example and one minus that value, for all training set examples.\n",
      "-----\n",
      "\n",
      "b859519a-49f9-4c93-89b6-b162939a400d\n",
      "Second-order Taylor (quadratic) approximation of a multivariate function at a point: sum of the function's value at a reference point, the inner product of its gradient at that point with the displacement from the reference, and one half the quadratic form involving the Hessian matrix at that point and the displacement vector.\n",
      "-----\n",
      "\n",
      "a4802e86-0db2-4942-9bcd-f7c55a6b7db1\n",
      "total number of examples in the training set\n",
      "-----\n",
      "\n",
      "30dbb160-d563-4892-8973-ecc0d0763ff9\n",
      "Expected value of the error or loss function E evaluated at parameter matrix W given dataset D; represents model error conditioned on D, minimized during training via iterative optimization methods due to lack of closed-form solution.\n",
      "-----\n",
      "\n",
      "a896a52b-1d54-434c-9cb0-379a2f5ed0b5\n",
      "Label indicator for example i and class k; equals 1 if example i belongs to class k, 0 otherwise.\n",
      "-----\n",
      "\n",
      "8e55cce3-8de3-4ea8-bde9-c4f0e9829d00\n",
      "Activation function applied to the scalar product of the weight vector and feature vector in a generalized linear model.\n",
      "-----\n",
      "\n",
      "c453729b-2113-48d1-af1d-026e7610805b\n",
      "The update rule for the weight vector in Newton's method for logistic regression subtracts the product of the inverse Hessian matrix of the weights and the gradient of the error function (with respect to the weights, given the dataset) from the current weight vector, with step size one.\n",
      "-----\n",
      "\n",
      "033bff2a-ddad-417c-9281-e7b51164d08f\n",
      "Symbol denotes the Hessian matrix associated with the cross-entropy error function, discussed regarding positive semi-definiteness and invertibility in logistic regression.\n",
      "-----\n",
      "\n",
      "3ec01d4f-858d-486e-a817-2c02b6f248d4\n",
      "Error function for multi-class classification: negative sum over all training examples and classes of the product of the class label indicator and the logarithm of the predicted class probability. Represents negative log-likelihood (cross-entropy loss) for a softmax-based model parameterized by a weight matrix on a given dataset.\n",
      "-----\n",
      "\n",
      "cd0ec03f-99ba-4487-b0d2-f16a330687dd\n",
      "Taylor series expansion of a differentiable function about a point, expressed as an explicit sum of terms involving derivatives at that point and as summation notation: the function is represented as an infinite sum of its derivatives evaluated at the point, each divided by the corresponding factorial and multiplied by the appropriate power of the difference between the variable and the expansion point.\n",
      "-----\n",
      "\n",
      "be4713b7-2c6e-4a42-b81a-a3f5d6ff8bd8\n",
      "Point in the domain of a multivariate function f, represented as a vector, serving as the expansion point for the quadratic (second-order) Taylor expansion of f with respect to a vector variable.\n",
      "-----\n",
      "\n",
      "b044e98c-89da-4b4c-a397-d703e981bf09\n",
      "Probability in multinomial logistic regression that an input belongs to a class, calculated as the exponential of a class-specific linear function of a feature transformation of the input, divided by the sum of such exponentials over all classes; equivalent to the softmax output for that class.\n",
      "-----\n",
      "\n",
      "6c98760f-1a80-4554-a1c7-e21eb839d17c\n",
      "Defines a function of an input vector and a parameter vector: applies a linear transformation to the input using weights indexed by two indices; applies a nonlinear function to the result to form basis functions; linearly combines these basis functions with another set of weights; applies the same nonlinear function to this combination. Provides an equivalent compact vector-matrix notation for the computation.\n",
      "-----\n",
      "\n",
      "13cd319a-1a1f-4a7c-97f2-4ac8596fdedd\n",
      "Defines a feature mapping for a two-dimensional input vector to a six-dimensional vector: [1, xâ, xâ, xâxâ, xâÂ², xâÂ²], representing a degree-two polynomial feature expansion for two variables.\n",
      "-----\n",
      "\n",
      "72a2a7eb-c243-4ec8-8193-1497b9c52a06\n",
      "Four components of the linear regression model: model output as a linear function of input features and weights; conditional probability of the target given input and weights as a normal distribution with mean equal to the model output and fixed variance; loss function as the squared difference between model output and target; gradient of the loss with respect to weights as the product of prediction error and feature vector.\n",
      "-----\n",
      "\n",
      "af1eca0b-3f8c-41cb-a499-a4b698ef8edd\n",
      "A feature mapping function takes an n-dimensional real vector input and outputs an (m+1)-dimensional real vector; the first component is 1, and the remaining m components are the results of applying m basis functions to the input vector.\n",
      "-----\n",
      "\n",
      "7a58773b-2493-4ec6-8fd6-40c99d0ca5c5\n",
      "Log-likelihood of observed labels given inputs, calculated as the sum over all examples and classes of the product of the label indicator and the logarithm of the predicted class probability, with predicted probability parameterized by model weights.\n",
      "-----\n",
      "\n",
      "470de08f-679b-4141-8188-f209dff72737\n",
      "Probability that an input vector belongs to class k, given the input, model parameters, and feature transformation, equals the k-th model output computed by exponentiating the inner product of the class k parameter vector and the feature vector, normalized by the sum of exponentials over all classes; this is the softmax function for multiclass classification.\n",
      "-----\n",
      "\n",
      "59571f9a-355c-482a-9fad-5ffc21525e4f\n",
      "y is a binary label with values 0 or 1, representing a Bernoulli random variable in binary logistic regression.\n",
      "-----\n",
      "\n",
      "54a1da96-2e9f-4f05-bc40-b584eb0f511d\n",
      "A multivariate function of a vector variable x, considered for a quadratic (second-order) Taylor expansion about a specific point.\n",
      "-----\n",
      "\n",
      "d6e38806-06f5-498d-91da-1fb9cb21c532\n",
      "Variable denoting a non-zero vector in the positive definiteness condition for a matrix, used in the inequality involving the vector's transpose, the matrix, and the vector.\n",
      "-----\n",
      "\n",
      "f8785169-9b49-4d8a-b9af-30d3567daf4b\n",
      "L2 regularization is a technique added to the error function and its derivatives in optimization algorithms like Newton's method for logistic regression to prevent overfitting.\n",
      "-----\n",
      "\n",
      "da25e8bb-7965-4bef-b8cb-c24646c1717b\n",
      "Four expressions for multinomial logistic regression: (1) predicted probability for class k given input and weight matrix via softmax, interpreted as conditional probability of class k; (2) probability of a one-hot encoded label vector given input and weights, as product over classes of predicted probabilities raised to label indicators; (3) loss function as negative sum over classes of label indicator times log predicted probability; (4) gradient of loss with respect to weight vector for class k as difference between predicted probability and label indicator, times feature vector.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in math_expression_descriptions_opt:\n",
    "    print(x.math_expression_description_id)\n",
    "    print(x.text)\n",
    "    print('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72734a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import unzip\n",
    "\n",
    "from math_rag.application.models.embedders import EmbedderInput\n",
    "\n",
    "\n",
    "inputs, input_id_to_item = InputCreatorUtil.create(\n",
    "    math_expression_descriptions_opt, lambda x: EmbedderInput(text=x.text)\n",
    ")\n",
    "outputs = await default_embedder.concurrent_embed(inputs)\n",
    "descriptions, embeddings = unzip(\n",
    "    (input_id_to_item[output.input_id], output.embedding) for output in outputs\n",
    ")\n",
    "descriptions, embeddings = list(descriptions), list(embeddings)\n",
    "await math_expression_description_opt_repository.insert_many(descriptions)\n",
    "await math_expression_description_opt_embedding_repository.upsert_many(descriptions, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dd1ea",
   "metadata": {},
   "source": [
    "### 5. MathExpressionGroup, requires: MathExpressionDescription, MathExpressionContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88992ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_descriptions = await math_expression_description_opt_embedding_repository.group(\n",
    "    grouper_service.group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80ad33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import Record\n",
    "\n",
    "\n",
    "grouped_records: list[list[Record]] = []\n",
    "\n",
    "for descriptions in grouped_descriptions:\n",
    "    ids = [x.id for x in descriptions]\n",
    "    records = await math_expression_description_opt_embedding_repository.client.retrieve(\n",
    "        collection_name=math_expression_description_opt_embedding_repository.collection_name,\n",
    "        ids=[str(id) for id in ids],\n",
    "        with_payload=True,\n",
    "        with_vectors=True,\n",
    "    )\n",
    "\n",
    "    for record in records:\n",
    "        # remove some data for a clener diagram\n",
    "        record.payload['description'] = record.payload['description'][:50]\n",
    "        record.payload.pop('math_expression_description_id')\n",
    "        record.payload.pop('math_expression_index_id')\n",
    "        record.payload.pop('timestamp')\n",
    "\n",
    "    grouped_records.append(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730e43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'math_expression_id': '419e63ac-31f8-4c46-a058-0705a0b746f2',\n",
       " 'math_expression_description_id': 'ff8ec75f-7c82-4d0f-a71d-296807e59033',\n",
       " 'math_expression_index_id': '6cafc7fa-982b-4285-92d0-a53dd2a8453a',\n",
       " 'timestamp': '2025-07-12T22:43:37.991873',\n",
       " 'description': 'square matrix with n rows and n columns'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouped_records[0][0].payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56eb769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "os.environ['NUMBA_CPU_FEATURES'] = str()  # avoid kernel crash on arm\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b2d88",
   "metadata": {},
   "source": [
    "#### Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ac99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    centers=5,\n",
    "    n_features=10,\n",
    "    cluster_std=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    metric='euclidean',\n",
    "    random_state=None,\n",
    ")\n",
    "X_umap = reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cfb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": {
          "bdata": "BAQDAQIBAAQAAwQDBAACAQQDAwAEAgMBAAACAAQDAgEEBAICAAAABAEAAwIEBAMEAgACBAMBAQEBAgQBAAABAQAEAQIBAwMDAQQCAwMDBAMBAgQCAAICAwABBAQEAgABAAQDBAQCAwACAQMAAAQEBAIEAAQDAgIEBAQCAgIEAAIAAgMBAQMDAQMABAECAAECAAICAgIDAgQEAwADAAEDAAQCAQQCBAABAAEEAgMEAAQCAQADBAAAAAQEAgMBAQMDAAMDAAMAAQADBAQCAgADBAMEAQQCBAIEAAACAgQABAMEAAECBAMCAwMAAAQDAQEBAAIEAgEEBAMABAICAAICBAMDAgEEAQQAAAMBAQMEAQQCAQMBAwMBBAAAAAIDAgEEAQAABAEAAQMAAAADAwEDAAIDAwECBAAAAAICAAACAgEAAwAEAwIBAAEDAwECAwIBAQQBAgIDAAMEAAMCAAIEAgQCAQEDAwIBAQIBAwMCAQADAwMCAQMBAAMCAQEBAQMEAAQBAwIAAAQCAQEDAwEBAgADAQIDAAECAAIBAwECAQQAAAQBAQMEBAIDBAQEBAECAwIBAgIBAQQCAQACBAEDAAMBBAQAAAQDBAIEAAABAwEAAQQDAAIBAAABAgEAAwMDBAECAQQCAwICAwIAAAQDAwAEAQA=",
          "dtype": "i1",
          "shape": "500, 1"
         },
         "hovertemplate": "UMAP1=%{x}<br>UMAP2=%{y}<br>cluster=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "BAQDAQIBAAQAAwQDBAACAQQDAwAEAgMBAAACAAQDAgEEBAICAAAABAEAAwIEBAMEAgACBAMBAQEBAgQBAAABAQAEAQIBAwMDAQQCAwMDBAMBAgQCAAICAwABBAQEAgABAAQDBAQCAwACAQMAAAQEBAIEAAQDAgIEBAQCAgIEAAIAAgMBAQMDAQMABAECAAECAAICAgIDAgQEAwADAAEDAAQCAQQCBAABAAEEAgMEAAQCAQADBAAAAAQEAgMBAQMDAAMDAAMAAQADBAQCAgADBAMEAQQCBAIEAAACAgQABAMEAAECBAMCAwMAAAQDAQEBAAIEAgEEBAMABAICAAICBAMDAgEEAQQAAAMBAQMEAQQCAQMBAwMBBAAAAAIDAgEEAQAABAEAAQMAAAADAwEDAAIDAwECBAAAAAICAAACAgEAAwAEAwIBAAEDAwECAwIBAQQBAgIDAAMEAAMCAAIEAgQCAQEDAwIBAQIBAwMCAQADAwMCAQMBAAMCAQEBAQMEAAQBAwIAAAQCAQEDAwEBAgADAQIDAAECAAIBAwECAQQAAAQBAQMEBAIDBAQEBAECAwIBAgIBAQQCAQACBAEDAAMBBAQAAAQDBAIEAAABAwEAAQQDAAIBAAABAgEAAwMDBAECAQQCAwICAwIAAAQDAwAEAQA=",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "f4"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f4"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "UMAP Projection of 5 Random Clusters"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP1"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'UMAP1': X_umap[:, 0],\n",
    "        'UMAP2': X_umap[:, 1],\n",
    "        'cluster': y,\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='cluster',\n",
    "    hover_data=['cluster'],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff01fd",
   "metadata": {},
   "source": [
    "#### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f69ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [r for grp in grouped_records for r in grp]\n",
    "vectors = [r.vector for r in records]\n",
    "cluster_labels = [i for i, grp in enumerate(grouped_records) for _ in grp]\n",
    "\n",
    "# figure out which payload keys exist across all records\n",
    "payload_keys = set().union(*(r.payload.keys() for r in records))\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    metric='cosine',\n",
    "    random_state=None,\n",
    ")\n",
    "X_umap = reducer.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00c622bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "square matrix with n rows and n columns",
           "419e63ac-31f8-4c46-a058-0705a0b746f2",
           "7ae13116-0eee-4378-8e92-ac78edd13b04",
           0
          ],
          [
           "Matrix with m+1 rows and N columns, designed for m",
           "1c81af1a-5b95-405d-a97d-2508d93679c5",
           "96644ea9-4260-4928-9783-7fe5a24caad0",
           0
          ],
          [
           "Describes matrix multiplication between a matrix w",
           "b844af84-aadc-4a4f-b05b-eaebed448452",
           "cb0f6d72-36de-43af-a828-799220fc352d",
           0
          ],
          [
           "the point at which a function's Taylor series is e",
           "914ac134-d758-49ec-b277-7aa44e8a1028",
           "08093a33-0ee2-47ab-a3bf-9a0df9939300",
           1
          ],
          [
           "Second-order Taylor (quadratic) approximation of a",
           "87a3410c-efd3-4efc-9def-9b808c7a8531",
           "3f422f17-dd2a-47be-9a2f-4207d96547f7",
           1
          ],
          [
           "the point where a function is expanded in its Tayl",
           "92af7b43-2138-4bf3-b25a-0d6d1dcdb62e",
           "6282ae34-88fc-459a-a8b6-bcdd4586fe9a",
           1
          ],
          [
           "Point in the domain of a multivariate function f, ",
           "959575ea-70b5-4e9d-89e4-205391719f84",
           "6714159b-936a-4f39-b51c-c5f39cd42ab0",
           1
          ],
          [
           "Differentiable single-variable function used in Ta",
           "8d022fa6-1ed4-437e-8263-c99b94316463",
           "692f3ce7-a04c-4d0a-997b-4348106597a1",
           1
          ],
          [
           "A multivariate function of a vector variable x, co",
           "5a43706b-5c67-4c29-b0b8-19f54ccca799",
           "785fa119-51a6-4e1d-9495-b117464fb193",
           1
          ],
          [
           "Second-order Taylor approximation of a differentia",
           "c609bb13-0b3b-4d6e-a0f8-3cb446ce9dd0",
           "aa76896f-f202-4eb9-aecf-3263a97220a4",
           1
          ],
          [
           "Value of a function evaluated at variable x, subje",
           "2dab51f9-72a7-432c-b1d8-3ca4d1ac0e5c",
           "cec7a5a8-6efb-46aa-89a9-112777ac6658",
           1
          ],
          [
           "Taylor series expansion of a differentiable functi",
           "bc64ef6b-c343-4585-b909-7c60250762a1",
           "d124c8fa-4d1c-431e-b787-13fb68eee00e",
           1
          ],
          [
           "Function of a vector variable defined on a multidi",
           "a2ec0f5c-0489-419c-bcdf-b78f6b030332",
           "d81b6856-5637-4680-ba3a-e84fb74cb88f",
           1
          ],
          [
           "General function evaluated at a vector variable, r",
           "3bbfc75f-8c1c-46d2-9dac-46453fafdb4c",
           "4b54ebe8-2386-45d1-aeed-92bd24e70eb9",
           2
          ],
          [
           "Function minimized using Newton's method, evaluate",
           "4db545b7-dda4-4869-8168-a8a3f9260cc8",
           "9aa021f2-dd4a-466a-abe4-86af1fb221c4",
           2
          ],
          [
           "Function minimized during optimization, approximat",
           "0af7801d-1603-4407-bc51-b3b161fa0f3d",
           "1016469e-a765-4259-bbad-254bf6787379",
           3
          ],
          [
           "Function minimized by quadratic approximation at e",
           "6ac0847e-8af6-4f31-a74b-6aa2c27674cc",
           "140a6613-ed1d-440a-bad6-ef7e263baf31",
           3
          ],
          [
           "Function of a single variable, shown as a black cu",
           "ddacecc3-74a7-4ffe-8602-2c1214f6370a",
           "3f7117ca-d821-4359-96b8-156bc6863393",
           3
          ],
          [
           "Function approximated by a quadratic at point xâ d",
           "87457692-09fb-471c-a01a-adfdc81f211f",
           "8bb3fc08-7b1a-42af-ab44-76ac062034a1",
           3
          ],
          [
           "Point where the quadratic approximation of a funct",
           "e0725888-8554-4e94-8035-9cda8e24a36b",
           "93f6f81b-ddb3-4694-a275-1939c56c7607",
           3
          ],
          [
           "Function of a single variable minimized using quad",
           "0445c80b-1df9-4099-849a-bb28377f3d86",
           "a83e3aec-6a67-4999-ad1d-80f659e37f8c",
           3
          ],
          [
           "Point obtained after the first step of minimizing ",
           "5cb242a0-132d-48b1-9836-7a636aed2227",
           "e8378a05-6d06-436f-94cf-a55cc57916b6",
           3
          ],
          [
           "The value representing the outcome of a Bernoulli ",
           "2ce94e57-645d-4b8b-9c01-232b8bb16e9d",
           "76443be2-9782-44a7-a249-230586242ee0",
           4
          ],
          [
           "The value representing the second outcome of a Ber",
           "56aed6fb-05f7-4226-92ab-1c2d96ef2262",
           "b9ab5a2f-24de-4e8d-b819-e47af8083edd",
           4
          ],
          [
           "Parameter indicating the probability of success in",
           "822a1f93-c29d-4e56-a285-817170f95f75",
           "da311180-27ed-4864-89e2-dc70a6d49b1b",
           4
          ],
          [
           "Probability mass function for a Bernoulli random v",
           "ecba3c86-29bd-405f-9b28-3423f410f1c4",
           "dbc4182d-bc16-48de-8454-4286926c2aa5",
           4
          ],
          [
           "L2 regularization term is a penalty added to the l",
           "9d168684-9984-477e-8943-7e3bb0f07084",
           "118bf9f8-a215-4a19-810f-8833cb0ab028",
           5
          ],
          [
           "L2 regularization is a technique added to the erro",
           "24204848-660e-48a9-afc4-a43ab96e7a2f",
           "18c24566-8b34-4118-bfb7-67dbb60289a9",
           5
          ],
          [
           "L1 regularization norm, non-differentiable, requir",
           "c905630a-1027-45da-af4f-81f4e68e5609",
           "2b87b52a-4f22-4f0d-a043-eeb57e08c6af",
           5
          ],
          [
           "L1 norm regularization in logistic regression is n",
           "5988b076-7d8b-4405-becf-168b24cb942e",
           "4a04af8f-f405-4b23-8d6a-407640d6a7a9",
           5
          ],
          [
           "Regularization factor on the diagonal of the regul",
           "7fccc43d-ff51-40eb-bab6-7edd0ca10216",
           "6b55504d-5537-4fa7-b2d8-e4f5316280af",
           5
          ],
          [
           "A type of regularization used in optimization meth",
           "ca3175eb-379d-483b-8c04-8f94c10ccc69",
           "a8671c7e-5598-4e13-9ce3-e7acd66cf4ab",
           5
          ],
          [
           "L2 regularization, a regularization technique, is ",
           "789b5fd9-b998-40f7-9717-cc877815facd",
           "bcac7a2d-9137-4128-aed3-8b9ea3495e61",
           5
          ],
          [
           "Regularization term based on the squared L2 norm a",
           "f1229e73-bd84-4f8d-8453-50debfc1bca5",
           "ddf6f6df-8d8a-4529-b1b4-653453f81c5a",
           5
          ],
          [
           "Error function for multi-class classification: neg",
           "a62b8ce6-8241-4ca5-ae5c-ace7c30065d2",
           "11c75987-1ae7-4436-8a16-672e0ad73691",
           6
          ],
          [
           "Defines the loss function for a single example in ",
           "38e31b4f-c4df-44f3-9053-930c30977200",
           "98b7bad6-3115-4d1c-8ae5-f27a489a2a51",
           6
          ],
          [
           "Log-likelihood of observed labels given inputs, ca",
           "0c096fad-47b4-429c-b594-c4a6f47897b4",
           "e2f4170b-0602-4b97-86f2-f92ddff18dbe",
           6
          ],
          [
           "Output vector dimension from the softmax function ",
           "3acfbca5-ff5f-4a6a-852a-ac71d98621dc",
           "03b313a2-b356-4226-bc5c-fd6352080a5c",
           7
          ],
          [
           "Total number of classes in the softmax function, d",
           "a59723bd-b844-4ea7-a78c-dcecce4a6f06",
           "e5f9817b-a539-4028-8293-6f9ccfa7213d",
           7
          ],
          [
           "Dimensionality of the vector output by the softmax",
           "3b75a0ac-3427-41cb-af96-47de8ca2ec2d",
           "f14d1137-3bc7-4612-921a-036fa030350b",
           7
          ],
          [
           "Total number of classes in the multinomial logisti",
           "e70eedd6-1355-4f6f-86ec-86887b34f951",
           "46b05d2c-b6b1-434d-87b7-417da8de6cdc",
           8
          ],
          [
           "Total number of classes in the multinomial logisti",
           "e86f1977-6287-4a9c-bba2-ceed19298594",
           "a8acca44-58bc-47c9-82e6-5239557e6b9e",
           8
          ],
          [
           "Collection of m basis functions, indexed from 1 to",
           "0cf7f3cb-7a51-4928-8c7d-95148c1b406b",
           "13711428-a7d3-4370-9a4c-37fd336ee83f",
           9
          ],
          [
           "The j-th basis function in a generalized linear mo",
           "c91d42ad-2831-479e-b9c6-d6b959bb7813",
           "8b9f2a24-64cd-4f1f-80ba-1eef788eb40c",
           9
          ],
          [
           "Function mapping an n-dimensional real vector to a",
           "6fbaa49c-e80d-476d-be4f-601e55b7ae46",
           "978dc19f-7e3a-4914-8e68-73cab73ecaeb",
           9
          ],
          [
           "Variable denoting the number of features in the fe",
           "9e7ca3f1-a0c1-443f-96d9-1e85fa646631",
           "a2e8e545-022b-44f2-8f3d-f3ccb3be037e",
           9
          ],
          [
           "A feature mapping function takes an n-dimensional ",
           "3e43abb1-daaf-4472-83e8-106364b84cc5",
           "d1cc532a-c010-457a-9ab7-b111d7a9f779",
           9
          ],
          [
           "Number of basis functions used in feature mapping ",
           "df5c241d-e9f4-487e-85f7-be8e792680f7",
           "d784effd-d0f7-4788-83b6-31bbe030e3b0",
           9
          ],
          [
           "A vector of weights used to combine outputs of bas",
           "62760521-71bf-4391-95ab-25dfb1d9f26e",
           "168a7e35-5e45-4022-9584-c0106558822b",
           10
          ],
          [
           "Defines a function of an input vector and a parame",
           "2e2d7309-474a-4b67-ac5b-51010f2abd6a",
           "f9de5a73-6cab-4889-b988-b5564d6ae7df",
           10
          ],
          [
           "Weight matrix for the model's first layer; each ba",
           "45c88463-43f7-4e53-b205-899e658b5546",
           "810a052c-5f5f-4caf-9c8b-f15be91f1717",
           11
          ],
          [
           "Weight parameter for the i-th input variable in th",
           "738d7aca-aeb3-4e19-aa1d-7e5b2038275c",
           "a63fd60f-0c9f-4660-982e-4f0e40fb0814",
           11
          ],
          [
           "Index specifying the position of a weight in the w",
           "d3544814-1406-43d5-bb24-b85dcefd5c07",
           "46f509e1-2c33-40fc-bce1-77d893502bc9",
           12
          ],
          [
           "Index identifying a specific basis function within",
           "a6fd39c4-051f-4ce3-bf60-7df4d6160482",
           "7d7fa6c3-5ede-4951-8336-d9dbfb556f6d",
           12
          ],
          [
           "Index identifying a specific basis function within",
           "c08fcca2-587b-416d-9718-5a6132cf6b8e",
           "e17de04e-c810-46ec-8024-418161c521d0",
           12
          ],
          [
           "Point in n-dimensional space where function f is e",
           "b0663387-5ccb-45aa-99a2-351fe888345d",
           "262c4acc-40a5-4517-bb76-f57b2ef015c2",
           13
          ],
          [
           "Hessian matrix of a vector-valued function evaluat",
           "fed299db-93ca-474d-a7fd-c3220173b0be",
           "eb2b17b9-dbb9-4b3e-ac75-6394ae24c9ff",
           13
          ],
          [
           "Activation function used as the mean function in g",
           "990c4f1e-9bd9-443c-9fe3-3ae2a11138d1",
           "420e78d7-f099-496c-8e95-e5cbc56c2ff3",
           14
          ],
          [
           "Activation function in generalized linear models t",
           "29ed660a-3900-42b6-95ba-c5ef34d16dcf",
           "a708c820-a0f1-4739-8b0b-bf44b16ca624",
           14
          ],
          [
           "Mean parameter of a distribution in the exponentia",
           "79454c76-8265-4f2e-b602-0da69c0991f2",
           "eb77a088-7209-4ba0-8cac-4f1f716e2188",
           14
          ],
          [
           "For every non-zero vector, the quadratic form defi",
           "762002c2-22b8-42ba-9406-1c5d7aeb8686",
           "8ac42ad4-8ff8-4a33-8160-bf96f5201c77",
           15
          ],
          [
           "Non-zero vector used in the condition for positive",
           "112bbc7d-2062-4dfe-8a07-d068d682d1d6",
           "c9a37cfa-061e-477f-a51f-34be43a48c39",
           15
          ],
          [
           "For every non-zero vector, the quadratic form defi",
           "a3401ab5-b9a8-4129-8aec-457b8fd379a7",
           "f93d9e77-db3f-4d58-ba3a-cb488e891133",
           15
          ],
          [
           "Activation function in a generalized linear model ",
           "836dc01e-ae03-4a39-bfec-8964886861bb",
           "18f308ca-f1e3-4ca6-bc7c-c1970fa9d290",
           16
          ],
          [
           "A generalized linear model output is the activatio",
           "6db4170b-d700-48e8-a806-21377160ddae",
           "4ef8dca9-e241-4028-961f-459febc83b2c",
           16
          ],
          [
           "A general model output is computed by applying an ",
           "fd44a447-7b06-4c95-a2ca-d3605467adb6",
           "78e21b2a-e17f-486b-ac10-719bd718bbb9",
           16
          ],
          [
           "Activation function applied to the scalar product ",
           "ebf8f2dc-1dbd-462a-bd71-e42e03404802",
           "7abf712c-997d-439f-960f-265892661c44",
           16
          ],
          [
           "A linear combination where a transposed weight vec",
           "82aaf573-0777-4656-9052-fd6d2324f29b",
           "35d1e670-7d03-4dea-8186-5084c920fa01",
           17
          ],
          [
           "Mean parameter of a distribution as a function of ",
           "0ac0b483-5eeb-4e5a-ae0e-b585de97bf1a",
           "4f163b6b-7af2-40b0-8747-d0f98c58bbbd",
           17
          ],
          [
           "A function mapping n-dimensional real vectors to r",
           "a880d03f-b657-4a0b-94b2-4a21ec10e55a",
           "13aed26b-1b95-4992-9f9d-ec732068fc08",
           18
          ],
          [
           "The Hessian matrix of a scalar-valued function of ",
           "b4a8f00b-9179-4c73-ad5c-9ad78c818849",
           "324e0d4e-6183-4cbd-873c-c45ad97baa0d",
           18
          ],
          [
           "A real-valued function mapping an n-dimensional ve",
           "94a0fd41-b4fa-4d42-9da1-576f3db17566",
           "fe5ad588-c7ea-403d-bd15-5d5d2590800b",
           18
          ],
          [
           "The k-th component of the indicator vector for a c",
           "b1e52b90-5b5f-4820-b046-96f62d1bbcd3",
           "2f422f9a-8ca5-47a4-b2e4-1069ac534c74",
           19
          ],
          [
           "Probability that the k-th indicator variable equal",
           "2128c416-8fc9-43e2-a25f-6abeb8af1da5",
           "322be170-003a-41e4-9000-fee7f297ae04",
           19
          ],
          [
           "The k-th component of the indicator vector equals ",
           "1237eb4f-76a6-4b6c-845f-e566189a200b",
           "63befcd8-a020-48a0-ac1b-1b0f91eec8f9",
           19
          ],
          [
           "Probability that the k-th indicator variable in a ",
           "f2195c08-4e22-47a6-980e-a81b78616709",
           "a77ae55d-66bb-407f-b481-4c7566c6df21",
           19
          ],
          [
           "The indicator vector for a categorical variable ha",
           "99f8b6cb-8fe3-4927-85bd-fa1c5063aa8b",
           "cf11ea81-06f6-4a10-b51d-4d12f7a4bfc2",
           19
          ],
          [
           "Index of a multinomial variable's outcome; the k-t",
           "afb1e47f-209f-43fb-ac36-2a6a8e4791fb",
           "e5079e1e-478f-452a-9998-d9fa8f993941",
           19
          ],
          [
           "The parameter vector mu sums to one across all com",
           "6ba53c1b-5514-44cb-b44a-0ebbde7575f8",
           "3e9d64e9-2a9e-4079-85b4-3b0d39382841",
           20
          ],
          [
           "The k-th component of a parameter vector represent",
           "3824fc7b-4028-4b57-a60d-86b1e96e12da",
           "c92980ff-b526-4e7f-a2d6-57f1d443a3ed",
           20
          ],
          [
           "Key components of logistic regression: model outpu",
           "c21bc376-883b-4c40-8e0b-3c373f709f1c",
           "03a65383-2e4b-477b-8088-aa96ca2d8656",
           21
          ],
          [
           "Defines the output of a binary logistic regression",
           "d7e30969-27cb-4dda-a057-66cebc709700",
           "1630e775-bac6-4ae9-a3b1-a07899ef722a",
           21
          ],
          [
           "Probability in multinomial logistic regression tha",
           "1ba16da8-6340-4e4b-969b-b7cc85089829",
           "444670e0-f137-4df6-8ab1-dce72b690832",
           21
          ],
          [
           "Four expressions for multinomial logistic regressi",
           "6de87ad1-51dd-4916-a05e-9dac1feaf0dc",
           "4963b0c1-9dfd-47a3-877e-40ca22d50d4f",
           21
          ],
          [
           "Probability that an input vector belongs to class ",
           "120a4a67-02d7-4c2b-b4fa-de7841d6211d",
           "64f7d526-bf1e-4850-bcea-d027bfca1828",
           21
          ],
          [
           "In multinomial logistic regression, the model cons",
           "c56ddeb6-4968-4eef-96ed-6be8a4bb074b",
           "7ed6b19a-fef1-41e3-8a26-4616cb8621d8",
           21
          ],
          [
           "Hessian matrix that is positive semi-definite but ",
           "303ed540-c38c-4890-b3e2-0100357be2ef",
           "039f74b9-71e8-4d64-a2a1-01cc9d3d3660",
           22
          ],
          [
           "The Hessian matrix is positive semi-definite but n",
           "5a088fdd-8a6b-41a5-ae0a-0caddca2979a",
           "0d1cdf53-fc22-4cea-9fc6-789c638486e0",
           22
          ],
          [
           "Hessian matrix in Newton's optimization method, ch",
           "859d57ce-9aaa-4e25-bc89-afe1f570bd90",
           "26791941-c8e1-4c78-a72b-0e1a25f125d5",
           22
          ],
          [
           "Hessian matrix in Newton's optimization method, re",
           "980cff84-0a35-472c-8736-133829432a02",
           "3565f08d-8178-4ec8-b6c7-adccc2885861",
           22
          ],
          [
           "Hessian matrix in logistic regression optimization",
           "1b58f46a-9fa6-4e4b-9d90-9b19a187daba",
           "04e4a960-fe65-426e-b83e-08cdefe9d1bf",
           23
          ],
          [
           "Hessian matrix of the cross-entropy error function",
           "42e4a2d8-eba5-4b01-938d-4984d9a7f914",
           "0bffe0a4-ed9c-4730-bbea-12f562e34210",
           23
          ],
          [
           "Hessian matrix of the cross-entropy error function",
           "9a68ea41-280f-40b3-a0ca-b2508e13cc62",
           "43b047b8-0575-4cc6-8146-27a98646574d",
           23
          ],
          [
           "Bold capital H at this index denotes the Hessian m",
           "1d50a194-350a-4de3-80eb-3b679c8c6fa8",
           "60e1d318-6a27-4807-889e-79f36821b451",
           23
          ],
          [
           "Hessian matrix of the cross-entropy error function",
           "ef873995-2bd5-447d-a7c0-ff5d48d0ba33",
           "94d0bf6b-462b-46a5-bb69-c6ff526bb669",
           23
          ],
          [
           "The symbol denotes the Hessian matrix for the cros",
           "3ae5141d-273c-4743-8676-f487a7f54d01",
           "9b722c49-66f7-41b2-8d80-c33281bade75",
           23
          ],
          [
           "Symbol denotes the Hessian matrix associated with ",
           "d8271062-3eec-4c90-9ce2-5d951e77c77d",
           "a231c544-61b0-425f-b295-ad1a5c13ebde",
           23
          ],
          [
           "The Hessian matrix for the cross-entropy error in ",
           "091744f9-a6d0-47b0-a308-242983bdb956",
           "c2c826f6-1893-4753-872b-46489f22c28f",
           23
          ],
          [
           "Weight update rule in stochastic gradient descent ",
           "e996359d-b916-460e-bae2-d426830a88d8",
           "5ad7cdeb-1f1f-4863-b280-76a9d8b8a54b",
           24
          ],
          [
           "Weight update rule for stochastic gradient descent",
           "f48751a7-2887-483a-b97e-64c417776b59",
           "62822a4e-cf73-4357-a455-ea130f663226",
           24
          ],
          [
           "The update rule for the weight vector in Newton's ",
           "59f332ba-1bea-425c-b3a8-d15ad18dfef0",
           "b5f78abe-fe60-4ac5-a556-19203908f5e3",
           24
          ],
          [
           "The target expression refers to the input example ",
           "8c1663cf-44bb-4faf-9b21-b246b912cf7d",
           "20a6c655-6f70-4dc1-a513-ad6198c09f4c",
           25
          ],
          [
           "The target expression represents the model for cla",
           "b560d279-f2d3-4275-9772-8ca555701f65",
           "303b47a9-25c9-443b-aeb0-246e62abaa95",
           25
          ],
          [
           "The target expression indicates the number of weig",
           "7c8b038a-3559-4442-8dc7-c700165d9341",
           "82c532d6-1a6c-4a2e-a14e-9ac028e834c7",
           25
          ],
          [
           "The target expression represents the model for cla",
           "5ab38980-381a-4caf-88b5-e543bd6f95c9",
           "e7bcc610-8dbd-4b07-aba4-0466ee7aeb70",
           25
          ],
          [
           "Models indexed by k, each corresponding to a speci",
           "7fb4993f-e7ed-41de-893f-6e8feac70ef7",
           "5d5e0713-2d24-4358-9799-3bb7497b7700",
           26
          ],
          [
           "Variable representing a specific class label, used",
           "ae20cef5-27b1-4019-8778-2333a64a830f",
           "803f6069-0bd7-48aa-9ae4-0cf5995e5e09",
           26
          ],
          [
           "Class label index representing one of K possible c",
           "b7fb772f-d3a8-4639-a364-3f23610d57df",
           "c728482e-410a-4cc9-85fc-357f45428102",
           26
          ],
          [
           "Variable denoting the index of a specific class am",
           "d09012c2-2368-4103-acf6-6126b923988a",
           "cdcddd1b-1854-4c6d-9f68-096d35a12bfe",
           26
          ],
          [
           "Scalar product of the k-th class weight vector and",
           "b793541b-d68a-487b-b305-9a7b79cc923f",
           "49537bab-48dd-4a20-a45d-5b75b0e180ad",
           27
          ],
          [
           "Matrix of K column vectors, each representing a we",
           "f1f50ab0-e41f-4408-8f00-76d2973acec5",
           "5682c53a-b729-44ec-a695-f2f8f581b33d",
           27
          ],
          [
           "Weight vector for class k, one of K weight vectors",
           "9b5ffe0b-5ed2-4763-ae6f-16c9737e2ac8",
           "90601225-03e0-4aa6-b0bb-a988316df232",
           27
          ],
          [
           "Weight vector for the k-th class in multinomial lo",
           "6f16283c-ca66-4d29-900a-117089ff0519",
           "9776b73f-ba1d-4422-b306-58208706b1b2",
           27
          ],
          [
           "Dot product of class k's weight vector and input e",
           "e4b96061-9dff-458a-87cd-9b5580b07b8d",
           "c0b0fd68-ae61-442d-a25a-bec80beeb5fc",
           27
          ]
         ],
         "hovertemplate": "UMAP1=%{x}<br>UMAP2=%{y}<br>description=%{customdata[0]}<br>math_expression_id=%{customdata[1]}<br>id=%{customdata[2]}<br>cluster=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "AAAAAQEBAQEBAQEBAQICAwMDAwMDAwQEBAQFBQUFBQUFBQYGBgcHBwgICQkJCQkJCgoLCwwMDA0NDg4ODw8PEBAQEBEREhISExMTExMTFBQVFRUVFRUWFhYWFxcXFxcXFxcYGBgZGRkZGhoaGhsbGxsb",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "qPd9QMnWbkDmQHpApoGsQC7jqkD2krBAPPakQAfxr0Dga6BAN7ivQM3MpkDeYrRA7mWEQDY8i0BsuIlAQWCSQD9jkEADNYxAFKaYQPBsnEAc+5VAxnWXQN1DRMDBVkfAkBVKwPZIVMDDAOlAHBXkQCLr4kBvd+FAKODcQE+E4UBqcedAM8TkQF8euD3nTUe+Hrf3vHNjGb/euye/ng0Hv52USL/rDF+/nQsVQJhGCEDkRCFAkQ0bQEiKNEDTbAlA+z3kP9JbBkB4guI/YOHVP+2QzD/o5Lg/DBHIP3Goi0BR+I5A3u2dP+rulT/c+Xk/M6ydQNQgpUBPSKFAbH7OP0QFuj9kteI/SxvLP1efA0D4sOo/P0BPQFkVjUATWH1Ao8otwGhhHcDBLCXARDsvwM/jMMDp9ibA9MUVwLFGF8DMOHS+lCPQvt7uVb/3Tjm/o+CSv5ijKL+Bsp9AeUqgQOTznkDvs6NAuSOsQD6clUBNZZ9AP5qoQMlzq0CX8ZhAZ9SgQKbUoECkgstAIorOQLM+yUCdqIC/ea6zvzJrjr/1bqi/WIjav4VWq7+h07i/WxGvv/fDrL9veOO/vCW+vwykzb/jhrm/",
          "dtype": "f4"
         },
         "xaxis": "x",
         "y": {
          "bdata": "5BOnQMfApEDojKFAkTUOQbnvD0FljQtBZCIMQRncD0E9BwpB3KwSQTjsDUGieQtBOnQBQUqdDkHk8RNBGn4XQaa3GEHioBNBHoEXQa+aFkF5/RVBcjkXQfRY8EDKnetAid3uQBrY6UBklNVAF/HYQLW03ECmct9A7EXPQE951kCUr9VA62rTQGow5EAJm99AwRvlQPC1A0EgDwNBgekEQZaN/UAug/5AgayxQBdcq0AH3qtAQsW4QDq6rkBTtbNAgdGoQIm6oUATgK5AkTmzQP5uu0AHNrhA8327QN5rwUCxpbtA9sCcQOTin0Ad2JtA7SG8QOzvvUAtwbpAYLuZQHEnmkDfIpdAIYKgQOfnmkBOSptAMii5QOIHuUBF2sBATAbqQLu44kBpq+xAeWnfQEsd7EDzFOJAfijrQFap6kBDuthA/9fZQCWD3UDXuOBAWXbdQKtY6EBM4sNAkunDQOS6wECFAcdA6curQDXVo0BqkapAA1OrQNbOrEAMsKBAuM2mQAfGo0AHV9dARSzVQBHs0EDiZeFAxmvfQBPl7UCWYt5A4lr3QINu/0ClNgBBmjYCQafy7kC/gvBA8S7vQN3680DoV+hA",
          "dtype": "f4"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP1"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = []\n",
    "for x, y, label, record in zip(X_umap[:, 0], X_umap[:, 1], cluster_labels, records):\n",
    "    row = {\n",
    "        'UMAP1': x,\n",
    "        'UMAP2': y,\n",
    "        'cluster': label,\n",
    "        'id': record.id,\n",
    "    }\n",
    "    row.update(record.payload or {})  # add all payload fields\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='cluster',\n",
    "    hover_data=list(payload_keys) + ['id', 'cluster'],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a42c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "from math_rag.application.models.assistants.inputs import MathExpressionComparator as AssistantInput\n",
    "from math_rag.application.utils import GroupPrunerUtil\n",
    "from math_rag.core.models import MathExpressionGroup\n",
    "\n",
    "\n",
    "grouped_math_expression_ids = [\n",
    "    [description.math_expression_id for description in descriptions]\n",
    "    for descriptions in grouped_descriptions\n",
    "]\n",
    "\n",
    "for math_expression_ids in grouped_math_expression_ids:\n",
    "    math_expressions = await math_expression_repository.find_many(\n",
    "        filter=dict(id=math_expression_ids)\n",
    "    )\n",
    "    math_expression_contexts = await math_expression_context_repository.find_many(\n",
    "        filter=dict(math_expression_id=math_expression_ids)\n",
    "    )\n",
    "    pairs = list(combinations(zip(math_expressions, math_expression_contexts), 2))\n",
    "\n",
    "    if not pairs:\n",
    "        continue\n",
    "\n",
    "    inputs: list[AssistantInput] = []\n",
    "    input_id_to_candidate_pair: dict[UUID, tuple[UUID, UUID]] = {}\n",
    "\n",
    "    for pair, other_pair in pairs:\n",
    "        math_expression, math_expression_context = pair\n",
    "        other_math_expression, other_math_expression_context = other_pair\n",
    "        input = AssistantInput(\n",
    "            katex=math_expression.katex,\n",
    "            context=math_expression_context.text,\n",
    "            other_katex=other_math_expression.katex,\n",
    "            other_context=other_math_expression_context.text,\n",
    "        )\n",
    "        inputs.append(input)\n",
    "        input_id_to_candidate_pair[input.id] = (math_expression.id, other_math_expression.id)\n",
    "\n",
    "    outputs = await math_expression_comparator_assistant.concurrent_assist(inputs)\n",
    "\n",
    "    candidates = math_expression_ids\n",
    "    candidate_pair_to_is_connected = {\n",
    "        input_id_to_candidate_pair[output.input_id]: output.is_identical for output in outputs\n",
    "    }\n",
    "\n",
    "    math_expression_ids_to_group = GroupPrunerUtil.prune(candidates, candidate_pair_to_is_connected)\n",
    "    math_expression_group = MathExpressionGroup(math_expression_index_id=index.id)\n",
    "\n",
    "    await math_expression_group_repository.insert_one(math_expression_group)\n",
    "    await math_expression_repository.update_group_id(\n",
    "        math_expression_ids_to_group, math_expression_group.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc088db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert updated math expressions to the graph database\n",
    "for math_expression_ids in grouped_math_expression_ids:\n",
    "    math_expressions = await math_expression_repository.find_many(\n",
    "        filter=dict(id=math_expression_ids)\n",
    "    )\n",
    "    await math_expression_graph_repository.insert_many_nodes(math_expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ee39e",
   "metadata": {},
   "source": [
    "## Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f44bd",
   "metadata": {},
   "source": [
    "### 1. MathArticleChunk, requires: MathExpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e063fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expressions = await math_expression_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bfa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathArticleChunk\n",
    "\n",
    "\n",
    "index_to_katex = {\n",
    "    math_expression.index: math_expression.katex for math_expression in math_expressions\n",
    "}\n",
    "chunk_templates = TemplateChunkerUtil.chunk(template, max_window_size=2048, max_padding=256)\n",
    "math_article_chunks: list[MathArticleChunk] = []\n",
    "\n",
    "for i, chunk_template in enumerate(chunk_templates):\n",
    "    indexes = TemplateIndexFinderUtil.find(chunk_template)\n",
    "\n",
    "    if len(indexes) < 2:\n",
    "        continue\n",
    "\n",
    "    formatted_chunk, _ = TemplateFormatterUtil.format(\n",
    "        chunk_template, index_to_katex, omit_wrapper=False\n",
    "    )\n",
    "    # print(_)\n",
    "    math_article_chunk = MathArticleChunk(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_index_id=index.id,\n",
    "        index=i,\n",
    "        indexes=indexes,\n",
    "        text=formatted_chunk,\n",
    "    )\n",
    "    math_article_chunks.append(math_article_chunk)\n",
    "\n",
    "await math_article_chunk_repository.insert_many(math_article_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f6fdc",
   "metadata": {},
   "source": [
    "### 2. MathExpressionRelationship, requires: MathExpression, MathArticleChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89922f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expressions = await math_expression_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")\n",
    "math_article_chunks = await math_article_chunk_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5259e873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2735"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_expected_chunks = sum(\n",
    "    len(math_article_chunk.indexes) - 1 for math_article_chunk in math_article_chunks\n",
    ")\n",
    "num_expected_chunks\n",
    "\n",
    "# TODO why did we get 2389 results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "542a3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionRelationshipDetector as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionRelationship\n",
    "\n",
    "\n",
    "for math_article_chunk in math_article_chunks:\n",
    "    if len(math_article_chunk.indexes) < 2:\n",
    "        continue\n",
    "\n",
    "    start_indexes = math_article_chunk.indexes[:-1]\n",
    "    last_index = math_article_chunk.indexes[-1]\n",
    "    index_pairs = [(index, last_index) for index in start_indexes]\n",
    "    inputs: list[AssistantInput] = []\n",
    "    input_id_to_math_expression_id_pair: dict[UUID, tuple[UUID, UUID]] = {}\n",
    "    input_id_to_math_expression_index_pair: dict[UUID, tuple[int, int]] = {}\n",
    "\n",
    "    for source_index, target_index in index_pairs:\n",
    "        input = AssistantInput(\n",
    "            chunk=math_article_chunk.text, source=source_index, target=target_index\n",
    "        )\n",
    "        inputs.append(input)\n",
    "\n",
    "        source_math_expression = next(\n",
    "            (x for x in math_expressions if x.index == source_index), None\n",
    "        )\n",
    "        target_math_expression = next(\n",
    "            (x for x in math_expressions if x.index == target_index), None\n",
    "        )\n",
    "\n",
    "        if source_math_expression is None or target_math_expression is None:\n",
    "            raise ValueError()\n",
    "\n",
    "        input_id_to_math_expression_id_pair[input.id] = (\n",
    "            source_math_expression.id,\n",
    "            target_math_expression.id,\n",
    "        )\n",
    "        input_id_to_math_expression_index_pair[input.id] = source_index, target_index\n",
    "\n",
    "    outputs = await math_expression_relationship_detector_assistant.concurrent_assist(inputs)\n",
    "    math_expression_relationships = [\n",
    "        MathExpressionRelationship(\n",
    "            math_article_chunk_id=math_article_chunk.id,\n",
    "            math_expression_index_id=index.id,\n",
    "            math_expression_source_id=input_id_to_math_expression_id_pair[output.input_id][0],\n",
    "            math_expression_target_id=input_id_to_math_expression_id_pair[output.input_id][1],\n",
    "            math_expression_source_index=input_id_to_math_expression_index_pair[output.input_id][0],\n",
    "            math_expression_target_index=input_id_to_math_expression_index_pair[output.input_id][1],\n",
    "        )\n",
    "        for output in outputs\n",
    "        if output.relationship_exists\n",
    "    ]\n",
    "    await math_expression_relationship_repository.insert_many(math_expression_relationships)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b852a9",
   "metadata": {},
   "source": [
    "### 3. MathExpressionRelationshipDescription, requires: MathArticleChunk, MathExpressionRelationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b89f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2389"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_expression_relationships = await math_expression_relationship_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")\n",
    "len(math_expression_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce57985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "await math_article_chunk_repository.find_one(filter=dict(index=0))\n",
    "# TODO missing math_article_chunk with index 0??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c042e384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MathArticleChunk(id=UUID('29995be7-5304-4c5e-bdb6-f78370a0bc8b'), math_article_id=UUID('03bd6e71-1c57-4a16-9f8d-86d7a3fc6412'), math_expression_index_id=UUID('6cafc7fa-982b-4285-92d0-a53dd2a8453a'), timestamp=datetime.datetime(2025, 7, 13, 0, 8, 17, 717000), index=142, indexes=[153, 154, 155, 156, 157, 158], text=\"generalized linear models are models that wrap the scalar product of the weight vector and example vector into an activation function [f | 153]. So:\\n\\n[h(\\\\mathbf{x} ; \\\\mathbf{w})=f\\\\left(\\\\mathbf{w}^{\\\\mathrm{T}} \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\\\right) | 154]\\n\\nLet's look at three generalized linear models we've considered so far. For each of them, let's consider four points: (1) how the model is defined, (2) what is the probability distribution to which their output corresponds, (3) how is the loss is defined, and (4) what is the gradient of the loss, which we need for gradient descent.\\n\\nFirst, let's look at the linear regression algorithm:\\n\\n[\\\\begin{aligned}\\nh(\\\\mathbf{x} ; \\\\mathbf{w}) & =f\\\\left(\\\\mathbf{w}^{\\\\mathrm{T}} \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\\\right)=\\\\mathbf{w}^{\\\\mathrm{T}} \\\\boldsymbol{\\\\phi}(\\\\mathbf{x}) \\\\\\\\\\nP(y \\\\mid \\\\mathbf{x}, \\\\mathbf{w}) & =\\\\mathcal{N}\\\\left(\\\\mu, \\\\sigma^{2}\\\\right)=\\\\mathcal{N}\\\\left(h(\\\\mathbf{x}), \\\\sigma^{2}\\\\right) \\\\\\\\\\nL(y, h(\\\\mathbf{x})) & =(h(\\\\mathbf{x})-y)^{2} \\\\\\\\\\n\\\\nabla_{\\\\mathbf{w}} L(y, h(\\\\mathbf{x})) & =(h(\\\\mathbf{x})-y) \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\n\\\\end{aligned} | 155]\\n\\nFor batch learning we use the least squares method (the pseudoinverse), and for online learning we use the LMS rule.\\n\\nLet's look at the logistic regression algorithm:\\n\\n[\\\\begin{aligned}\\nh(\\\\mathbf{x} ; \\\\mathbf{w}) & =f\\\\left(\\\\mathbf{w}^{\\\\mathrm{T}} \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\\\right)=\\\\frac{1}{1+\\\\exp \\\\left(-\\\\mathbf{w}^{\\\\mathrm{T}} \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\\\right)}=P(y=1 \\\\mid \\\\mathbf{x}, \\\\mathbf{w}) \\\\\\\\\\nP(y \\\\mid \\\\mathbf{x}, \\\\mathbf{w}) & =\\\\mu^{y}(1-\\\\mu)^{(1-y)}=h(\\\\mathbf{x})^{y}(1-h(\\\\mathbf{x}))^{(1-y)} \\\\\\\\\\nL(y, h(\\\\mathbf{x})) & =-y \\\\ln h(\\\\mathbf{x})-(1-y) \\\\ln (1-h(\\\\mathbf{x})) \\\\\\\\\\n\\\\nabla_{\\\\mathbf{w}} L(y, h(\\\\mathbf{x})) & =(h(\\\\mathbf{x})-y) \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\n\\\\end{aligned} | 156]\\n\\nFor batch learning we use gradient descent, the Newton's (IRLS) or the quasi-Newton method (BFSG, L-BFSG). We use the LMS rule for online learning.\\n\\nFinally, let's look at the multinomial logistic regression:\\n\\n[\\\\begin{aligned}\\nh_{k}(\\\\mathbf{x} ; \\\\mathbf{W}) & =\\\\operatorname{softmax}\\\\left(\\\\mathbf{w}^{\\\\mathrm{T}} \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\\\right)=\\\\frac{\\\\exp \\\\left(\\\\mathbf{w}_{k}^{\\\\mathrm{T}} \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\\\right)}{\\\\sum_{j} \\\\exp \\\\left(\\\\mathbf{w}_{j}^{\\\\mathrm{T}} \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\\\right)}=P(y=k \\\\mid \\\\mathbf{x}, \\\\mathbf{w}) \\\\\\\\\\nP(\\\\mathbf{y} \\\\mid \\\\mathbf{x}, \\\\mathbf{w}) & =\\\\prod_{k=1}^{K} \\\\mu_{k}^{y_{k}}=\\\\prod_{k=1}^{K} h_{k}(\\\\mathbf{x})^{y_{k}} \\\\\\\\\\nL\\\\left(\\\\mathbf{y}, h_{k}(\\\\mathbf{x})\\\\right) & =-\\\\sum_{k=1}^{K} y_{k} \\\\ln h_{k}(\\\\mathbf{x} ; \\\\mathbf{W}) \\\\\\\\\\n\\\\nabla_{\\\\mathbf{w}_{k}} L\\\\left(y_{k}, h_{k}(\\\\mathbf{x})\\\\right) & =\\\\left(h_{k}(\\\\mathbf{x})-y_{k}\\\\right) \\\\boldsymbol{\\\\phi}(\\\\mathbf{x})\\n\\\\end{aligned} | 157]\\n\\nWe use the same (mutatis mutandis) optimization procedures for model learning as for logistic regression.\\n\\nNotice the commonalities. For all three algorithms, we derived the loss function from the negative logarithm of the probability of labels of the examples from the dataset. We did this by using the normal, Bernoulli, and multinoulli distribution distribution for linear, binary logistic, and multinoulli logistic regression, respectively. Furthermore, for all three algorithms we derived an identical rule (the LMS) for online weights update.\\n\\nThe question is: how come we always get the same weights update rule? Also, what is the relationship between the logistic function and the Bernoulli variable, and between the softmax function and the multinoulli distribution? There seems to be a connection, because in both cases we obtained the cross-entropy error. The answer lies in the properties of the distributions we used to model the labels [y^{(i)} | 158].\\n\\n\\nThe distributions we have encountered so far (Gaussian, Bernoulli, multinoulli), but also some others that are often used in\")]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await math_article_chunk_repository.find_many(filter=dict(math_expression_index_id=[index.id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "757aec11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await math_article_chunk_repository.find_many(\n",
    "    filter=dict(id=[UUID('b2126cf8-d76f-4985-96b1-3aae11bfbd6b')])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c41549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO maybe add batch_find_many_in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87f5b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Batch request 32c81965-fb9b-472a-a72b-b33471ece2f8 is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     27\u001b[0m     input_id_to_math_expression_relationship_id[\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m math_expression_relationship\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m---> 29\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m math_expression_relationship_description_writer_assistant\u001b[38;5;241m.\u001b[39mconcurrent_assist(inputs)\n\u001b[1;32m     30\u001b[0m descriptions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m     MathExpressionRelationshipDescription(\n\u001b[1;32m     32\u001b[0m         math_expression_index_id\u001b[38;5;241m=\u001b[39mindex\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs\n\u001b[1;32m     39\u001b[0m ]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m math_expression_relationship_description_repository\u001b[38;5;241m.\u001b[39minsert_many(descriptions)\n",
      "File \u001b[0;32m/workspaces/math_rag/application/assistants/partials/partial_concurrent_assistant.py:33\u001b[0m, in \u001b[0;36mPartialConcurrentAssistant.concurrent_assist\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconcurrent_assist\u001b[39m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     28\u001b[0m     inputs: \u001b[38;5;28mlist\u001b[39m[AssistantInputType],\n\u001b[1;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[AssistantOutputType]:\n\u001b[1;32m     30\u001b[0m     concurrent_request \u001b[38;5;241m=\u001b[39m LLMConcurrentRequest(\n\u001b[1;32m     31\u001b[0m         requests\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_to_request(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m     32\u001b[0m     )\n\u001b[0;32m---> 33\u001b[0m     concurrent_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mconcurrent_generate(concurrent_request)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# map BoundAssistantOutput to AssistantOutput\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response_list \u001b[38;5;129;01min\u001b[39;00m concurrent_result\u001b[38;5;241m.\u001b[39mresponse_lists:\n",
      "File \u001b[0;32m/workspaces/math_rag/infrastructure/inference/routers/managed_llm_router.py:93\u001b[0m, in \u001b[0;36mManagedLLMRouter.concurrent_generate\u001b[0;34m(self, concurrent_request)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconcurrent_generate\u001b[39m(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m, concurrent_request: LLMConcurrentRequest[LLMResponseType]\n\u001b[1;32m     92\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMConcurrentResult[LLMResponseType]:\n\u001b[0;32m---> 93\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcurrent_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mconcurrent_generate(concurrent_request)\n",
      "File \u001b[0;32m/workspaces/math_rag/infrastructure/inference/routers/managed_llm_router.py:40\u001b[0m, in \u001b[0;36mManagedLLMRouter._llm\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(request, LLMConcurrentRequest):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m request\u001b[38;5;241m.\u001b[39mrequests:\n\u001b[0;32m---> 40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch request \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is empty\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m     inference_provider \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mrequests[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mrouter_params\u001b[38;5;241m.\u001b[39minference_provider\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Batch request 32c81965-fb9b-472a-a72b-b33471ece2f8 is empty"
     ]
    }
   ],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionRelationshipDescriptionWriter as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionRelationshipDescription\n",
    "\n",
    "\n",
    "math_article_chunk_ids = [\n",
    "    math_expression_relationship.math_article_chunk_id\n",
    "    for math_expression_relationship in math_expression_relationships\n",
    "]\n",
    "math_article_chunks = await math_article_chunk_repository.find_many(\n",
    "    filter=dict(id=math_article_chunk_ids)  # TODO\n",
    ")\n",
    "\n",
    "inputs: list[AssistantInput] = []\n",
    "input_id_to_math_expression_relationship_id: dict[UUID, UUID] = {}\n",
    "\n",
    "for math_article_chunk, math_expression_relationship in zip(\n",
    "    math_article_chunks, math_expression_relationships\n",
    "):\n",
    "    input = AssistantInput(\n",
    "        chunk=math_article_chunk.text,\n",
    "        source=math_expression_relationship.math_expression_source_index,\n",
    "        target=math_expression_relationship.math_expression_target_index,\n",
    "    )\n",
    "    inputs.append(input)\n",
    "    input_id_to_math_expression_relationship_id[input.id] = math_expression_relationship.id\n",
    "\n",
    "outputs = await math_expression_relationship_description_writer_assistant.concurrent_assist(inputs)\n",
    "descriptions = [\n",
    "    MathExpressionRelationshipDescription(\n",
    "        math_expression_index_id=index.id,\n",
    "        math_expression_relationship_id=input_id_to_math_expression_relationship_id[\n",
    "            output.input_id\n",
    "        ],\n",
    "        text=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]\n",
    "await math_expression_relationship_description_repository.insert_many(descriptions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
