{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32ba622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from math_rag.application.containers import ApplicationContainer\n",
    "    from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "    application_container: ApplicationContainer\n",
    "    infrastructure_container: InfrastructureContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c020d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 16:25:59,660 - INFO - datasets - config.py:54 - PyTorch version 2.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "RESET = False\n",
    "%load_ext hooks.notebook_hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386180b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92db620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expression_description_writer_assistant = (\n",
    "    application_container.math_expression_description_writer_assistant()\n",
    ")\n",
    "math_expression_description_optimizer_assistant = (\n",
    "    application_container.math_expression_description_optimizer_assistant()\n",
    ")\n",
    "math_expression_comparator_assistant = application_container.math_expression_comparator_assistant()\n",
    "math_expression_relationship_description_writer_assistant = (\n",
    "    application_container.math_expression_relationship_description_writer_assistant()\n",
    ")\n",
    "math_expression_relationship_detector_assistant = (\n",
    "    application_container.math_expression_relationship_detector_assistant()\n",
    ")\n",
    "\n",
    "default_embedder = application_container.default_embedder()\n",
    "math_expression_description_opt_embedding_repository = (\n",
    "    infrastructure_container.math_expression_description_opt_embedding_repository()\n",
    ")\n",
    "math_expression_description_opt_repository = (\n",
    "    infrastructure_container.math_expression_description_opt_repository()\n",
    ")\n",
    "math_expression_description_repository = (\n",
    "    infrastructure_container.math_expression_description_repository()\n",
    ")\n",
    "math_expression_group_repository = infrastructure_container.math_expression_group_repository()\n",
    "math_expression_group_graph_repository = (\n",
    "    await infrastructure_container.math_expression_group_graph_repository()\n",
    ")\n",
    "math_expression_repository = infrastructure_container.math_expression_repository()\n",
    "grouper_service = application_container.grouper_service()\n",
    "\n",
    "math_expression_graph_repository = await infrastructure_container.math_expression_graph_repository()\n",
    "math_expression_relationship_repository = (\n",
    "    infrastructure_container.math_expression_relationship_repository()\n",
    ")\n",
    "math_expression_relationship_description_repository = (\n",
    "    infrastructure_container.math_expression_relationship_description_repository()\n",
    ")\n",
    "katex_corrector_service = application_container.katex_corrector_service()\n",
    "math_expression_context_repository = infrastructure_container.math_expression_context_repository()\n",
    "math_article_chunk_repository = infrastructure_container.math_article_chunk_repository()\n",
    "math_expression_index_repository = infrastructure_container.math_expression_index_repository()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb39d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:43:40,565 - INFO - googleapiclient.discovery_cache - __init__.py:49 - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from math_rag.core.models import MathArticle\n",
    "\n",
    "\n",
    "google_drive_repository = infrastructure_container.google_drive_repository()\n",
    "math_article_parser_service = infrastructure_container.math_article_parser_service()\n",
    "\n",
    "file_id = google_drive_repository.get_file_id(\n",
    "    Path('ml/lectures/L07-LogisticRegression2/2024_08_10_2174b40686820b4cb591g.tex')\n",
    ")\n",
    "\n",
    "if not file_id:\n",
    "    raise ValueError()\n",
    "\n",
    "file_content = google_drive_repository.get_file_by_id(file_id)\n",
    "\n",
    "math_article = MathArticle(\n",
    "    math_expression_dataset_id=None,\n",
    "    math_expression_index_id=None,\n",
    "    name='article',\n",
    "    bytes=file_content.getvalue(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081db754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('91ad04fa-05f3-4250-8041-e04efb492515')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import UUID\n",
    "\n",
    "from math_rag.application.utils import InputCreatorUtil\n",
    "from math_rag.core.models import MathExpressionIndex\n",
    "from math_rag.infrastructure.utils import (\n",
    "    TemplateChunkerUtil,\n",
    "    TemplateContextChunkerUtil,\n",
    "    TemplateFormatterUtil,\n",
    "    TemplateIndexFinderUtil,\n",
    ")\n",
    "\n",
    "\n",
    "index = MathExpressionIndex()\n",
    "index.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81928c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = MathExpressionIndex(id=UUID('d7672957-dc0d-4f9d-8fbb-da2d91cb0dc2'))\n",
    "# index.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99cb323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 16:25:55,219 - WARNING - neo4j.notifications - result.py:337 - Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (a) { ... }} {position: line: 3, column: 13, offset: 35} for query: '\\n            MATCH (a)\\n            CALL { WITH a DETACH DELETE a }\\n            IN TRANSACTIONS OF 5000 rows\\n        '\n",
      "2025-07-13 16:25:55,224 - WARNING - neo4j.notifications - result.py:337 - Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (a) { ... }} {position: line: 3, column: 13, offset: 35} for query: '\\n            MATCH (a)\\n            CALL { WITH a DETACH DELETE a }\\n            IN TRANSACTIONS OF 5000 rows\\n        '\n"
     ]
    }
   ],
   "source": [
    "index_id_to_remove = index.id\n",
    "common_filter = dict(math_expression_index_id=index_id_to_remove)\n",
    "\n",
    "await math_expression_index_repository.delete_one(filter=dict(id=index_id_to_remove))\n",
    "\n",
    "await math_expression_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_context_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_description_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_description_opt_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_group_repository.delete_many(filter=common_filter.copy())\n",
    "await math_article_chunk_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_relationship_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_relationship_description_repository.delete_many(filter=common_filter.copy())\n",
    "await math_expression_description_opt_embedding_repository.clear()\n",
    "await math_expression_graph_repository.clear()\n",
    "await math_expression_group_graph_repository.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0146e",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816adeb6",
   "metadata": {},
   "source": [
    "### 1. MathExpression, requires: MathArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aad9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathExpression\n",
    "\n",
    "\n",
    "math_nodes, _, template = math_article_parser_service.parse_for_index(math_article)\n",
    "math_nodes.sort(key=lambda x: x.position)\n",
    "\n",
    "katexes = [math_node.latex.strip('$') for math_node in math_nodes]\n",
    "valid_katexes = await katex_corrector_service.correct(katexes, max_num_retries=3)\n",
    "math_expressions = [\n",
    "    MathExpression(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_dataset_id=None,\n",
    "        math_expression_group_id=None,\n",
    "        math_expression_index_id=index.id,\n",
    "        latex=node.latex,\n",
    "        katex=katex.strip(),\n",
    "        index=i,\n",
    "        position=node.position,\n",
    "        is_inline=node.is_inline,\n",
    "    )\n",
    "    for i, (node, katex) in enumerate(zip(math_nodes, valid_katexes))\n",
    "]\n",
    "await math_expression_repository.insert_many(math_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff89ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jan Šnajder, lectures, v2.0\n",
      "\n",
      "Last time we introduced the logistic regression algorithm. We defined the model and derived the cross-entropy error function as the negative probability of the labels in the training set. We established that minimizing that error had no solution in closed form, so we turned to iterative procedures. We have considered the simplest such procedure, the gradient descent algorithm, and we applied it to logistic regression, in standard (batch) and stochastic variant. In the end, we talked about regularization, specifically [math_placeholder | 0] regularization, which we incorporated quite straightforwardly into the optimization process.\n",
      "\n",
      "Today we'll talk a bit more about logistic regression. First, we'll consider some more efficient (read: faster) alternatives to gradient descent. Second, we'll consider the extension of binary logistic regression to multiclass logistic regression. Third, we'll look at all the models discussed thus far and see what they have in common and how they can be generalized. Finally, we'll talk about adaptive basis functions, which is a way to learn the feature mapping function directly from data, instead of defining it manually.\n",
      "\n",
      "Unlike previous lectures, this one won't go into details, because we do not have the time for this. My goal is to give you enough information to know where to look for more, should you feel motivated to do so or if you need it\n",
      "\n",
      "\n",
      "Last time we established that the gradient descent needs to be coupled with line search, otherwise there is no guarantee for global convergence. This means that, depending on where the descent initially starts, it can happen that the gradient descent does not converge but diverge (effectively it starts to ascend instead of descend). Line search prevents this from happening. However, line search can result in a zig-zag descent. Let's recall the figure we discussed last time:\n",
      "\n",
      "\n",
      "[image_placeholder | 0]\n",
      "\n",
      "\n",
      "The blue trajectory corresponds to the best scenario, and the red to the worst-case scenario for a gradient descent with line search. Which one will play out depends on the choice of the starting point of the search. As we can see, it can happen that the descent zig-zags quite a lot, which means that the optimization will consume a lot of iterations. Obviously, the problem arises because the descent direction is not as good as it could be. Imagine descending into a pit from the starting point for the red trajectory. It is hard to imagine that you will descend\n",
      "as pointed to by the line. It is more likely that the gravity would pull you and your descend direction would be steeper (i.e., at a smaller angle to the blue line). This reasoning of ours is based on the curvature of the isocontours (that is, the curvature of the surface down which we descend). In other words, the curvature of the surface gives us, in addition to the gradient, additional information about where the minimum is likely to be located (at least when it comes to convex functions).\n",
      "\n",
      "The above observations apply to the standard (batch) gradient. In stochastic gradient descent we typically do not use line search. But there the descent will zig-zag quite a lot anyway, since each step is taken based on the gradient calculated for a single example. In the following, we focus on non-stochastic, i.e., batch gradient descent.\n",
      "\n",
      "Based on the above consideration, we can conclude that the batch gradient descent could be improved if we take into account not only the slope (gradient) but also the curvature (the change in gradient, i.e., the second derivative) of the error function. Such optimization methods are referred to as second-order optimization, as opposed to first-order optimization methods, such as gradient descent. The basic second-order optimization method is the Newton's method.\n",
      "\n",
      "\n",
      "Consider minimization of function [math_placeholder | 1]. We know that the parameter update in gradient descent is as follows:\n",
      "\n",
      "[math_placeholder | 2]\n",
      "\n",
      "If we introduce an index for the iterations, then we can write this as an equation:\n",
      "\n",
      "[math_placeholder | 3]\n",
      "\n",
      "The idea with Newton's method is to take the point [math_placeholder | 4] (the current minimum) and compute at it the quadratic approximation of the function [math_placeholder | 5], and then move to the minimizer of this quadratic approximation (which is known analytically). If [math_placeholder | 6] is a function of one variable, this would look like this:\n",
      "\n",
      "\n",
      "[image_placeholder | 1]\n",
      "\n",
      "\n",
      "The black curve is the function [math_placeholder | 7] that we minimize. We start from point [math_placeholder | 8]. At this point we do a quadratic approximation of the function [math_placeholder | 9], thus obtaining a parabola that is tangential to the function [math_placeholder | 10] at the point [math_placeholder | 11]. The search then moves to a point that minimizes the quadratic approximation of [math_placeholder | 12]. In the picture it is the point [math_placeholder | 13]. At that point we again do a quadratic approximation of the function [math_placeholder | 14] and move to the point that minimizes that approximation. The procedure is repeated until the update is small enough.\n",
      "\n",
      "The methods works just the same for functions of several variables, i.e., in a multidimensional parameter space. In a two-dimensional space, it looks like this:\n",
      "\n",
      "\n",
      "[image_placeholder | 2]\n",
      "\n",
      "\n",
      "So, the idea is to take the step exactly so that we land in the minimum of the quadratic approximation. This will work well if [math_placeholder | 15] is a convex function, which happens to be the case with logistic regression.\n",
      "\n",
      "Let us now deal with the technical details. We need to find a quadratic approximation of a function at some point. How can we do this? Recall, from calculus, that every differentiable function [math_placeholder | 16] can be expressed at some given point [math_placeholder | 17] in the form of a power series. More precisely, every differentiable function can be expanded into a Taylor series about the point [math_placeholder | 18], as follows:\n",
      "\n",
      "[math_placeholder | 19]\n",
      "\n",
      "As we only need a quadratic approximation, we will take only the first three terms of the Taylor series. This then is the second-order Taylor expansion:\n",
      "\n",
      "[math_placeholder | 20]\n",
      "\n",
      "Note that this now is an approximation; the equality between [math_placeholder | 21] on the left-hand side and the series on the right-hand side is valid only when the series is infinite.\n",
      "\n",
      "Our error function, [math_placeholder | 22], is a function of multiple variables (i.e., vector w). For a multivariate function [math_placeholder | 23], the quadratic expansion about the point [math_placeholder | 24] is:\n",
      "\n",
      "[math_placeholder | 25]\n",
      "\n",
      "where [math_placeholder | 26] is the Hessian matrix (hrv. Hesseova matrica) of function [math_placeholder | 27] at point [math_placeholder | 28]. The Hessian matrix is a square [math_placeholder | 29] matrix of second-order partial derivatives of function [math_placeholder | 30], which is a function that maps [math_placeholder | 31]-dimensional vectors into scalars. The Hessian matrix is defined as follows:\n",
      "\n",
      "[math_placeholder | 32]\n",
      "\n",
      "that is, an element of the Hessian matrix is defined as:\n",
      "\n",
      "[math_placeholder | 33]\n",
      "\n",
      "Note that the Hessian matrix is a symmetric matrix, as the order of partial differentiation does not affect the result (commutativity).\n",
      "\n",
      "Using the expansion of the quadratic function [math_placeholder | 34], one can now derive its minimum, which us exactly where we want to step to when descending (we omit the derivation). The parameter update is then:\n",
      "\n",
      "[math_placeholder | 35]\n",
      "\n",
      "If the Hessian matrix is computed exactly (i.e., it is not an approximation), then we can set [math_placeholder | 36], because then the step will land exactly in the minimum of the quadratic approximation.\n",
      "\n",
      "We see that the Newton's optimization method requires us to compute the inverse of the Hessian matrix. In general, the Hessian matrix [math_placeholder | 37] is positive semi-definite ( [math_placeholder | 38] for every non-zero vector [math_placeholder | 39] ) if and only if [math_placeholder | 40] is a convex function. However, for its inverse to exist, the matrix [math_placeholder | 41] has to be positive definite ( [math_placeholder | 42] for each non-zero vector [math_placeholder | 43] ). If [math_placeholder | 44] is positive semi-definite, but not positive definite, then [math_placeholder | 45] has no inverse and we cannot apply the Newton's method.\n",
      "\n",
      "\n",
      "Let us now apply Newton's procedure to logistic regression. The weights update rule is as follows:\n",
      "\n",
      "[math_placeholder | 46]\n",
      "\n",
      "One can easily derive that the Hessian matrix for the cross-entropy error equals:\n",
      "\n",
      "[math_placeholder | 47]\n",
      "\n",
      "where [math_placeholder | 48], i.e., a diagonal matrix that on its diagonal has the firstorder derivatives of the logistic output [math_placeholder | 49] for each of the [math_placeholder | 50] examples from the training set. Just in case, let's check the compatibility of the matrices we multiply here: [math_placeholder | 51] [math_placeholder | 52]. Everything is fine: the dimension of the matrix [math_placeholder | 53] is [math_placeholder | 54], where [math_placeholder | 55] is the number of features in the feature space (i.e., after mapping).\n",
      "\n",
      "Let's go back quickly to the problem of computing the inverse of the Hessian matrix [math_placeholder | 56]. As we already said, [math_placeholder | 57] is positive semi-definite if and only if the function is convex. The crossentropy error is convex, so the [math_placeholder | 58] matrix for logistic regression will be positive semi-definite. But this alone does not guarantee it is invertible. However, one can show that the Hessian matrix [math_placeholder | 59] of cross-entropy, which can be decomposed as [math_placeholder | 60], must be positive definite. In other words, the Hessian matrix of the cross entropy error always has an inverse, so we can always use the Newton's method to optimize the parameters of logistic regression. However, due to multicollinearity, [math_placeholder | 61] may be ill-conditioned and in this case the solution will be unstable, which means we have to either select a linearly independent subset of features or apply regularization (we'll see how soon).\n",
      "\n",
      "If we were now to plug in [math_placeholder | 62] for [math_placeholder | 63] in the weights update rule above, and rearrange the expression a bit, we would obtain the iteratively reweighted least squares (IRLS) algorithm (hrv. algoritam najmanjih kvadrata s iterativnim ažuriranjem težina). We won't go into the details; it suffices for you to know that this algorithm is used for faster optimization for logistic regression, and that it is in fact an application of the Newton's method, which is a second-order optimization procedure.\n",
      "\n",
      "\n",
      "The problem with Newton's method (and thus with IRLS) is that the computation of the Hessian (and especially its inverse) can be expensive (especially if [math_placeholder | 64], the number of dimensions of the feature space, is large). Note that one has to compute the Hessian matrix and its inverse in every step of the optimization procedure. It might well be the case that doing gradient descent is less taxing computationally, even if the descent zig-zags a lot!\n",
      "\n",
      "The alternative is that, instead of computing the exact Hessian matrix, we compute its approximation. This is what the so-called quasi-Newton methods do. These procedures using the gradient vectors (from the current and previous step) to approximate the Hessian matrix (or its inverse) in each step of the descent. The most common such method is the BFSG algorithm. Again, we won't go into details, we just want you to be aware that the method exists.\n",
      "\n",
      "Another problem is that, even if we approximate the Hessian matrix, storing it in memory can be problematic because its dimensions are [math_placeholder | 65]. In that case, the matrix can be \"compressed\" using a low-rank approximation method. The algorithm that works that way is the limited BFSG (L-BFSG) (hrv. ograničen BFSG). Again, it suffices for you to know that this algorithm exists.\n",
      "\n",
      "\n",
      "We already know about the benefits of regularization, and last time we talked about the additional benefit of regularization in logistic regression (when optimizing the parameters for linearly separable problems, we have [math_placeholder | 66] and logistic regression overfits easily). Extending the Newton's method to regularized cross-entropy error is simple. Let's look at [math_placeholder | 67] regulation. We already considered adding the [math_placeholder | 68] regularization term to the gradient:\n",
      "\n",
      "[math_placeholder | 69]\n",
      "\n",
      "We have the same for the Hessian matrix:\n",
      "\n",
      "[math_placeholder | 70]\n",
      "\n",
      "where [math_placeholder | 71] is a diagonal matrix with a regularization factor [math_placeholder | 72] on the diagonal (except for the upper-left element, because the weight [math_placeholder | 73] is not regularized).\n",
      "\n",
      "We see, thus, that [math_placeholder | 74] regularization is easily incorporated into the gradient descent and in the Newton's method, and it was just as easily incorporated into the least squares optimization method for the linear regression. All in all, [math_placeholder | 75] regularization is a tame beast.\n",
      "\n",
      "What about [math_placeholder | 76] regularization? So far, we haven't dealt with it at all, and we won't deal with it now, either. Let it be said that things get complicated here, which is expected because [math_placeholder | 77] norm is not differentiable, so we cannot calculate the gradient. We instead compute the subgradient (engl. podgradijent). We then typically use the coordinate descent (engl. koordinatni spust), where we optimize by each variable in term (dimension after dimension), or we use proximal or projection optimization methods. For now, you only need to know that optimization for [math_placeholder | 78] regularized logistic regression is possible, that there are algorithms for it, and that they are implemented in standard tools.\n",
      "\n",
      "\n",
      "Last time we talked about binary logistic regression: we classified into classes [math_placeholder | 79] and [math_placeholder | 80]. To get the probabilities, we used the sigmoid for the activation function:\n",
      "\n",
      "[math_placeholder | 81]\n",
      "\n",
      "But what if we have more than two classes, i.e. [math_placeholder | 82] ? We could apply the OVO or OVR decomposition scheme, but the problem is that the probabilities for the individual classes would not add up to 1 . Moreover, in statistical sense the estimates for the parameters for the individual models are less reliable than estimates for a model that considers all classes at once. Instead, it's better to use the multinomial logistic regression (MNR, occasionally MLR), also referred to as maximum entropy classifier (hrv. klasifikator maksimalne entropije).\n",
      "\n",
      "\n",
      "The idea is actually very simple: use a separate weight vector [math_placeholder | 83] for each of the [math_placeholder | 84] classes, but then pass the scalar product [math_placeholder | 85] through an appropriate activation function to be make sure that the probabilities of all classes add up to 1. A function that does exactly this is the softmax\n",
      "\n",
      "function. For some input example [math_placeholder | 86], the softmax function takes the values [math_placeholder | 87] for each of the [math_placeholder | 88] classes, i.e., a [math_placeholder | 89]-dimensional vector, and maps them to a [math_placeholder | 90]-dimensional vector whose components sum to 1 . Formally, softmax : [math_placeholder | 91], where [math_placeholder | 92] is the component of the output vector equal to:\n",
      "\n",
      "[math_placeholder | 93]\n",
      "\n",
      "The softmax function accomplishes two things: it normalizes all values to a total of 1 , but it also amplifies the larger values and attenuates the smaller values. The function is called softmax because it corresponds a \"soft\" variant of the max function (in the sense that, unlike the max function, it is continuous and differentiable). Let's look at an example.\n",
      "\n",
      "[math_placeholder | 94] EXAMPLE\n",
      "\n",
      "\n",
      "[image_placeholder | 3]\n",
      "\n",
      "\n",
      "[math_placeholder | 95]\n",
      "\n",
      "\n",
      "[image_placeholder | 4]\n",
      "\n",
      "\n",
      "[math_placeholder | 96]\n",
      "\n",
      "We will define the model [math_placeholder | 97] of multinomial logistic regression as a set of models [math_placeholder | 98], where each model [math_placeholder | 99] is responsible for class [math_placeholder | 100] out of [math_placeholder | 101] classes. We define each model [math_placeholder | 102] so that it outputs the probability of example [math_placeholder | 103] belonging to class [math_placeholder | 104], using the softmax function:\n",
      "\n",
      "[math_placeholder | 105]\n",
      "\n",
      "where [math_placeholder | 106] is a matrix [math_placeholder | 107] weight vectors [math_placeholder | 108]. Note that, by virtue of the softmax function, the model [math_placeholder | 109] for class [math_placeholder | 110] takes into account the outputs of the other [math_placeholder | 111] models for the remaining classes.\n",
      "\n",
      "This defines the model. Let us now derive the error function.\n",
      "\n",
      "\n",
      "In binary logistic regression, we defined derived the error function staring from the negative logarithm of the probability of the labels. The labels were binary, [math_placeholder | 112], that is, they were Bernoulli variables. Now, since the output of multiclass regression can take more than two [math_placeholder | 113] values, then we move to categorical variable, which is also also called multinomial, or, perhaps better, the multinoulli variable. We represent such a variable as a vector of indicator (binary) variables:\n",
      "\n",
      "[math_placeholder | 114]\n",
      "\n",
      "where [math_placeholder | 115] if the outcome of the variable is [math_placeholder | 116], otherwise [math_placeholder | 117]. For example, [math_placeholder | 118] indicates that the multinomial variable has taken the third state out of four possible states. [math_placeholder | 119] is valid (outcomes are mutually exclusive and complete). Let's denote the probability [math_placeholder | 120] as [math_placeholder | 121].\n",
      "\n",
      "We will now define the distribution of this variable. Recall, the distribution of the Bernoulli variable, which has only two values, [math_placeholder | 122] and [math_placeholder | 123], is defined via the [math_placeholder | 124] parameter as follows:\n",
      "\n",
      "[math_placeholder | 125]\n",
      "\n",
      "We can generalize this to [math_placeholder | 126] values as follows. First, we need [math_placeholder | 127] parameters, so we will define a parameter vector:\n",
      "\n",
      "[math_placeholder | 128]\n",
      "\n",
      "where parameters [math_placeholder | 129] satisfy [math_placeholder | 130] and [math_placeholder | 131], as they represent probabilities.\n",
      "\n",
      "Now, by analogy with the Bernoulli distribution, the distribution of a categorical variable can be defined as:\n",
      "\n",
      "[math_placeholder | 132]\n",
      "\n",
      "Note: as with binary logistic regression, the probability that example [math_placeholder | 133] belongs to class [math_placeholder | 134] is exactly what we are given by the model:\n",
      "\n",
      "[math_placeholder | 135]\n",
      "\n",
      "Now we can finally write the logarithm of the probability of the labels from [math_placeholder | 136] as:\n",
      "\n",
      "[math_placeholder | 137]\n",
      "\n",
      "The error function we wish to minimize is the negative logarithm of the probability of the labels:\n",
      "\n",
      "[math_placeholder | 138]\n",
      "\n",
      "We see that we arrived at the generalization of the cross-entropy error to [math_placeholder | 139] classes. Also, from this we can read off the loss function as:\n",
      "\n",
      "[math_placeholder | 140]\n",
      "\n",
      "The logic is the same as with binary logistic regression: if the label [math_placeholder | 141] of some example [math_placeholder | 142] for class [math_placeholder | 143] is equal to 1 , then we want the model prediction (softmax output) to be a high probability close to 1 , because then [math_placeholder | 144] and the loss will be zero. Otherwise, if the model for the example whose label is 1 gives a value close to 0 , then the logarithm will be a large negative number, its negation will be a large positive number, and consequently the loss will be large.\n",
      "\n",
      "\n",
      "As with binary logistic regression, we cannot minimize [math_placeholder | 145] in closed form, so we need to rely on iterative optimization. For gradient descent, one can show (although it is a bit clumsy) that the gradient of the error function is equal to:\n",
      "\n",
      "[math_placeholder | 146]\n",
      "\n",
      "This is the gradient of the weights specifically for class [math_placeholder | 147]. The idea is that we can update the weights for each class separately. From this we can directly derive the stochastic gradient descent (we update the weights for each example, for each class). We can also derive the standard (batch) gradient descent, where we accumulate updates for all examples, for each class separately. We can also derive the Newton method (the Hessian matrix), but we will skip that.\n",
      "\n",
      "\n",
      "You may have noticed that the gradient of the loss we derived for multinomial logistic regression is actually the same as that for binary logistic regression: \"model's error times the example vector\". When we use this for gradient descent, we update the weights as follows:\n",
      "\n",
      "[math_placeholder | 148]\n",
      "\n",
      "We have already (in the context of the perceptron, if you remember) said this rule is called the Widrow-Hoff rule, while the alternative name is least-mean-squares (LMS) algorithm (not to be confused with least squares, although there is obviously a connection).\n",
      "\n",
      "The LMS algorithm, that is, this kind of learning where we use stochastic gradient descent to minimize the error by updating the model's weights in the way defined above, allows for online learning. We mentioned online learning the last time. Recall: online learning is a type of learning where not all learning examples need to be available up front, rather they can become available one after the other, and the model weights will be updated as new examples arrive.\n",
      "\n",
      "Let's go back a few weeks, to linear regression. When we introduced linear regression, we were actually talking only about batch optimization, which is achieved by calculating the pseudoinverse of the design matrix. However, already then we could have resorted to a stochastic (i.e., online) weight update. Namely, instead of searching analytically for the minimum of the error function, we can calculate the gradient of the error function, and then apply the gradient descent. (We didn't do that at that at that time, probably because we were blinded by the sheer elegance of the closed-form solution). Well, let's do it now. The quadratic error function of linear regression is:\n",
      "\n",
      "[math_placeholder | 149]\n",
      "\n",
      "The gradient (for one example [math_placeholder | 150] ) is:\n",
      "\n",
      "[math_placeholder | 151]\n",
      "\n",
      "So, the rule for updating the weights is:\n",
      "\n",
      "[math_placeholder | 152]\n",
      "\n",
      "which, again, is the LMS rule! So, there seems to be some underlying principle at play here all three regression algorithms use the same rule (LMS) for online learning (i.e., for stochastic gradient descent). How can that be? Well, this is because all these models - linear regression, logistic regression, and multinomial regression - belong to the generalized linear models family.\n",
      "\n",
      "Now it's time to round out the story and give a unified perspective on the three algorithms.\n",
      "\n",
      "\n",
      "For starters, let's recall from last lecture that generalized linear models are models that wrap the scalar product of the weight vector and example vector into an activation function [math_placeholder | 153]. So:\n",
      "\n",
      "[math_placeholder | 154]\n",
      "\n",
      "Let's look at three generalized linear models we've considered so far. For each of them, let's consider four points: (1) how the model is defined, (2) what is the probability distribution to which their output corresponds, (3) how is the loss is defined, and (4) what is the gradient of the loss, which we need for gradient descent.\n",
      "\n",
      "First, let's look at the linear regression algorithm:\n",
      "\n",
      "[math_placeholder | 155]\n",
      "\n",
      "For batch learning we use the least squares method (the pseudoinverse), and for online learning we use the LMS rule.\n",
      "\n",
      "Let's look at the logistic regression algorithm:\n",
      "\n",
      "[math_placeholder | 156]\n",
      "\n",
      "For batch learning we use gradient descent, the Newton's (IRLS) or the quasi-Newton method (BFSG, L-BFSG). We use the LMS rule for online learning.\n",
      "\n",
      "Finally, let's look at the multinomial logistic regression:\n",
      "\n",
      "[math_placeholder | 157]\n",
      "\n",
      "We use the same (mutatis mutandis) optimization procedures for model learning as for logistic regression.\n",
      "\n",
      "Notice the commonalities. For all three algorithms, we derived the loss function from the negative logarithm of the probability of labels of the examples from the dataset. We did this by using the normal, Bernoulli, and multinoulli distribution distribution for linear, binary logistic, and multinoulli logistic regression, respectively. Furthermore, for all three algorithms we derived an identical rule (the LMS) for online weights update.\n",
      "\n",
      "The question is: how come we always get the same weights update rule? Also, what is the relationship between the logistic function and the Bernoulli variable, and between the softmax function and the multinoulli distribution? There seems to be a connection, because in both cases we obtained the cross-entropy error. The answer lies in the properties of the distributions we used to model the labels [math_placeholder | 158].\n",
      "\n",
      "\n",
      "The distributions we have encountered so far (Gaussian, Bernoulli, multinoulli), but also some others that are often used in machine learning (binomial, multinomial, Student's t-distribution, uniform, beta distribution, gamma distribution, Dirichlet's) belong to the so-called exponential family (hrv. eksponencijalna familija). What is an exponential family? The exponential family is a broad group of distributions that can be written in the following form:\n",
      "\n",
      "[math_placeholder | 159]\n",
      "\n",
      "The exponential family distributions have many properties that are important for machine learning, but mostly for probabilistic approaches to machine learning, so we won't go into further into that here.\n",
      "\n",
      "What is of interesting to us here is that it is that the exponential family is crucial for for generalized linear models. Specifically, for the distributions belonging to the exponential family (including Gaussian, Bernoulli, and multinoulli) there is a relationship between the distribution and its (possibly nonlinear) activation function [math_placeholder | 160]. This function is in this context is referred to as the mean function (hrv. funkcija sredine), because it defines the [math_placeholder | 161] parameter of a distribution, i.e., the distribution's mean. Thus, the activation function [math_placeholder | 162] therefore [math_placeholder | 163] as a function of [math_placeholder | 164]. You probably already guessed that for the Gaussian distribution the activation function is an identity function, since [math_placeholder | 165]. For the Bernoulli's distribution it is the logistic function, and for the multinoulli distribution it is the softmax function.\n",
      "\n",
      "After this inspiring topic, let's take a look at another no less inspiring thing ...\n",
      "\n",
      "\n",
      "In generalized linear models (for both regression and classification) we had the option to map examples to the feature space using a feature mapping function:\n",
      "\n",
      "[math_placeholder | 166]\n",
      "\n",
      "where [math_placeholder | 167] is a set of [math_placeholder | 168] basis functions (nonlinear functions of the input variables): [math_placeholder | 169]. For example, polynomial mapping for [math_placeholder | 170] and [math_placeholder | 171] :\n",
      "\n",
      "[math_placeholder | 172]\n",
      "\n",
      "We then easily incorporated such a mapping into any generalized linear model:\n",
      "\n",
      "[math_placeholder | 173]\n",
      "\n",
      "where [math_placeholder | 174] is a chosen activation function (i.e., the mean function, to make use of the term we just introduced).\n",
      "\n",
      "Although we haven't tried it, the basis functions [math_placeholder | 175] need not necessarily be potencies or factors of the input features, but rather these can really be any functions. One interesting possibility is to use functions that measure the similarity of an example with some prototype\n",
      "examples in the input space. This then is called a kernel machine, and we'll talk about that in two weeks.\n",
      "\n",
      "At any rate, the limiting factor is that these are fixed basis functions: their number and shape is predetermined. This is a problem because in most cases we do not know in advance which basis functions are good for our problem. In other words, we generally don't know which feature mapping will make our problem linearly separable in feature space.\n",
      "\n",
      "We can solve this problem by letting the basis functions adapt to our data (examples from the training set). Here we have two options: the first, used by the aforementioned kernel machines, is to select some examples from the training set as prototypes, and then make the basis functions measure the similarity between an input example and these prototypes. This adjusts the total number of basis functions depending on the data. Another possibility is to use a fixed number of basis functions, but let each of them adapt to the data. Let's look into that in a bit more detail.\n",
      "\n",
      "The idea of adaptive basis functions is to define them up to some parameters, which we can then adjust to the data. That is, we will define parameterized basis functions. Does that sound familiar? Of course. It is exactly the same as training machine learning models: we define a function up to some parameters, and the parameters are determined by optimizing the empirical error on the training set. But let's look first at how one could go about parameterizing the basis functions. One possibility is to say that each basis function is a small generalized linear model on its own! So, we will have a generalized linear model and within it we'll have generalized linear models as its basis functions:\n",
      "\n",
      "[math_placeholder | 176]\n",
      "\n",
      "Note that each basis function in our model should have its own weight vector, so weights [math_placeholder | 177] in the inner sum have two indices: [math_placeholder | 178] is the basis function index, while [math_placeholder | 179] is the index of the weight in the weight vector of basis function [math_placeholder | 180]. With superscripts [math_placeholder | 181] and [math_placeholder | 182] we indicated which weights are used first in the calculation of model prediction, and which are used second. The expression on the right is just a matrix notation of the model, where we managed to get rid of the sums and we combined the weights into a weight vector and weight matrix. Take some time to convince yourself that this notation is the same as the one with sums.\n",
      "\n",
      "Now that we have metabolized this, it's time for a big revelation. What kind of model is this actually? For each basis function there are weights from the matrix [math_placeholder | 183], which we multiply with all the input features and add them up. We then multiply these values again by weights [math_placeholder | 184] and add them up. We've built something most of you already know: a neural network!\n",
      "\n",
      "\n",
      "[image_placeholder | 5]\n",
      "\n",
      "\n",
      "This network of ours is two-layered, however nothing prevents us from going deeper: we can make the basis functions be combinations of other basis functions, and those again be combinations of yet another basis functions, etc.\n",
      "\n",
      "Obviously, neural networks are a more complex model than generalized linear models. This, of course, comes at a price: a more complex optimization procedure (due to non-convexity of\n",
      "the error function, since the loss in the output layer - which can still be the quadratic loss or cross entropy loss - now has a very complex dependence on the weights of the previous layers) and a greater possibility of model overfitting. Of course, various solutions have been proposed to tackle these issues, in particular within the now popular deep learning paradigm. We won't go any further, however, as this topic is receiving a rather comprehensive treatment in other courses.\n",
      "\n",
      "What matters here is to be aware of the connection: a neural network is an extension of a generalized linear model in which the basis functions are adaptive, i.e., the feature mapping function is also learned from the data.\n",
      "\n",
      "\n",
      "\n",
      "  Newton's method is a second-order optimization method that converges faster than the gradient descent, and is based on the calculation of the Hessian matrix. The logistic regression variant is called IRLS\n",
      "  The calculation of the Hessian matrix is expensive to compute in both time and space, so we may resort to a quasi-Newton method, such as L-BSFG\n",
      "  Multinomial logistic regression is a generalization of logistic regression to more than two classes, with softmax function as the activation function\n",
      "  Common to generalized linear models is that their outputs are variables from the exponential family distributions\n",
      "  Instead of using fixed basis function, we can use parameterized adaptive basis functions, which brings us to neural networks\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd377fa0",
   "metadata": {},
   "source": [
    "### 2. MathExpressionContext, requires MathExpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6a6eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "above consideration, we can conclude that the batch gradient descent could be improved if we take into account not only the slope (gradient) but also the curvature (the change in gradient, i.e., the second derivative) of the error function. Such optimization methods are referred to as second-order optimization, as opposed to first-order optimization methods, such as gradient descent. The basic second-order optimization method is the Newton's method.\n",
      "\n",
      "\n",
      "Consider minimization of function [math_placeholder | 1]. We know that the parameter update in gradient descent is as follows:\n",
      "\n",
      "[math_placeholder | 2]\n",
      "\n",
      "If we introduce an index for the iterations, then we can write this as an equation:\n",
      "\n",
      "[math_placeholder | 3]\n",
      "\n",
      "The idea with Newton's method is to take the point [math_placeholder | 4] (the current minimum) and compute at it the quadratic approximation of the function [math_placeholder | 5], and then move to the minimizer of this quadratic approximation (which is known analytically). If [math_placeholder | 6]\n"
     ]
    }
   ],
   "source": [
    "context_templates = TemplateContextChunkerUtil.chunk(template, max_context_size=1000)\n",
    "assert len(context_templates) == len(math_expressions)\n",
    "\n",
    "print(context_templates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e5d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathExpressionContext\n",
    "\n",
    "\n",
    "index_to_katex = {\n",
    "    math_expression.index: math_expression.katex for math_expression in math_expressions\n",
    "}\n",
    "math_expression_contexts: list[MathExpressionContext] = []\n",
    "\n",
    "for math_expression, context_template in zip(math_expressions, context_templates):\n",
    "    formatted_context, _ = TemplateFormatterUtil.format(\n",
    "        context_template, index_to_katex, omit_wrapper=False\n",
    "    )\n",
    "    math_expression_context = MathExpressionContext(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_id=math_expression.id,\n",
    "        math_expression_index_id=index.id,\n",
    "        text=formatted_context,\n",
    "    )\n",
    "    math_expression_contexts.append(math_expression_context)\n",
    "\n",
    "await math_expression_context_repository.insert_many(math_expression_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315a5bb",
   "metadata": {},
   "source": [
    "### 3. MathExpressionDescription, requires MathExpression, MathExpressionContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c764cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionDescriptionWriter as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionDescription\n",
    "from math_rag.infrastructure.constants.services import MATH_TEMPLATE\n",
    "\n",
    "\n",
    "inputs: list[AssistantInput] = []\n",
    "input_id_to_math_expression_id: dict[UUID, UUID] = {}\n",
    "\n",
    "for math_expression, math_expression_context in zip(math_expressions, math_expression_contexts):\n",
    "    input = AssistantInput(\n",
    "        katex=MATH_TEMPLATE.format(katex=math_expression.katex, index=math_expression.index),\n",
    "        context=math_expression_context.text,\n",
    "    )\n",
    "    inputs.append(input)\n",
    "    input_id_to_math_expression_id[input.id] = math_expression.id\n",
    "\n",
    "outputs = await math_expression_description_writer_assistant.concurrent_assist(inputs)\n",
    "math_expression_descriptions = [\n",
    "    MathExpressionDescription(\n",
    "        math_expression_index_id=index.id,\n",
    "        math_expression_id=input_id_to_math_expression_id[output.input_id],\n",
    "        text=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]\n",
    "await math_expression_description_repository.insert_many(math_expression_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605301ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca3e0217-7435-428f-b730-548872b6fcb9\n",
      "the point about which the Taylor series expansion of a differentiable function is performed\n",
      "-----\n",
      "\n",
      "1cca5ec7-5406-480f-b0c8-8d20d5601030\n",
      "the value of a general differentiable function evaluated at the variable x\n",
      "-----\n",
      "\n",
      "4fa1f4b5-66c0-4c72-a0f9-46f876035c4f\n",
      "The number of examples in the training set.\n",
      "-----\n",
      "\n",
      "ae67cc28-59b1-4ed6-a289-85933dbf9137\n",
      "the black curve representing the function of a single variable that is being minimized in the context of Newton's method\n",
      "-----\n",
      "\n",
      "40de932a-3537-479b-a354-9321d97b13e6\n",
      "The error function that depends on the vector w and is conditioned on the dataset D.\n",
      "-----\n",
      "\n",
      "082446e9-48b4-4cb1-bf39-2638a9aa62b1\n",
      "The number of dimensions of the feature space.\n",
      "-----\n",
      "\n",
      "77abde18-8d84-4cb8-bbf0-c67858d23cd4\n",
      "the initial point from which the minimization process starts in the context of minimizing a function of one variable using Newton's method\n",
      "-----\n",
      "\n",
      "f6a6a73b-f275-43ad-8a0d-5956cca28a04\n",
      "the point at which the quadratic approximation of the function is constructed and to which the parabola is tangential in the minimization process\n",
      "-----\n",
      "\n",
      "bf9900fe-ea6c-4ce1-9fbd-e34c754376ca\n",
      "the value of the parameter vector at iteration t in an iterative optimization process\n",
      "-----\n",
      "\n",
      "9c9821cd-6361-4e03-83c2-6acf6291d2b9\n",
      "A function of a vector variable, representing a real-valued function that takes a vector as input.\n",
      "-----\n",
      "\n",
      "68e6418c-1209-4b4d-bd30-14dc913dc533\n",
      "the Hessian matrix referenced in the context of Newton's optimization method, which is used to determine the curvature of the function being minimized\n",
      "-----\n",
      "\n",
      "4bdf55a1-065f-463c-8b44-77db592f1888\n",
      "the point at which the function is expanded in its Taylor series representation\n",
      "-----\n",
      "\n",
      "fee0f857-90da-428f-987d-811d3ad1f34e\n",
      "This is a function that takes as input an n-dimensional real vector and produces a real-valued scalar output.\n",
      "-----\n",
      "\n",
      "36b9967c-d97b-40f6-a890-edfa0b7f9a89\n",
      "the dimension of the output vector produced by the softmax function, which matches the number of classes\n",
      "-----\n",
      "\n",
      "dd828d70-4bcc-4ada-a766-35e0a49b2339\n",
      "the symbol representing a non-zero vector used in the context of testing the positive semi-definiteness of the Hessian matrix\n",
      "-----\n",
      "\n",
      "64559b13-b854-42ef-90f0-69ca004c25c8\n",
      "the logistic output function evaluated at a generic input vector, representing the predicted probability for that input in the context of logistic regression\n",
      "-----\n",
      "\n",
      "678ad48f-4fee-4578-b13a-06937cc64f2b\n",
      "The target expression refers to the function being approximated and minimized at the point labeled x sub one during the iterative optimization procedure described.\n",
      "-----\n",
      "\n",
      "1e47dcb6-fb74-4c27-9ea3-c609f866c0e0\n",
      "The expression denotes the case where the number of classes, represented by the variable K, is greater than two.\n",
      "-----\n",
      "\n",
      "34a96b49-6c94-43b4-a80f-762fedc7fbbf\n",
      "the point in n-dimensional space at which the function f is evaluated and at which the Hessian matrix is computed\n",
      "-----\n",
      "\n",
      "ef397fe8-d96f-4c3d-847e-9e238f344137\n",
      "the function being minimized, which is approximated quadratically at the point x sub zero\n",
      "-----\n",
      "\n",
      "dbd7170c-8286-4f4b-96b9-58c2c81ce0ee\n",
      "the number of features in the feature space after mapping\n",
      "-----\n",
      "\n",
      "439674fc-da9b-4aee-a295-1e5dcffe84aa\n",
      "the function being minimized, which is locally approximated by a quadratic at a given point in the optimization process\n",
      "-----\n",
      "\n",
      "b2f94925-5508-4400-89fb-a2f8b15f840d\n",
      "the total number of classes in the multinomial logistic regression model\n",
      "-----\n",
      "\n",
      "9d76e49b-632a-43a2-9a03-90d2e419246c\n",
      "The regularization technique that penalizes the sum of the squares of the model parameters, commonly used to prevent overfitting in optimization problems.\n",
      "-----\n",
      "\n",
      "c1464fc5-5c77-4e63-859b-fc49f97f364b\n",
      "The target expression denotes the Hessian matrix of the cross-entropy error function in the context of logistic regression.\n",
      "-----\n",
      "\n",
      "22130700-8d60-4393-bf17-3c63514b898c\n",
      "the input example vector that is provided to the softmax function, representing a data point in the context of multiclass classification\n",
      "-----\n",
      "\n",
      "c864dc57-20c2-42a7-ac43-1a26c89aac86\n",
      "This expression gives the second-order Taylor expansion of a multivariate function about a specified point, representing the function as the sum of its value at that point, the linear term involving the gradient, and the quadratic term involving the Hessian matrix.\n",
      "-----\n",
      "\n",
      "78f695fc-469b-4aa7-82c7-984a389b8f1d\n",
      "the function being minimized in the context of Newton's method, evaluated at a variable x, where x is not specified as a vector or scalar in this instance\n",
      "-----\n",
      "\n",
      "34956ff4-988d-4d09-8168-f470fff8c78b\n",
      "the Hessian matrix associated with the cross-entropy error in logistic regression, which has dimensions equal to the number of features plus one by the number of features plus one\n",
      "-----\n",
      "\n",
      "af0943fd-69d4-4a29-aef7-7259a84819fb\n",
      "the function being minimized, which is approximated quadratically at each iteration to determine the next point in the optimization process\n",
      "-----\n",
      "\n",
      "e23debf9-1c77-429e-bb5f-ff9f540419eb\n",
      "The total number of classes considered in the multinomial logistic regression model, with each class assigned its own weight vector.\n",
      "-----\n",
      "\n",
      "87845e30-5371-4804-8a62-da82578b6350\n",
      "The norm associated with L1 regularization, which is not differentiable and thus requires the use of subgradients or specialized optimization methods such as coordinate descent or proximal algorithms.\n",
      "-----\n",
      "\n",
      "8dfbf708-1514-4083-a410-acb48752b580\n",
      "the Hessian matrix whose invertibility is required for applying Newton's optimization method, specifically referring to the case when it is positive semi-definite but not positive definite\n",
      "-----\n",
      "\n",
      "c6762485-73f9-4259-a0cc-4085d373d4bf\n",
      "the regularization factor that appears on the diagonal of the diagonal matrix used in the regularized Hessian, except for the upper-left element, which is excluded from regularization\n",
      "-----\n",
      "\n",
      "df46cd82-f132-4ae0-8986-a599d10b5ad0\n",
      "The dimensions of the Hessian matrix, which is a square matrix with both its number of rows and columns equal to one more than the number of features in the feature space.\n",
      "-----\n",
      "\n",
      "4b2af739-afa4-4c5c-93fa-46c5912bc896\n",
      "The expression asserts that, for every non-zero vector, the quadratic form defined by the transpose of the vector, the Hessian matrix, and the vector itself is strictly greater than zero, which is the condition for the Hessian matrix to be positive definite.\n",
      "-----\n",
      "\n",
      "10518bb1-c507-4ed9-bca1-80b6232aa8ec\n",
      "the number of rows and columns of the Hessian matrix, where m is a parameter related to the problem's dimensionality\n",
      "-----\n",
      "\n",
      "8e4af8c7-7c9f-481d-9bd7-a6fd79900bca\n",
      "This expression represents the parameter update rule in gradient descent, where the current value of the parameter vector is updated by subtracting the product of a learning rate and the gradient of the function being minimized, both evaluated at the current parameter value.\n",
      "-----\n",
      "\n",
      "e4f46e8f-65da-4f07-bd68-8abe8b6369a0\n",
      "The target expression defines the element in the i-th row and j-th column of the Hessian matrix for a scalar-valued function of n variables as the second-order partial derivative of the function with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "2ea7ec24-9d58-4ed1-bd01-3235af27dd71\n",
      "The regularized Hessian matrix, defined as the sum of the original Hessian matrix and a scaled identity matrix, where the scaling factor is the regularization parameter.\n",
      "-----\n",
      "\n",
      "2bd9968a-982f-484a-968e-de7035dd08e1\n",
      "the weight vector associated with the k-th class in a multinomial logistic regression model, used to compute the class score for that class\n",
      "-----\n",
      "\n",
      "07ae1554-bef0-4f5c-9b66-ca603a645e5d\n",
      "the index representing a specific class among multiple classes in the context of multinomial logistic regression\n",
      "-----\n",
      "\n",
      "92e51a0a-32c7-42ea-b1c6-4fa580e66d72\n",
      "This expression denotes the point reached after the first update in an iterative minimization process, specifically the point that minimizes the quadratic approximation of the function following the initial step from the starting point.\n",
      "-----\n",
      "\n",
      "8332e1a5-6072-439f-abef-63af7e597882\n",
      "the index indicating a specific component of the output vector produced by the softmax function, corresponding to the k-th class or entry\n",
      "-----\n",
      "\n",
      "fab70b50-b79c-4f05-a309-7eae75c343a9\n",
      "This expression represents the iterative update rule for gradient descent, where the parameter vector at the next iteration is obtained by subtracting the product of a step size and the gradient of the function evaluated at the current parameter vector from the current parameter vector. The subscript denotes the iteration index.\n",
      "-----\n",
      "\n",
      "9b2507f4-4c66-4184-b87b-cea9c8b0a225\n",
      "the Hessian matrix of the function of a vector variable, evaluated at the point denoted by the subscript t\n",
      "-----\n",
      "\n",
      "3b1587f8-183e-482d-9292-88845399871c\n",
      "the Hessian matrix referenced in the context, which is associated with the cross-entropy error in logistic regression and has dimensions equal to the number of features plus one, squared\n",
      "-----\n",
      "\n",
      "ff676e66-86f6-47e4-865e-ffd8971dcfdc\n",
      "a square matrix with both the number of rows and columns equal to n\n",
      "-----\n",
      "\n",
      "314e6b6d-eac3-4e1d-a333-c55bfab24ae4\n",
      "The value representing the negative class label in binary logistic regression, used to distinguish it from the positive class label.\n",
      "-----\n",
      "\n",
      "be08a562-0456-47d4-b2bb-e23167c3ee42\n",
      "the diagonal matrix whose diagonal entries are equal to the regularization factor, except possibly for the upper-left element, which may differ because the corresponding weight is not regularized\n",
      "-----\n",
      "\n",
      "522e9c2d-0988-454b-a4c5-74992f062175\n",
      "The target is a right arrow symbol used to indicate the start of an example section following the explanation of the softmax function.\n",
      "-----\n",
      "\n",
      "0c6f64f8-580f-435a-8048-c26d481cfd78\n",
      "the number of classes, which is also the dimension of the vector produced by applying the softmax function to the set of class scores\n",
      "-----\n",
      "\n",
      "19fd0e54-57f6-43aa-99ef-ce2f4efa6b9e\n",
      "the number of dimensions of the input vector for a function that maps vectors to scalars, as referenced in the context of the Hessian matrix and the function's domain\n",
      "-----\n",
      "\n",
      "30aeb71a-0a0b-448b-9d25-6de4d348f2ba\n",
      "The parameter is set to the value one, indicating a full step size in the context of the parameter update rule for Newton's method when the Hessian matrix is computed exactly.\n",
      "-----\n",
      "\n",
      "805260f2-1861-4b65-8d61-0e3e223074a5\n",
      "the total number of classes considered in the generalization of the cross-entropy error and loss function\n",
      "-----\n",
      "\n",
      "cca9b94d-0688-4e48-9907-1700c34c8362\n",
      "A general function of a vector variable, which is the objective being minimized in the optimization context described.\n",
      "-----\n",
      "\n",
      "518298ff-5719-4b64-8054-4945c2e690bf\n",
      "A diagonal matrix whose diagonal entries are given by the product of the logistic function evaluated at each training example and one minus that value, for all examples in the dataset.\n",
      "-----\n",
      "\n",
      "47d95b5e-8d4b-4c4b-b7cf-74ec1a2349b4\n",
      "The target expression denotes the input example vector whose class membership probability is being evaluated by the multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "b0037e3b-e02d-4fbf-b51f-f8362567a34e\n",
      "the index representing a specific class among the possible classes in a multiclass classification setting\n",
      "-----\n",
      "\n",
      "f6b18dc0-384e-4976-94cb-5b2f822eea22\n",
      "the norm of the parameter vector w increases without bound, approaching infinity\n",
      "-----\n",
      "\n",
      "2ff98b89-de00-4879-9aa4-f84491953115\n",
      "the total number of classes in a classification problem, as referenced in the context of the softmax function and weight vectors for each class\n",
      "-----\n",
      "\n",
      "43622309-4dca-4e2d-ba17-f7bfe30a0b74\n",
      "The expression represents the decomposition of the Hessian matrix as the product of the transpose of a matrix denoted by Phi, a matrix S, and Phi itself. This decomposition is specifically for the Hessian matrix associated with the cross-entropy error in logistic regression.\n",
      "-----\n",
      "\n",
      "194ea07e-b617-4625-9e59-ce483c31a2e1\n",
      "the function of a vector variable whose convexity is being discussed in relation to the positive semi-definiteness of its Hessian matrix\n",
      "-----\n",
      "\n",
      "c98a9752-0a26-46d7-9314-97b9424584f0\n",
      "the number of basis functions in the set used for feature mapping in generalized linear models\n",
      "-----\n",
      "\n",
      "81a76926-9494-4b86-aaba-f0850a7198ce\n",
      "the activation function that defines the mean parameter of a distribution from the exponential family in the context of generalized linear models\n",
      "-----\n",
      "\n",
      "5c610df4-8b93-4ca9-ae7f-e2a80c4598fb\n",
      "the quadratic function used to approximate the original function in the context of Newton's optimization method, whose expansion is referenced for deriving the minimum and parameter update step\n",
      "-----\n",
      "\n",
      "29da21d7-b79c-46a5-b81e-6dbe5f8fff5b\n",
      "The expression asserts that for every non-zero vector, the quadratic form defined by the Hessian matrix is greater than or equal to zero, which is the condition for the Hessian matrix to be positive semi-definite.\n",
      "-----\n",
      "\n",
      "3828b5ab-bb4b-44ca-966b-2152681a0e01\n",
      "the point in the domain of the multivariate function f, about which the quadratic (second-order) Taylor expansion is performed; it is a vector and serves as the expansion point in the context of the Taylor series for f of a vector variable\n",
      "-----\n",
      "\n",
      "4fd61653-a791-4597-a469-d67704aa55dd\n",
      "The number of possible output values or classes in the multiclass regression setting is greater than two.\n",
      "-----\n",
      "\n",
      "e8838c6a-c174-4fdc-bc20-080ee7613a9c\n",
      "The matrix referenced, which may be ill-conditioned due to multicollinearity, is the Hessian matrix of the cross-entropy error function in logistic regression.\n",
      "-----\n",
      "\n",
      "b4c3e70d-afb6-4045-a646-590cb981d6c4\n",
      "The value representing the first possible outcome of a Bernoulli random variable, which can take on two values.\n",
      "-----\n",
      "\n",
      "aae4d847-6c05-492a-a1d3-4b43ace582fe\n",
      "The target expression denotes a multivariate function that maps an n-dimensional vector input to a scalar output, and is the function with respect to which the Hessian matrix is defined at a given point in the context of a quadratic expansion.\n",
      "-----\n",
      "\n",
      "25a977a8-9809-4a95-86f4-a1669039d3c2\n",
      "The variable y is restricted to take values from the set containing 0 and 1, indicating that it is a binary variable.\n",
      "-----\n",
      "\n",
      "57046e6a-56e3-4d27-992b-fe93ecf87c40\n",
      "The matrix product formed by multiplying the transpose of a matrix denoted by bold capital phi, a matrix denoted by bold capital S, and the matrix phi itself.\n",
      "-----\n",
      "\n",
      "f59960ac-5ced-473a-8da6-fa626ce1b014\n",
      "The target expression denotes the number of weight vectors in the matrix, which corresponds to the total number of classes in the multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "25f848a1-e15f-4c13-b7dc-dfe721a804fa\n",
      "The matrix associated with the Hessian of the cross-entropy error function in logistic regression, which is positive semi-definite.\n",
      "-----\n",
      "\n",
      "29a55367-a156-4a13-a46d-6e55032ef716\n",
      "A matrix multiplication where the first matrix has dimensions (m plus 1) by N and the second matrix has dimensions N by an unspecified number of columns.\n",
      "-----\n",
      "\n",
      "886b122d-7cb5-4f38-8c8e-7f5ca01b0632\n",
      "the Hessian matrix in the context of determining whether it is invertible, specifically referring to the case when it is positive semi-definite but not positive definite\n",
      "-----\n",
      "\n",
      "794401da-cca3-438d-8706-07128cc5d517\n",
      "the weight parameter corresponding to the upper-left element of the parameter vector, which is excluded from regularization in the context of L2 regularization\n",
      "-----\n",
      "\n",
      "7a7cf1dc-8f53-47e4-b670-e74d56cd2a91\n",
      "The target expression refers to the squared L2 norm regularization term that is added to the loss function or gradient in the context of regularized logistic regression, commonly used to penalize large weights and prevent overfitting.\n",
      "-----\n",
      "\n",
      "713bcf6b-ccfd-4675-abe4-0fe0be1aced1\n",
      "The target expression denotes the model corresponding to class k in a multinomial logistic regression framework, where each such model outputs the probability that a given input belongs to class k.\n",
      "-----\n",
      "\n",
      "cdb17f97-8cbe-45dd-b331-c1016cf1a590\n",
      "A five-dimensional vector whose components are 1, 2, 0.5, 5, and 3, used as an input example for the softmax function in the given context.\n",
      "-----\n",
      "\n",
      "e2b75b01-d9ca-4a14-b2fd-4a0a0f0f9de9\n",
      "the activation function that is applied to the scalar product of the weight vector and the feature vector in the definition of a generalized linear model\n",
      "-----\n",
      "\n",
      "b948cc6e-99aa-4d6b-8ea9-2d05fd2423e5\n",
      "the model corresponding to class k, which outputs the probability that an input belongs to class k, as defined by the softmax function\n",
      "-----\n",
      "\n",
      "290f6caf-c614-4f62-a843-b8f7d4b17868\n",
      "The target expression represents the scalar product of the weight vector associated with the k-th class and the input vector, used as the input to the activation function in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "4cc25d05-c6bf-4c68-8549-abada1720941\n",
      "the variable representing a non-zero vector used to test whether the Hessian matrix is positive definite, as indicated by the condition that the quadratic form with this vector is strictly greater than zero\n",
      "-----\n",
      "\n",
      "79d40230-d171-4c8a-afeb-745f0ba541ab\n",
      "A function that takes an n-dimensional real vector as input and produces an n-dimensional real vector as output.\n",
      "-----\n",
      "\n",
      "a49403b1-7863-4187-baef-2c234c4ffbfb\n",
      "The target expression refers to a type of regularization used in logistic regression, specifically the one that employs the sum of the absolute values of the model parameters as the penalty term.\n",
      "-----\n",
      "\n",
      "a2074584-10a1-4702-b24b-3fce21094d57\n",
      "The target expression gives the Taylor series expansion of a differentiable function about a point, showing both the explicit sum of terms involving derivatives of the function at that point and the equivalent summation notation for the infinite series.\n",
      "-----\n",
      "\n",
      "dddb0f08-cd1b-412e-b168-16554e740ef8\n",
      "the index representing a specific class in a multi-class classification problem, where the total number of classes is denoted by an uppercase variable\n",
      "-----\n",
      "\n",
      "4329f057-e667-4660-8ee5-d75086f34e56\n",
      "the integer one less than the class index k, representing the number of other models corresponding to the remaining classes except class k\n",
      "-----\n",
      "\n",
      "261700c7-d72a-491e-9156-51fc002df408\n",
      "the input feature vector for which the probability of belonging to a particular class is being evaluated in the context of a categorical variable model\n",
      "-----\n",
      "\n",
      "0f64770a-7716-4adc-a388-0f8cf543be1c\n",
      "The function evaluated at x, which is being approximated by a Taylor series expansion about the point a; in the context, it is the original function whose value is approximated by a finite or infinite sum of terms involving its derivatives at a.\n",
      "-----\n",
      "\n",
      "fe528624-4079-4e3a-8f2c-79378734d019\n",
      "The parameter that represents the probability of success in the Bernoulli distribution, used to define the probability of a binary outcome variable.\n",
      "-----\n",
      "\n",
      "20909273-e567-4fa8-9b1c-39eaef06e508\n",
      "This expression represents the second-order Taylor approximation of a differentiable function at a point, using the function's value, its first derivative, and its second derivative evaluated at that point, and is obtained by truncating the Taylor series after the quadratic term.\n",
      "-----\n",
      "\n",
      "6c0331c3-6433-4ea4-9071-a4c470bc9e02\n",
      "The target expression states that when the model's predicted probability for a given class is close to one, the natural logarithm of this probability is approximately zero.\n",
      "-----\n",
      "\n",
      "42ae3142-5c7d-4238-aa48-94c8508b31ef\n",
      "the activation function that defines the mean parameter of a distribution in the context of generalized linear models, mapping the linear predictor to the mean\n",
      "-----\n",
      "\n",
      "7338e299-4463-43cd-97b8-a25acc2d3f15\n",
      "The target expression denotes a matrix composed of K column vectors, where each column is a weight vector associated with one of the K classes in a multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "806b7d73-38ff-40d9-814b-c3a400687fbe\n",
      "the mean parameter of a distribution in the exponential family, as defined by the mean function in the context of generalized linear models\n",
      "-----\n",
      "\n",
      "505f56af-4ca6-4f4b-86c9-7567383ced09\n",
      "The target expression asserts that the number of possible values, denoted by an uppercase variable, is greater than two, indicating a generalization beyond the binary case.\n",
      "-----\n",
      "\n",
      "2eb83ec6-8638-4e54-84e4-b6eb03af7810\n",
      "The target expression refers to a function, denoted by a single letter, that is being discussed in the context of optimization methods such as gradient descent and Newton's method. It is the general function whose minimum is being sought, and it is considered as a function of one variable in this context.\n",
      "-----\n",
      "\n",
      "5c19855b-8893-411c-97a5-238d81adb34d\n",
      "The target expression represents the result of taking the dot product between the weight vector associated with class k and the input example vector, producing a scalar value that serves as the input to the softmax function for class k.\n",
      "-----\n",
      "\n",
      "fdd2be9b-8e84-4033-8eb9-bca0bdcb6d20\n",
      "The target expression refers to a type of regularization, specifically the one that is contrasted with another regularization method in the context of gradient descent, Newton's method, and least squares optimization for linear regression. It is described as being straightforward to incorporate into these optimization methods.\n",
      "-----\n",
      "\n",
      "84f8aa77-8b7e-4566-8e20-4ec5e24f54a2\n",
      "the input vector corresponding to the i-th training example, as referenced in the context of the quadratic error function and its gradient for linear regression\n",
      "-----\n",
      "\n",
      "36064af6-6db1-43e7-b664-67b6864cf96b\n",
      "The mean parameter of a distribution is given as the dot product of a weight vector and a feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "77bdfa7d-918b-4e53-87fe-07e2c8c1d0c4\n",
      "A column vector containing K elements, where each element corresponds to an indicator variable for one of K possible outcomes in a multinomial (multiclass) setting.\n",
      "-----\n",
      "\n",
      "ee823664-f200-47f1-a588-357c3925a362\n",
      "The Hessian matrix for the cross-entropy error in logistic regression, expressed as the product of the transpose of the design matrix, a diagonal matrix whose entries are the first derivatives of the logistic output for each training example, and the design matrix.\n",
      "-----\n",
      "\n",
      "82cd0d72-d9c4-435c-96c2-fd635109242f\n",
      "the index used to denote a specific example in a dataset, typically ranging from 1 to N, where N is the total number of examples\n",
      "-----\n",
      "\n",
      "ddb4c3d7-e7d0-4748-bea3-2d803f119c84\n",
      "The expression defines the k-th component of the softmax function applied to a vector of n real numbers, where the output is computed by exponentiating the k-th input and dividing by the sum of the exponentials of all input components.\n",
      "-----\n",
      "\n",
      "0bf62a1e-cb42-4a87-88e5-308f46fce98f\n",
      "This symbol refers to the Hessian matrix associated with the cross-entropy error function, which is discussed in the context of its positive semi-definiteness and invertibility properties for logistic regression.\n",
      "-----\n",
      "\n",
      "b2f39b9a-1ffe-48cf-b54b-cc9093d5337c\n",
      "The sum of all elements in the parameter vector, where each element represents a probability associated with a category, is equal to one.\n",
      "-----\n",
      "\n",
      "dbaba03e-0946-444b-999b-b3372b501355\n",
      "The target expression specifies that the input space has two dimensions, meaning each input vector consists of two components.\n",
      "-----\n",
      "\n",
      "7cf5d5fe-51ce-46af-83cf-b913f6107a7e\n",
      "The target expression denotes a specific class label, indexed by a variable, in a classification problem with multiple classes.\n",
      "-----\n",
      "\n",
      "091a9431-0ef7-4821-a6d9-821005fd6b3c\n",
      "the index representing a specific class in a multiclass classification setting, used to indicate which class's weights or outputs are being referenced or updated\n",
      "-----\n",
      "\n",
      "ca38986e-11ba-438f-a984-c2cd6e21bff2\n",
      "The target expression denotes the model used in multinomial logistic regression, which is further specified as a set of models indexed by class, each outputting the probability of an input belonging to a particular class using the softmax function.\n",
      "-----\n",
      "\n",
      "b85d9a46-578b-4528-94b1-1d1d5a0feb7c\n",
      "The target expression denotes the L2 norm, which is used as a regularization technique in the context of logistic regression and optimization, specifically referring to the addition of an L2 regularization term to the loss function or gradient.\n",
      "-----\n",
      "\n",
      "709e2d60-28ec-48c3-ba6f-bc317a49c990\n",
      "The probability that the k-th indicator variable in the vector representation of a multinoulli variable equals one, meaning the probability that the outcome corresponds to the k-th possible state.\n",
      "-----\n",
      "\n",
      "0d9cf02b-ae13-4b47-81ca-1b972b6d5bbc\n",
      "The target expression represents the case where the Bernoulli random variable takes the value one, which is one of its two possible outcomes.\n",
      "-----\n",
      "\n",
      "da2018b8-9e3d-4879-b7ab-b0584504a859\n",
      "The k-th element of a parameter vector representing the probability assigned to the k-th category in a categorical distribution, where all such elements are non-negative and sum to one.\n",
      "-----\n",
      "\n",
      "3b9269c6-609d-4142-97b4-fad4ab243e27\n",
      "The argument to the mean function, representing a linear combination of the weight vector and a feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "be3e625a-399b-4137-b000-a713bbe3b232\n",
      "The expression represents the Newton's method update rule for the weight vector in logistic regression, where the new weight vector is obtained by subtracting the product of the inverse of the Hessian matrix of the error function (with respect to the weights) and the gradient of the error function (with respect to the weights, given the dataset) from the current weight vector, with the step size set to one.\n",
      "-----\n",
      "\n",
      "abb6f77a-4d0b-4b6b-831e-23a532e70a93\n",
      "The condition that each parameter in the parameter vector, indexed by k, must be greater than or equal to zero, reflecting the requirement that probabilities cannot be negative.\n",
      "-----\n",
      "\n",
      "e36d8154-3205-452b-a71d-8e4c45bf1982\n",
      "The k-th weight vector used in the softmax regression model, which is one of the columns of the weight matrix and is associated with class k.\n",
      "-----\n",
      "\n",
      "2cd20333-c419-48e3-80c0-5e924f780204\n",
      "The value of the k-th indicator variable in the outcome vector is zero when the outcome is not the k-th possible state.\n",
      "-----\n",
      "\n",
      "5123d4f3-b9b0-4179-becb-c65ac770aa77\n",
      "This expression denotes the dimensions of a matrix product where a matrix with N rows is multiplied by a matrix with N rows and m plus one columns, resulting in a matrix with N rows and m plus one columns.\n",
      "-----\n",
      "\n",
      "eabab787-b04f-4637-a934-36068deb65b7\n",
      "the activation function used in the generalized linear model, which determines the mean function applied to the linear combination of weights and mapped input features\n",
      "-----\n",
      "\n",
      "412ebbf3-a82d-4165-a66d-56778e0a26cb\n",
      "The label variable for the i-th data point, as referenced in the context of probabilistic modeling of labels using distributions such as Bernoulli or multinoulli.\n",
      "-----\n",
      "\n",
      "922c0663-b652-40d9-ba09-22386b37cce4\n",
      "The expression gives the gradient of the regularized error function with respect to the parameter vector, showing it as the sum of the gradient of the original error function and an additional term proportional to the parameter vector, scaled by the regularization factor.\n",
      "-----\n",
      "\n",
      "2d3a70cb-cf79-4f72-ad33-74bdd449a5ef\n",
      "The target expression specifies the degree of the polynomial mapping used as an example, indicating that the polynomial basis functions are constructed up to and including degree two.\n",
      "-----\n",
      "\n",
      "b626be5e-2612-4d3f-a609-2faad72f0827\n",
      "the number of possible values or categories for the categorical variable, representing the total number of classes in the generalization from the Bernoulli to the categorical distribution\n",
      "-----\n",
      "\n",
      "e995235e-1a23-4956-88fe-9459c079ebba\n",
      "The target expression denotes the model corresponding to class k in the context of multinomial logistic regression, where a set of such models is defined, each responsible for predicting the probability that an input belongs to a specific class.\n",
      "-----\n",
      "\n",
      "89a08536-5c93-4bbd-951f-c93f16033139\n",
      "The target expression refers to a type of regularization commonly used in optimization and machine learning, specifically the one that penalizes the sum of the squares of the model parameters. It is being discussed in the context of its incorporation into gradient descent, Newton's method, and least squares optimization.\n",
      "-----\n",
      "\n",
      "21ea1f3e-0d8e-435c-a11d-f6208bccabea\n",
      "The target expression denotes the collection of models, each indexed by k, that together constitute the overall model for multinomial logistic regression. Each individual model in this set is associated with a specific class and is responsible for outputting the probability that a given input belongs to that class.\n",
      "-----\n",
      "\n",
      "bde986be-6e8e-412f-b7d6-316a0433c2a0\n",
      "The target expression denotes the label or class value corresponding to the positive class in binary logistic regression, as contrasted with the negative class label given by the other expression in the context.\n",
      "-----\n",
      "\n",
      "e060bd86-ae28-4327-9e87-7ae2211e0994\n",
      "The target expression denotes the probability that the k-th indicator variable equals one, that is, the probability that the multinomial variable takes its k-th possible state.\n",
      "-----\n",
      "\n",
      "fe9fc46c-8298-45a7-9f27-d62d9c50268a\n",
      "The sum over all components of the indicator vector representing the categorical variable equals one, reflecting that exactly one outcome occurs for each realization of the variable.\n",
      "-----\n",
      "\n",
      "e03da266-5414-4be3-af81-5c32f77b4d03\n",
      "The target expression defines a general model output as the result of applying an activation function to the dot product of a weight vector and a feature transformation of the input vector, representing the standard form of a generalized linear model.\n",
      "-----\n",
      "\n",
      "21f468ea-2631-4c62-a2ad-c39c2cc50e7e\n",
      "the collection of m basis functions, each being a nonlinear function of the input variables, used in the feature mapping for generalized linear models\n",
      "-----\n",
      "\n",
      "a890f4b4-7c50-42bc-94e2-80d286bbb6ce\n",
      "A function labeled with subscript j that maps from n-dimensional real vectors to real numbers, representing the j-th basis function in a set of m basis functions used for feature mapping in generalized linear models.\n",
      "-----\n",
      "\n",
      "d3daed8e-1cb6-46e2-a73d-c7e8e2c9c96b\n",
      "The target expression gives the update rule for the parameter vector at iteration t plus one, where the new value is obtained by subtracting the product of a step size parameter, the inverse of the Hessian matrix evaluated at the current step, and the gradient of the function evaluated at the current parameter vector, from the current parameter vector.\n",
      "-----\n",
      "\n",
      "38f87889-b427-44f6-901c-e3a30e641f99\n",
      "This expression specifies that the k-th component of the indicator vector representing a categorical (multinoulli) variable is equal to one, which occurs if and only if the outcome corresponds to the k-th category.\n",
      "-----\n",
      "\n",
      "90d66d2f-b61c-4c45-ac82-0fd6d122c5fd\n",
      "A function that maps an n-dimensional real vector to an (m+1)-dimensional real vector, where the output vector consists of the scalar 1 followed by the values of m basis functions evaluated at the input vector.\n",
      "-----\n",
      "\n",
      "a601c704-edc5-4d64-80fd-d165ba3de124\n",
      "A four-dimensional indicator vector representing a multinomial variable in which only the third component is one and all others are zero, signifying that the third category is the selected outcome among four possible categories.\n",
      "-----\n",
      "\n",
      "4e5e2e26-0ef0-43c1-8ee5-6783c6ec2405\n",
      "the dataset consisting of input-output pairs, where the inputs are denoted by a collection of feature vectors and the outputs are the corresponding labels, as referenced in the context of defining the probability and error functions for a classification model\n",
      "-----\n",
      "\n",
      "24c4f5e6-c8f3-44ee-985d-9995df215303\n",
      "The target expression defines the Hessian matrix of a scalar-valued function of n variables as a square matrix whose entries are the second-order partial derivatives of the function with respect to each pair of variables, arranged so that the entry in the i-th row and j-th column corresponds to the second partial derivative with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "9f6d1d24-7def-4856-b308-824816a2fd5b\n",
      "the index of a class in a classification problem, where the total number of classes is denoted by an uppercase variable, and the probability that an example belongs to this class is given by the model\n",
      "-----\n",
      "\n",
      "60716851-b601-40ed-a6c1-991e04c4cc38\n",
      "The target expression defines a function that computes the probability of the binary class label being one, given an input vector and a weight vector. It does this by applying the sigmoid function to the inner product of the weight vector and a feature transformation of the input, and equates this result to the conditional probability of the class label being one given the input.\n",
      "-----\n",
      "\n",
      "ce50fff0-8eba-44f2-974b-0e6652a957b9\n",
      "The target expression denotes the mean parameter of a distribution, specifically as it is represented as a function of the linear combination of weights and features in the context of exponential family distributions and their associated activation (mean) functions.\n",
      "-----\n",
      "\n",
      "ecd7a685-cd1a-434b-9f51-6812e3127353\n",
      "The target expression represents the weight update rule for stochastic gradient descent applied to the quadratic error function of linear regression, where the current weight vector is updated by subtracting the product of the learning rate and the gradient of the error with respect to a single training example.\n",
      "-----\n",
      "\n",
      "3a2a5fec-9672-47f4-9b53-e1d832f9e7da\n",
      "The label assigned to example indexed by i for class indexed by k, typically taking the value 1 if the example belongs to class k and 0 otherwise, as used in the context of multi-class classification and cross-entropy loss.\n",
      "-----\n",
      "\n",
      "b8aab71b-0eb4-4d49-b73a-70208cccc286\n",
      "The target expression defines the quadratic error function for linear regression, representing the sum over all N data points of the squared difference between the predicted value, given by the dot product of the weight vector and a feature mapping of the input, and the actual target value, with the sum scaled by one half.\n",
      "-----\n",
      "\n",
      "b6384dd2-1ca0-4d26-81ab-219da6eff786\n",
      "The target expression defines a parameter vector consisting of K elements, where each element is denoted by a subscripted symbol, and the vector is used to represent the parameters required for a categorical distribution with K possible outcomes.\n",
      "-----\n",
      "\n",
      "1251043d-81bd-46be-905d-de385a6efe3d\n",
      "The expression represents the update rule for the weight vector in a learning algorithm, where the current weight vector is adjusted by subtracting a scaled product of the model's prediction error for a specific example and the feature vector of that example. The scaling factor is the learning rate.\n",
      "-----\n",
      "\n",
      "4c78271e-bbca-4cfd-9ce2-f37ecd2e65d1\n",
      "the index corresponding to the outcome of a categorical variable, such that the k-th component of the indicator vector is one if the outcome is this value\n",
      "-----\n",
      "\n",
      "94c40d58-e450-4f2d-8016-e8d8f6dc84a1\n",
      "The target expression gives the probability mass function for a categorical random variable represented by a one-hot encoded vector, where the probability of observing a particular outcome is the product over all classes of the corresponding class probability parameter raised to the power of the indicator variable for that class.\n",
      "-----\n",
      "\n",
      "8b1cbc40-2053-462a-b280-afc14a50daf3\n",
      "The target expression gives the result of applying the softmax function to the five-dimensional vector whose entries are 1, 2, 0.5, 5, and 3, resulting in a five-dimensional vector of normalized values that sum to one: approximately 0.015, 0.041, 0.009, 0.823, and 0.112.\n",
      "-----\n",
      "\n",
      "dabe518c-6af9-4bce-b0d9-c65fa9afe79f\n",
      "The target expression denotes a function of a vector variable, specifically representing a function evaluated at a vector input in a multidimensional parameter space. It is referenced in the context of optimization methods, particularly in relation to convexity and quadratic approximation, and is used as an example of a function for which such methods are applicable.\n",
      "-----\n",
      "\n",
      "f1158a74-22e2-4823-9243-dd2fde7802cc\n",
      "The target expression refers to the Hessian matrix used in the context of optimizing the cross entropy error for logistic regression, specifically the matrix that can be replaced by the product of the transpose of a feature matrix, a diagonal matrix, and the feature matrix itself in the weights update rule.\n",
      "-----\n",
      "\n",
      "a6d3ab0e-ac1a-4b56-b42f-fa5edecafa6b\n",
      "This expression defines a specific feature mapping for a two-dimensional input vector, where the mapped feature vector consists of six components: a constant term, the two input variables, their product, and the squares of each input variable. This corresponds to a polynomial feature mapping of degree two for two input variables.\n",
      "-----\n",
      "\n",
      "b0ba148d-c40f-4c79-9d44-2ba597c8bbaa\n",
      "The target expression represents the gradient of the quadratic error function for linear regression, evaluated with respect to the weight vector and for a single data example. It is given as the product of the difference between the predicted output and the true output for the i-th example, and the feature vector corresponding to that example.\n",
      "-----\n",
      "\n",
      "251e57f2-1f8c-44f9-af08-73e8cb09e1f2\n",
      "the expected value of a function or quantity, denoted by a boldface W, conditioned on a dataset denoted by calligraphic D; it represents the error or loss function to be minimized in the context of a model, and is not generally solvable in closed form, thus requiring iterative optimization methods\n",
      "-----\n",
      "\n",
      "c77e9401-e8bc-4bd3-8b59-7e53c033f23c\n",
      "The target expression refers to the individual basis functions, indexed by j, used in the mapping from input variables to features in a generalized linear model. These functions can be any nonlinear transformations of the input variables and are not restricted to polynomials or specific forms.\n",
      "-----\n",
      "\n",
      "f0205d29-3342-4f91-97b9-fd8f4d253346\n",
      "The target expression defines the general form of a probability distribution belonging to the exponential family, where the probability of a variable given parameters is expressed as a product of a base measure and an exponential function involving the parameters, sufficient statistics, and a log-partition function.\n",
      "-----\n",
      "\n",
      "f379a0d2-b59d-4edd-85be-7b96098f2e63\n",
      "The target expression gives the log-likelihood of the observed labels given the input data, expressed as the sum over all examples and classes of the product of the label indicator and the logarithm of the predicted class probability, where the predicted probability is parameterized by a weight matrix and input features.\n",
      "-----\n",
      "\n",
      "814d4816-75cb-44fc-b398-61ef429e9fbc\n",
      "This expression represents the gradient of the error function with respect to the weight vector for class k, where the error function depends on the weight matrix given the dataset. The gradient is computed as a sum over all N examples, where for each example, the difference between the model's predicted value for class k and the true label for class k is multiplied by the feature vector of the example.\n",
      "-----\n",
      "\n",
      "e9f4fe22-ec28-4322-8ec3-1678ec2702e2\n",
      "the index that identifies a specific basis function within the model\n",
      "-----\n",
      "\n",
      "76f4b7cb-db17-4325-b240-00b1e2af08e5\n",
      "The target expression represents the error function to be minimized in a multiclass classification setting, defined as the negative sum over all training examples and all classes of the product of the true label indicator for each class and the logarithm of the predicted probability for that class, given the model parameters and input data.\n",
      "-----\n",
      "\n",
      "5254db3f-5eea-4d9e-a71a-2ff51c82f021\n",
      "This expression denotes the weight parameter associated with the i-th input feature for the j-th basis function in the inner layer of a generalized linear model with basis functions, where each basis function has its own set of weights.\n",
      "-----\n",
      "\n",
      "5d3ea627-1c51-4d14-8128-84ef42c1f3f0\n",
      "The target expression defines the probability that an input example belongs to a particular class in multinomial logistic regression. It expresses this probability as the output of the softmax function applied to a linear transformation of the input, where the numerator is the exponential of the dot product between the weight vector for the class and a feature mapping of the input, and the denominator is the sum of such exponentials over all classes. The expression is also equated to the conditional probability of the class label given the input and the set of weight vectors.\n",
      "-----\n",
      "\n",
      "91da4cb5-8eab-44d2-8879-991f96d5f727\n",
      "This expression defines the probability that an input vector belongs to class k, given the input and a set of model parameters. It equates this probability to the k-th output of a model, which is computed by exponentiating a linear function of the input (parameterized by a weight vector for class k), and then normalizing by the sum of exponentiated linear functions over all classes. This is the softmax function used in multiclass classification models.\n",
      "-----\n",
      "\n",
      "8e243264-f95d-4141-aa98-5b2ff5ad619a\n",
      "The target expression refers to the type of regularization that uses the sum of the absolute values of the model parameters, commonly known as L1 regularization, which is distinct from L2 regularization and is associated with the L1 norm.\n",
      "-----\n",
      "\n",
      "290b1886-a80b-4736-ad17-f2e44adf9814\n",
      "the index identifying the position of a weight within the weight vector associated with a particular basis function\n",
      "-----\n",
      "\n",
      "78a83656-0d88-4b33-bae7-998e21e57ada\n",
      "This is a set of four equations that together define the logistic regression algorithm: (1) the model output as the sigmoid of a linear function of the input features, which is also the probability of the positive class given the input and weights; (2) the probability of the label given the input and weights, expressed as a Bernoulli distribution parameterized by the model output; (3) the loss function, given by the negative log-likelihood (cross-entropy) for a binary label; and (4) the gradient of the loss with respect to the weights, which is the difference between the model output and the label, times the feature vector.\n",
      "-----\n",
      "\n",
      "9a210d5a-5fae-4ca5-9f1f-8bbd43180bbc\n",
      "The target expression defines the loss function for a single data example in a K-class classification setting, where the loss is computed as the negative sum over all classes of the product of the true label indicator for each class and the logarithm of the model's predicted probability for that class, evaluated at the given input and model parameters.\n",
      "-----\n",
      "\n",
      "32c67ef0-fa57-49c7-ad6a-0e490d165277\n",
      "This expression defines a function of an input vector and a parameter vector, where the function is constructed by first applying a linear transformation to the input using a matrix of weights, then applying a nonlinear activation function elementwise, then forming a weighted sum of the resulting values using a second set of weights, and finally applying the same nonlinear activation function to this sum. The expression also shows that each basis function is itself a nonlinear transformation of a linear combination of the input, and the overall function is a composition of these basis functions with another linear combination and nonlinearity.\n",
      "-----\n",
      "\n",
      "269bb1b6-23db-470a-89b0-c4a18cbbce4f\n",
      "This is a set of four expressions related to multinomial logistic regression: the first defines the k-th class probability as the softmax of a linear transformation of the input features, equating it to the probability of class k given the input and weights; the second gives the probability of a one-hot encoded label vector as a product over classes of the predicted probabilities raised to the power of the corresponding label entries; the third defines the loss function as the negative sum over classes of the label entries times the logarithm of the predicted probabilities; and the fourth gives the gradient of the loss with respect to the k-th class weight vector as the difference between the predicted probability and the label entry, times the feature vector.\n",
      "-----\n",
      "\n",
      "f47bb906-a8ce-4bdf-bedd-c4b9695cb14d\n",
      "the index that identifies a specific basis function in the model, used to distinguish between different weight vectors and their associated basis functions\n",
      "-----\n",
      "\n",
      "9745e002-9668-465f-abb2-89f237650fb2\n",
      "This is a set of four expressions that together specify the linear regression model in the context of generalized linear models. The first line defines the model output as a linear function of a feature transformation of the input, with the activation function being the identity. The second line gives the conditional probability of the target given the input and weights as a normal distribution with mean equal to the model output and fixed variance. The third line defines the loss as the squared difference between the model output and the target. The fourth line gives the gradient of the loss with respect to the weights as the product of the prediction error and the feature vector.\n",
      "-----\n",
      "\n",
      "93fe6bea-3f0f-4abb-a9cd-65d4d5a6b7c5\n",
      "This expression gives the probability mass function of a Bernoulli random variable, where the outcome variable can take values zero or one, and the probability of the outcome being one is given by a parameter. The probability is expressed as a function of the outcome and the parameter, using exponents to select the appropriate probability for each possible outcome.\n",
      "-----\n",
      "\n",
      "0ae86862-b5b5-4503-8b02-9968b312fe7e\n",
      "This is a superscript label used to distinguish between two different sets of weights in the model, specifically indicating the first set of weights used in the calculation of the model prediction.\n",
      "-----\n",
      "\n",
      "0723a3de-d204-4fba-b7f6-fd2ea9014292\n",
      "This is a superscript label used to distinguish the second set of weights in the model, indicating their role as the second set of parameters applied in the calculation of the model prediction.\n",
      "-----\n",
      "\n",
      "0658bf17-1226-4c17-9f74-016b95db149e\n",
      "The target expression denotes a vector of weights that are used to combine the outputs of the basis functions (or the first layer) in a two-layer neural network model.\n",
      "-----\n",
      "\n",
      "a3d59bdd-4230-4ac2-b19e-b66647a06038\n",
      "The target expression defines a generalized linear model function that takes an input vector and a weight vector, applies a feature mapping to the input, computes the weighted sum of the mapped features, and then applies an activation function to this sum. It also shows that this process can be written equivalently as applying the activation function to the dot product of the weight vector and the mapped feature vector, or as applying the activation function to the sum over all weights multiplied by their corresponding basis functions evaluated at the input.\n",
      "-----\n",
      "\n",
      "152e94d4-285c-460a-98ed-6934bc50780b\n",
      "The target expression represents a weight matrix associated with the first layer of a model, where each basis function has corresponding weights that are multiplied with all the input features and summed.\n",
      "-----\n",
      "\n",
      "2660f961-87ad-44bc-bd6e-408942d53863\n",
      "the Hessian matrix referenced in the context of Newton's optimization method, whose invertibility is required for the method to be applicable\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in math_expression_descriptions:\n",
    "    print(x.id)\n",
    "    print(x.text)\n",
    "    print('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ebf8e",
   "metadata": {},
   "source": [
    "### 4. MathExpressionDescriptionOpt, requires: MathExpressionDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55912beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionDescriptionOptimizer as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionDescriptionOpt\n",
    "\n",
    "\n",
    "inputs, input_id_to_math_expression_description = InputCreatorUtil.create(\n",
    "    math_expression_descriptions, lambda x: AssistantInput(description=x.text)\n",
    ")\n",
    "outputs = await math_expression_description_optimizer_assistant.concurrent_assist(inputs)\n",
    "math_expression_descriptions_opt = [\n",
    "    MathExpressionDescriptionOpt(\n",
    "        math_expression_id=input_id_to_math_expression_description[\n",
    "            output.input_id\n",
    "        ].math_expression_id,\n",
    "        math_expression_description_id=input_id_to_math_expression_description[output.input_id].id,\n",
    "        math_expression_index_id=index.id,\n",
    "        text=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0aa5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4fa1f4b5-66c0-4c72-a0f9-46f876035c4f\n",
      "Number of examples in the training set.\n",
      "-----\n",
      "\n",
      "082446e9-48b4-4cb1-bf39-2638a9aa62b1\n",
      "Number of dimensions in the feature space.\n",
      "-----\n",
      "\n",
      "bf9900fe-ea6c-4ce1-9fbd-e34c754376ca\n",
      "Value of the parameter vector at iteration t in an iterative optimization process.\n",
      "-----\n",
      "\n",
      "b2f94925-5508-4400-89fb-a2f8b15f840d\n",
      "total number of classes in multinomial logistic regression model\n",
      "-----\n",
      "\n",
      "34a96b49-6c94-43b4-a80f-762fedc7fbbf\n",
      "Point in n-dimensional space where function f is evaluated and Hessian matrix is computed.\n",
      "-----\n",
      "\n",
      "68e6418c-1209-4b4d-bd30-14dc913dc533\n",
      "Hessian matrix in Newton's optimization method determines the curvature of the function being minimized.\n",
      "-----\n",
      "\n",
      "40de932a-3537-479b-a354-9321d97b13e6\n",
      "Error function dependent on vector w, conditioned on dataset D.\n",
      "-----\n",
      "\n",
      "fee0f857-90da-428f-987d-811d3ad1f34e\n",
      "Function mapping an n-dimensional real vector to a real-valued scalar output.\n",
      "-----\n",
      "\n",
      "1cca5ec7-5406-480f-b0c8-8d20d5601030\n",
      "value of a differentiable function at variable x\n",
      "-----\n",
      "\n",
      "4bdf55a1-065f-463c-8b44-77db592f1888\n",
      "point where a function is expanded in its Taylor series representation\n",
      "-----\n",
      "\n",
      "f6b18dc0-384e-4976-94cb-5b2f822eea22\n",
      "The norm of the parameter vector w increases without bound, approaching infinity.\n",
      "-----\n",
      "\n",
      "dbd7170c-8286-4f4b-96b9-58c2c81ce0ee\n",
      "Number of features in the feature space after mapping.\n",
      "-----\n",
      "\n",
      "c1464fc5-5c77-4e63-859b-fc49f97f364b\n",
      "Hessian matrix of the cross-entropy error function in logistic regression.\n",
      "-----\n",
      "\n",
      "ae67cc28-59b1-4ed6-a289-85933dbf9137\n",
      "Black curve representing a single-variable function minimized using Newton's method.\n",
      "-----\n",
      "\n",
      "77abde18-8d84-4cb8-bbf0-c67858d23cd4\n",
      "Initial point for starting minimization of a single-variable function using Newton's method.\n",
      "-----\n",
      "\n",
      "64559b13-b854-42ef-90f0-69ca004c25c8\n",
      "Logistic output function evaluated at a generic input vector, representing the predicted probability for that input in logistic regression.\n",
      "-----\n",
      "\n",
      "ef397fe8-d96f-4c3d-847e-9e238f344137\n",
      "Function minimized, approximated quadratically at point x₀.\n",
      "-----\n",
      "\n",
      "439674fc-da9b-4aee-a295-1e5dcffe84aa\n",
      "Function minimized in optimization, locally approximated by a quadratic at a given point.\n",
      "-----\n",
      "\n",
      "07ae1554-bef0-4f5c-9b66-ca603a645e5d\n",
      "Index identifying a specific class among multiple classes in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "c98a9752-0a26-46d7-9314-97b9424584f0\n",
      "Number of basis functions used for feature mapping in generalized linear models.\n",
      "-----\n",
      "\n",
      "9c9821cd-6361-4e03-83c2-6acf6291d2b9\n",
      "Real-valued function with a vector input variable.\n",
      "-----\n",
      "\n",
      "4fd61653-a791-4597-a469-d67704aa55dd\n",
      "In multiclass regression, the number of possible output values or classes exceeds two.\n",
      "-----\n",
      "\n",
      "c6762485-73f9-4259-a0cc-4085d373d4bf\n",
      "Regularization factor applied to the diagonal elements of the regularized Hessian matrix, excluding the upper-left element from regularization.\n",
      "-----\n",
      "\n",
      "e23debf9-1c77-429e-bb5f-ff9f540419eb\n",
      "Total number of classes in a multinomial logistic regression model, each with an assigned weight vector.\n",
      "-----\n",
      "\n",
      "df46cd82-f132-4ae0-8986-a599d10b5ad0\n",
      "The Hessian matrix is a square matrix with dimensions equal to one more than the number of features in the feature space for both rows and columns.\n",
      "-----\n",
      "\n",
      "36b9967c-d97b-40f6-a890-edfa0b7f9a89\n",
      "Output vector dimension from the softmax function equals the number of classes.\n",
      "-----\n",
      "\n",
      "dd828d70-4bcc-4ada-a766-35e0a49b2339\n",
      "Symbol denoting a non-zero vector used to test the positive semi-definiteness of the Hessian matrix.\n",
      "-----\n",
      "\n",
      "8332e1a5-6072-439f-abef-63af7e597882\n",
      "Index specifying a component of the output vector from the softmax function, corresponding to the k-th class or entry.\n",
      "-----\n",
      "\n",
      "f6a6a73b-f275-43ad-8a0d-5956cca28a04\n",
      "Point where the quadratic approximation of a function is constructed and the parabola is tangential during minimization.\n",
      "-----\n",
      "\n",
      "2bd9968a-982f-484a-968e-de7035dd08e1\n",
      "Weight vector for the k-th class in multinomial logistic regression, used to compute the class score for that class.\n",
      "-----\n",
      "\n",
      "194ea07e-b617-4625-9e59-ce483c31a2e1\n",
      "Function of a vector variable; convexity determined by positive semi-definiteness of its Hessian matrix.\n",
      "-----\n",
      "\n",
      "81a76926-9494-4b86-aaba-f0850a7198ce\n",
      "Activation function specifying the mean parameter of an exponential family distribution in generalized linear models.\n",
      "-----\n",
      "\n",
      "10518bb1-c507-4ed9-bca1-80b6232aa8ec\n",
      "Number of rows and columns of the Hessian matrix; m denotes a parameter related to the problem's dimensionality.\n",
      "-----\n",
      "\n",
      "af0943fd-69d4-4a29-aef7-7259a84819fb\n",
      "Function minimized by quadratic approximation at each iteration to determine the next optimization point.\n",
      "-----\n",
      "\n",
      "e4f46e8f-65da-4f07-bd68-8abe8b6369a0\n",
      "The element in the i-th row and j-th column of the Hessian matrix for a scalar-valued function of n variables is the second-order partial derivative of the function with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "92e51a0a-32c7-42ea-b1c6-4fa580e66d72\n",
      "Point reached after the first update in an iterative minimization process, minimizing the quadratic approximation of the function after the initial step from the starting point.\n",
      "-----\n",
      "\n",
      "9d76e49b-632a-43a2-9a03-90d2e419246c\n",
      "Regularization technique penalizing the sum of the squares of model parameters, commonly used to prevent overfitting in optimization problems.\n",
      "-----\n",
      "\n",
      "43622309-4dca-4e2d-ba17-f7bfe30a0b74\n",
      "Decomposition of the Hessian matrix for the cross-entropy error in logistic regression as the product of the transpose of matrix Phi, matrix S, and Phi.\n",
      "-----\n",
      "\n",
      "7338e299-4463-43cd-97b8-a25acc2d3f15\n",
      "A matrix of K column vectors, each representing a weight vector for one of the K classes in a multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "261700c7-d72a-491e-9156-51fc002df408\n",
      "Input feature vector evaluated for class membership probability in a categorical variable model.\n",
      "-----\n",
      "\n",
      "42ae3142-5c7d-4238-aa48-94c8508b31ef\n",
      "Activation function in generalized linear models that maps the linear predictor to the mean parameter of a distribution.\n",
      "-----\n",
      "\n",
      "29da21d7-b79c-46a5-b81e-6dbe5f8fff5b\n",
      "For every non-zero vector, the quadratic form of the Hessian matrix is greater than or equal to zero, indicating the Hessian is positive semi-definite.\n",
      "-----\n",
      "\n",
      "47d95b5e-8d4b-4c4b-b7cf-74ec1a2349b4\n",
      "The target expression refers to the input example vector evaluated for class membership probability by the multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "22130700-8d60-4393-bf17-3c63514b898c\n",
      "Input example vector provided to the softmax function, representing a data point in multiclass classification.\n",
      "-----\n",
      "\n",
      "fe528624-4079-4e3a-8f2c-79378734d019\n",
      "Parameter representing the probability of success in the Bernoulli distribution, defining the probability of a binary outcome variable.\n",
      "-----\n",
      "\n",
      "8e4af8c7-7c9f-481d-9bd7-a6fd79900bca\n",
      "Parameter update rule in gradient descent: the parameter vector is updated by subtracting the product of the learning rate and the gradient of the minimized function, both evaluated at the current parameter value.\n",
      "-----\n",
      "\n",
      "4329f057-e667-4660-8ee5-d75086f34e56\n",
      "Integer equal to the class index k minus one, indicating the number of models for all classes except class k.\n",
      "-----\n",
      "\n",
      "dddb0f08-cd1b-412e-b168-16554e740ef8\n",
      "Index identifying a specific class in a multi-class classification problem with the total number of classes represented by an uppercase variable.\n",
      "-----\n",
      "\n",
      "b0037e3b-e02d-4fbf-b51f-f8362567a34e\n",
      "Index identifying a specific class among possible classes in multiclass classification.\n",
      "-----\n",
      "\n",
      "5c610df4-8b93-4ca9-ae7f-e2a80c4598fb\n",
      "Quadratic function approximating the original function in Newton's optimization method, used for expansion to derive the minimum and parameter update step.\n",
      "-----\n",
      "\n",
      "87845e30-5371-4804-8a62-da82578b6350\n",
      "L1 regularization norm, non-differentiable, requires subgradients or specialized optimization methods such as coordinate descent or proximal algorithms.\n",
      "-----\n",
      "\n",
      "806b7d73-38ff-40d9-814b-c3a400687fbe\n",
      "Mean parameter of an exponential family distribution, defined by the mean function in generalized linear models.\n",
      "-----\n",
      "\n",
      "78f695fc-469b-4aa7-82c7-984a389b8f1d\n",
      "Function minimized using Newton's method, evaluated at variable x of unspecified type (vector or scalar not defined).\n",
      "-----\n",
      "\n",
      "0f64770a-7716-4adc-a388-0f8cf543be1c\n",
      "Function evaluated at x, approximated by a Taylor series expansion about point a; the original function whose value is represented by a sum of terms involving its derivatives at a.\n",
      "-----\n",
      "\n",
      "c864dc57-20c2-42a7-ac43-1a26c89aac86\n",
      "Second-order Taylor expansion of a multivariate function about a specified point, representing the function as the sum of its value at that point, a linear term with the gradient, and a quadratic term with the Hessian matrix.\n",
      "-----\n",
      "\n",
      "eabab787-b04f-4637-a934-36068deb65b7\n",
      "Activation function in a generalized linear model that defines the mean function applied to the linear combination of weights and input features.\n",
      "-----\n",
      "\n",
      "6c0331c3-6433-4ea4-9071-a4c470bc9e02\n",
      "When a model's predicted probability for a class approaches one, the natural logarithm of this probability approaches zero.\n",
      "-----\n",
      "\n",
      "34956ff4-988d-4d09-8168-f470fff8c78b\n",
      "Hessian matrix of the cross-entropy error in logistic regression with dimensions (number of features + 1) × (number of features + 1).\n",
      "-----\n",
      "\n",
      "2ff98b89-de00-4879-9aa4-f84491953115\n",
      "Total number of classes in a classification problem, used in the context of the softmax function and weight vectors for each class.\n",
      "-----\n",
      "\n",
      "f47bb906-a8ce-4bdf-bedd-c4b9695cb14d\n",
      "Index identifying a specific basis function in the model, distinguishing between different weight vectors and their associated basis functions.\n",
      "-----\n",
      "\n",
      "25a977a8-9809-4a95-86f4-a1669039d3c2\n",
      "y is a binary variable restricted to values 0 and 1.\n",
      "-----\n",
      "\n",
      "2ea7ec24-9d58-4ed1-bd01-3235af27dd71\n",
      "Regularized Hessian matrix: sum of the original Hessian matrix and a scaled identity matrix, with the scaling factor as the regularization parameter.\n",
      "-----\n",
      "\n",
      "2d3a70cb-cf79-4f72-ad33-74bdd449a5ef\n",
      "The target expression specifies that the polynomial mapping uses basis functions constructed up to and including degree two.\n",
      "-----\n",
      "\n",
      "0658bf17-1226-4c17-9f74-016b95db149e\n",
      "Vector of weights used to combine outputs of basis functions or first layer in a two-layer neural network model.\n",
      "-----\n",
      "\n",
      "678ad48f-4fee-4578-b13a-06937cc64f2b\n",
      "The target expression is the function approximated and minimized at x₁ during the iterative optimization procedure.\n",
      "-----\n",
      "\n",
      "abb6f77a-4d0b-4b6b-831e-23a532e70a93\n",
      "Each parameter in the parameter vector, indexed by k, must be greater than or equal to zero, reflecting the requirement that probabilities cannot be negative.\n",
      "-----\n",
      "\n",
      "505f56af-4ca6-4f4b-86c9-7567383ced09\n",
      "The number of possible values, represented by an uppercase variable, exceeds two, indicating a generalization beyond the binary case.\n",
      "-----\n",
      "\n",
      "5c19855b-8893-411c-97a5-238d81adb34d\n",
      "The dot product of the weight vector for class k and the input example vector yields a scalar used as the softmax input for class k.\n",
      "-----\n",
      "\n",
      "ce50fff0-8eba-44f2-974b-0e6652a957b9\n",
      "Mean parameter of a distribution represented as a function of the linear combination of weights and features in exponential family distributions and their associated activation (mean) functions.\n",
      "-----\n",
      "\n",
      "fab70b50-b79c-4f05-a309-7eae75c343a9\n",
      "Iterative update rule for gradient descent: the parameter vector at the next iteration equals the current parameter vector minus the product of the step size and the gradient of the function evaluated at the current parameter vector; the subscript indicates the iteration index.\n",
      "-----\n",
      "\n",
      "0d9cf02b-ae13-4b47-81ca-1b972b6d5bbc\n",
      "The target expression corresponds to the outcome where the Bernoulli random variable equals one, one of its two possible values.\n",
      "-----\n",
      "\n",
      "25f848a1-e15f-4c13-b7dc-dfe721a804fa\n",
      "Matrix representing the Hessian of the cross-entropy error function in logistic regression; positive semi-definite.\n",
      "-----\n",
      "\n",
      "57046e6a-56e3-4d27-992b-fe93ecf87c40\n",
      "Matrix product of the transpose of bold capital phi, bold capital S, and bold capital phi.\n",
      "-----\n",
      "\n",
      "f59960ac-5ced-473a-8da6-fa626ce1b014\n",
      "The number of weight vectors in the matrix equals the total number of classes in the multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "3b9269c6-609d-4142-97b4-fad4ab243e27\n",
      "Argument to the mean function; a linear combination of the weight vector and the feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "2eb83ec6-8638-4e54-84e4-b6eb03af7810\n",
      "A single-letter function representing the general objective in optimization methods like gradient descent and Newton's method, considered as a function of one variable whose minimum is sought.\n",
      "-----\n",
      "\n",
      "5123d4f3-b9b0-4179-becb-c65ac770aa77\n",
      "A matrix with N rows multiplied by a matrix with N rows and m+1 columns yields a matrix with N rows and m+1 columns.\n",
      "-----\n",
      "\n",
      "9b2507f4-4c66-4184-b87b-cea9c8b0a225\n",
      "Hessian matrix of a vector-valued function evaluated at point t\n",
      "-----\n",
      "\n",
      "4c78271e-bbca-4cfd-9ce2-f37ecd2e65d1\n",
      "Index of a categorical variable's outcome; the k-th component of the indicator vector is one if the outcome equals this value.\n",
      "-----\n",
      "\n",
      "709e2d60-28ec-48c3-ba6f-bc317a49c990\n",
      "Probability that the k-th indicator variable in a multinoulli vector equals one, representing the probability that the outcome is the k-th possible state.\n",
      "-----\n",
      "\n",
      "3828b5ab-bb4b-44ca-966b-2152681a0e01\n",
      "The vector in the domain of a multivariate function f that serves as the expansion point for the quadratic (second-order) Taylor expansion of f with respect to a vector variable.\n",
      "-----\n",
      "\n",
      "152e94d4-285c-460a-98ed-6934bc50780b\n",
      "Weight matrix for the first model layer; each basis function has weights multiplied by all input features and summed.\n",
      "-----\n",
      "\n",
      "b6384dd2-1ca0-4d26-81ab-219da6eff786\n",
      "Parameter vector of K elements, each denoted by a subscripted symbol, representing parameters for a categorical distribution with K possible outcomes.\n",
      "-----\n",
      "\n",
      "e9f4fe22-ec28-4322-8ec3-1678ec2702e2\n",
      "Index identifying a specific basis function within the model.\n",
      "-----\n",
      "\n",
      "e36d8154-3205-452b-a71d-8e4c45bf1982\n",
      "The k-th weight vector in softmax regression, a column of the weight matrix corresponding to class k.\n",
      "-----\n",
      "\n",
      "da2018b8-9e3d-4879-b7ab-b0584504a859\n",
      "The k-th element of a parameter vector representing the probability of the k-th category in a categorical distribution, with all elements non-negative and summing to one.\n",
      "-----\n",
      "\n",
      "e8838c6a-c174-4fdc-bc20-080ee7613a9c\n",
      "The Hessian matrix of the cross-entropy error function in logistic regression may be ill-conditioned due to multicollinearity.\n",
      "-----\n",
      "\n",
      "ca3e0217-7435-428f-b730-548872b6fcb9\n",
      "The point at which a differentiable function's Taylor series expansion is centered.\n",
      "-----\n",
      "\n",
      "77bdfa7d-918b-4e53-87fe-07e2c8c1d0c4\n",
      "Column vector with K elements, each representing an indicator variable for one of K possible outcomes in a multinomial (multiclass) setting.\n",
      "-----\n",
      "\n",
      "20909273-e567-4fa8-9b1c-39eaef06e508\n",
      "Second-order Taylor approximation of a differentiable function at a point, using the function's value, first derivative, and second derivative at that point, obtained by truncating the Taylor series after the quadratic term.\n",
      "-----\n",
      "\n",
      "2cd20333-c419-48e3-80c0-5e924f780204\n",
      "The k-th indicator variable in the outcome vector equals zero if the outcome is not the k-th possible state.\n",
      "-----\n",
      "\n",
      "0ae86862-b5b5-4503-8b02-9968b312fe7e\n",
      "Superscript label distinguishing between two sets of model weights, indicating the first set used in model prediction calculation.\n",
      "-----\n",
      "\n",
      "ff676e66-86f6-47e4-865e-ffd8971dcfdc\n",
      "A matrix with n rows and n columns.\n",
      "-----\n",
      "\n",
      "3a2a5fec-9672-47f4-9b53-e1d832f9e7da\n",
      "Label for example i and class k, equal to 1 if example i belongs to class k and 0 otherwise, used in multi-class classification and cross-entropy loss.\n",
      "-----\n",
      "\n",
      "4b2af739-afa4-4c5c-93fa-46c5912bc896\n",
      "For every non-zero vector, the quadratic form involving the vector transpose, the Hessian matrix, and the vector is strictly greater than zero, indicating the Hessian matrix is positive definite.\n",
      "-----\n",
      "\n",
      "412ebbf3-a82d-4165-a66d-56778e0a26cb\n",
      "Label variable for the i-th data point in probabilistic modeling, using distributions such as Bernoulli or multinoulli.\n",
      "-----\n",
      "\n",
      "aae4d847-6c05-492a-a1d3-4b43ace582fe\n",
      "Multivariate function mapping n-dimensional vector input to scalar output, serving as the function for which the Hessian matrix is defined at a given point in quadratic expansion.\n",
      "-----\n",
      "\n",
      "0723a3de-d204-4fba-b7f6-fd2ea9014292\n",
      "Superscript label distinguishing the second set of weights in the model, indicating their role as the second set of parameters applied in model prediction calculation.\n",
      "-----\n",
      "\n",
      "b4c3e70d-afb6-4045-a646-590cb981d6c4\n",
      "Value representing the first possible outcome of a Bernoulli random variable, which can take two values.\n",
      "-----\n",
      "\n",
      "b626be5e-2612-4d3f-a609-2faad72f0827\n",
      "Number of possible values or categories for a categorical variable, representing the total number of classes in the generalization from the Bernoulli to the categorical distribution.\n",
      "-----\n",
      "\n",
      "314e6b6d-eac3-4e1d-a333-c55bfab24ae4\n",
      "Value assigned to the negative class label in binary logistic regression, distinguishing it from the positive class label.\n",
      "-----\n",
      "\n",
      "84f8aa77-8b7e-4566-8e20-4ec5e24f54a2\n",
      "Input vector for the i-th training example in the context of the quadratic error function and its gradient for linear regression.\n",
      "-----\n",
      "\n",
      "60716851-b601-40ed-a6c1-991e04c4cc38\n",
      "Defines a function computing the probability that a binary class label is one, given an input vector and weight vector, by applying the sigmoid function to the inner product of the weight vector and a feature transformation of the input, equating this to the conditional probability of the class label being one given the input.\n",
      "-----\n",
      "\n",
      "ecd7a685-cd1a-434b-9f51-6812e3127353\n",
      "Weight update rule for stochastic gradient descent on the quadratic error function in linear regression: current weight vector is updated by subtracting the product of the learning rate and the gradient of the error with respect to a single training example.\n",
      "-----\n",
      "\n",
      "b8aab71b-0eb4-4d49-b73a-70208cccc286\n",
      "Quadratic error function for linear regression: sum over N data points of squared difference between predicted value (dot product of weight vector and feature mapping of input) and actual target value, scaled by one half.\n",
      "-----\n",
      "\n",
      "94c40d58-e450-4f2d-8016-e8d8f6dc84a1\n",
      "Probability mass function for a categorical random variable represented by a one-hot encoded vector, where the probability of an outcome equals the product over all classes of the class probability parameter raised to the power of the indicator variable for that class.\n",
      "-----\n",
      "\n",
      "29a55367-a156-4a13-a46d-6e55032ef716\n",
      "Matrix multiplication with a first matrix of dimensions (m+1)×N and a second matrix of dimensions N×k, where k is unspecified.\n",
      "-----\n",
      "\n",
      "e060bd86-ae28-4327-9e87-7ae2211e0994\n",
      "Probability that the k-th indicator variable equals one, representing the probability that the multinomial variable takes its k-th possible state.\n",
      "-----\n",
      "\n",
      "1251043d-81bd-46be-905d-de385a6efe3d\n",
      "The update rule adjusts the weight vector by subtracting the learning rate multiplied by the prediction error for a specific example and its feature vector.\n",
      "-----\n",
      "\n",
      "886b122d-7cb5-4f38-8c8e-7f5ca01b0632\n",
      "Hessian matrix that is positive semi-definite but not positive definite, with focus on invertibility.\n",
      "-----\n",
      "\n",
      "ee823664-f200-47f1-a588-357c3925a362\n",
      "Hessian matrix for cross-entropy error in logistic regression equals the product of the transpose of the design matrix, a diagonal matrix with entries as first derivatives of the logistic output for each training example, and the design matrix.\n",
      "-----\n",
      "\n",
      "3b1587f8-183e-482d-9292-88845399871c\n",
      "Hessian matrix associated with cross-entropy error in logistic regression; dimensions are (number of features plus one) squared.\n",
      "-----\n",
      "\n",
      "36064af6-6db1-43e7-b664-67b6864cf96b\n",
      "The mean parameter of a distribution equals the dot product of a weight vector and a feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "0bf62a1e-cb42-4a87-88e5-308f46fce98f\n",
      "Hessian matrix of the cross-entropy error function, analyzed for positive semi-definiteness and invertibility in logistic regression.\n",
      "-----\n",
      "\n",
      "2660f961-87ad-44bc-bd6e-408942d53863\n",
      "Hessian matrix in Newton's optimization method, required to be invertible for method applicability.\n",
      "-----\n",
      "\n",
      "8b1cbc40-2053-462a-b280-afc14a50daf3\n",
      "Applying the softmax function to the vector [1, 2, 0.5, 5, 3] yields the normalized vector [0.015, 0.041, 0.009, 0.823, 0.112], with values summing to one.\n",
      "-----\n",
      "\n",
      "93fe6bea-3f0f-4abb-a9cd-65d4d5a6b7c5\n",
      "Probability mass function of a Bernoulli random variable with outcomes 0 or 1, where the probability of outcome 1 is a parameter; the probability is expressed as a function of the outcome and the parameter using exponents to select the corresponding probability for each outcome.\n",
      "-----\n",
      "\n",
      "ddb4c3d7-e7d0-4748-bea3-2d803f119c84\n",
      "Defines the k-th component of the softmax function for a vector of n real numbers as the exponential of the k-th input divided by the sum of exponentials of all input components.\n",
      "-----\n",
      "\n",
      "be3e625a-399b-4137-b000-a713bbe3b232\n",
      "Newton's method update rule for logistic regression weight vector: the new weight vector equals the current weight vector minus the product of the inverse Hessian matrix of the error function (with respect to weights) and the gradient of the error function (with respect to weights, given the dataset), with step size one.\n",
      "-----\n",
      "\n",
      "1e47dcb6-fb74-4c27-9ea3-c609f866c0e0\n",
      "K represents the number of classes, with K > 2.\n",
      "-----\n",
      "\n",
      "21f468ea-2631-4c62-a2ad-c39c2cc50e7e\n",
      "A set of m basis functions, each a nonlinear function of input variables, used in feature mapping for generalized linear models.\n",
      "-----\n",
      "\n",
      "bde986be-6e8e-412f-b7d6-316a0433c2a0\n",
      "The target expression specifies the positive class label in binary logistic regression, contrasted with the negative class label indicated by another expression.\n",
      "-----\n",
      "\n",
      "f379a0d2-b59d-4edd-85be-7b96098f2e63\n",
      "Log-likelihood of observed labels given input data, computed as the sum over all examples and classes of the product of the label indicator and the logarithm of the predicted class probability, with predicted probability parameterized by a weight matrix and input features.\n",
      "-----\n",
      "\n",
      "251e57f2-1f8c-44f9-af08-73e8cb09e1f2\n",
      "Expected value of a function or quantity, denoted by boldface W, conditioned on dataset calligraphic D; represents the error or loss function to be minimized in a model; generally not solvable in closed form, requiring iterative optimization methods.\n",
      "-----\n",
      "\n",
      "e995235e-1a23-4956-88fe-9459c079ebba\n",
      "The target expression represents the model for class k in multinomial logistic regression, where each model predicts the probability that an input belongs to its respective class.\n",
      "-----\n",
      "\n",
      "e03da266-5414-4be3-af81-5c32f77b4d03\n",
      "A general model output is computed by applying an activation function to the dot product of a weight vector and a feature transformation of the input vector, representing the standard form of a generalized linear model.\n",
      "-----\n",
      "\n",
      "89a08536-5c93-4bbd-951f-c93f16033139\n",
      "A regularization technique in optimization and machine learning that penalizes the sum of the squares of model parameters, applied in gradient descent, Newton's method, and least squares optimization.\n",
      "-----\n",
      "\n",
      "fe9fc46c-8298-45a7-9f27-d62d9c50268a\n",
      "The indicator vector for a categorical variable has components that sum to one, indicating exactly one outcome occurs per realization.\n",
      "-----\n",
      "\n",
      "f0205d29-3342-4f91-97b9-fd8f4d253346\n",
      "Defines the general form of a probability distribution in the exponential family, expressing the probability of a variable given parameters as the product of a base measure and an exponential function of the parameters, sufficient statistics, and a log-partition function.\n",
      "-----\n",
      "\n",
      "a6d3ab0e-ac1a-4b56-b42f-fa5edecafa6b\n",
      "Defines a feature mapping for a two-dimensional input vector, producing a six-component vector: constant term, two input variables, their product, and the squares of each input variable. Represents a degree-two polynomial feature mapping for two variables.\n",
      "-----\n",
      "\n",
      "814d4816-75cb-44fc-b398-61ef429e9fbc\n",
      "Gradient of the error function with respect to the weight vector for class k, where the error function depends on the weight matrix and the dataset. Computed as a sum over all N examples: for each example, the difference between the model's predicted value for class k and the true label for class k is multiplied by the example's feature vector.\n",
      "-----\n",
      "\n",
      "8dfbf708-1514-4083-a410-acb48752b580\n",
      "Hessian matrix that is positive semi-definite but not positive definite, affecting invertibility required for Newton's optimization method.\n",
      "-----\n",
      "\n",
      "922c0663-b652-40d9-ba09-22386b37cce4\n",
      "Gradient of the regularized error function with respect to the parameter vector equals the sum of the gradient of the original error function and a term proportional to the parameter vector, scaled by the regularization factor.\n",
      "-----\n",
      "\n",
      "21ea1f3e-0d8e-435c-a11d-f6208bccabea\n",
      "The target expression represents a set of models indexed by k, each corresponding to a specific class in multinomial logistic regression, where each model outputs the probability that an input belongs to its associated class.\n",
      "-----\n",
      "\n",
      "f1158a74-22e2-4823-9243-dd2fde7802cc\n",
      "Hessian matrix in logistic regression cross entropy optimization, represented as the product of the transpose of the feature matrix, a diagonal matrix, and the feature matrix in the weights update rule.\n",
      "-----\n",
      "\n",
      "dabe518c-6af9-4bce-b0d9-c65fa9afe79f\n",
      "Function of a vector variable evaluated at a vector input in a multidimensional parameter space, referenced in optimization methods involving convexity and quadratic approximation, and used as an example of a function applicable to these methods.\n",
      "-----\n",
      "\n",
      "c77e9401-e8bc-4bd3-8b59-7e53c033f23c\n",
      "The target expression denotes individual basis functions, indexed by j, used to map input variables to features in a generalized linear model. These functions may be any nonlinear transformations of the input variables and are not limited to polynomials or specific forms.\n",
      "-----\n",
      "\n",
      "32c67ef0-fa57-49c7-ad6a-0e490d165277\n",
      "Defines a function of an input vector and parameter vector: applies a linear transformation to the input with a weight matrix, applies a nonlinear activation function elementwise, forms a weighted sum with a second set of weights, and applies the same nonlinear activation to this sum. Each basis function is a nonlinear transformation of a linear combination of the input. The overall function composes these basis functions with another linear combination and nonlinearity.\n",
      "-----\n",
      "\n",
      "9745e002-9668-465f-abb2-89f237650fb2\n",
      "Set of four expressions specifying the linear regression model in generalized linear models: (1) model output is a linear function of a feature transformation of the input with identity activation; (2) conditional probability of the target given input and weights is a normal distribution with mean equal to model output and fixed variance; (3) loss is the squared difference between model output and target; (4) gradient of the loss with respect to weights is the product of prediction error and feature vector.\n",
      "-----\n",
      "\n",
      "269bb1b6-23db-470a-89b0-c4a18cbbce4f\n",
      "Set of four expressions for multinomial logistic regression: (1) k-th class probability as softmax of linear transformation of input features, representing probability of class k given input and weights; (2) probability of a one-hot encoded label vector as product over classes of predicted probabilities raised to corresponding label entries; (3) loss function as negative sum over classes of label entries times logarithm of predicted probabilities; (4) gradient of loss with respect to k-th class weight vector as difference between predicted probability and label entry, times feature vector.\n",
      "-----\n",
      "\n",
      "82cd0d72-d9c4-435c-96c2-fd635109242f\n",
      "Index denoting a specific example in a dataset, typically ranging from 1 to N, where N is the total number of examples.\n",
      "-----\n",
      "\n",
      "b2f39b9a-1ffe-48cf-b54b-cc9093d5337c\n",
      "The parameter vector contains elements representing category probabilities that sum to one.\n",
      "-----\n",
      "\n",
      "522e9c2d-0988-454b-a4c5-74992f062175\n",
      "Right arrow symbol indicating the start of an example section after the explanation of the softmax function.\n",
      "-----\n",
      "\n",
      "805260f2-1861-4b65-8d61-0e3e223074a5\n",
      "total number of classes in the generalization of the cross-entropy error and loss function\n",
      "-----\n",
      "\n",
      "a49403b1-7863-4187-baef-2c234c4ffbfb\n",
      "Regularization in logistic regression using the sum of the absolute values of model parameters as the penalty term.\n",
      "-----\n",
      "\n",
      "79d40230-d171-4c8a-afeb-745f0ba541ab\n",
      "Function mapping an n-dimensional real vector to an n-dimensional real vector.\n",
      "-----\n",
      "\n",
      "cca9b94d-0688-4e48-9907-1700c34c8362\n",
      "General function of a vector variable serving as the objective to be minimized in the described optimization context.\n",
      "-----\n",
      "\n",
      "cdb17f97-8cbe-45dd-b331-c1016cf1a590\n",
      "Five-dimensional vector with components 1, 2, 0.5, 5, and 3, used as input for the softmax function.\n",
      "-----\n",
      "\n",
      "7a7cf1dc-8f53-47e4-b670-e74d56cd2a91\n",
      "Squared L2 norm regularization term added to the loss function or gradient in regularized logistic regression to penalize large weights and prevent overfitting.\n",
      "-----\n",
      "\n",
      "fdd2be9b-8e84-4033-8eb9-bca0bdcb6d20\n",
      "A type of regularization contrasted with another method in gradient descent, Newton's method, and least squares optimization for linear regression; described as straightforward to incorporate into these optimization methods.\n",
      "-----\n",
      "\n",
      "30aeb71a-0a0b-448b-9d25-6de4d348f2ba\n",
      "Parameter set to one, indicating full step size in Newton's method parameter update when Hessian matrix is computed exactly.\n",
      "-----\n",
      "\n",
      "19fd0e54-57f6-43aa-99ef-ce2f4efa6b9e\n",
      "Number of dimensions of the input vector for a function mapping vectors to scalars, as related to the Hessian matrix and the function's domain.\n",
      "-----\n",
      "\n",
      "dbaba03e-0946-444b-999b-b3372b501355\n",
      "The target expression defines a two-dimensional input space, with each input vector containing two components.\n",
      "-----\n",
      "\n",
      "7cf5d5fe-51ce-46af-83cf-b913f6107a7e\n",
      "Target expression indicates a specific class label, indexed by a variable, in a multi-class classification problem.\n",
      "-----\n",
      "\n",
      "091a9431-0ef7-4821-a6d9-821005fd6b3c\n",
      "Index identifying a specific class in multiclass classification, used to reference or update that class's weights or outputs.\n",
      "-----\n",
      "\n",
      "4cc25d05-c6bf-4c68-8549-abada1720941\n",
      "Variable representing a non-zero vector used to test positive definiteness of the Hessian matrix, where the quadratic form with this vector is strictly greater than zero.\n",
      "-----\n",
      "\n",
      "290f6caf-c614-4f62-a843-b8f7d4b17868\n",
      "Scalar product of the k-th class weight vector and input vector, serving as input to the activation function in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "794401da-cca3-438d-8706-07128cc5d517\n",
      "Weight parameter at the upper-left position of the parameter vector, excluded from L2 regularization.\n",
      "-----\n",
      "\n",
      "a2074584-10a1-4702-b24b-3fce21094d57\n",
      "Taylor series expansion of a differentiable function about a point, including explicit sum of terms with derivatives at that point and equivalent infinite series summation notation.\n",
      "-----\n",
      "\n",
      "a3d59bdd-4230-4ac2-b19e-b66647a06038\n",
      "Defines a generalized linear model function that takes an input vector and weight vector, applies a feature mapping to the input, computes the weighted sum of mapped features, and applies an activation function to this sum. Equivalent formulations include applying the activation function to the dot product of the weight vector and mapped feature vector, or to the sum over all weights multiplied by their corresponding basis functions evaluated at the input.\n",
      "-----\n",
      "\n",
      "b85d9a46-578b-4528-94b1-1d1d5a0feb7c\n",
      "L2 norm used as a regularization technique in logistic regression and optimization, referring to the addition of an L2 regularization term to the loss function or gradient.\n",
      "-----\n",
      "\n",
      "e2b75b01-d9ca-4a14-b2fd-4a0a0f0f9de9\n",
      "Activation function applied to the scalar product of the weight vector and feature vector in a generalized linear model.\n",
      "-----\n",
      "\n",
      "b0ba148d-c40f-4c79-9d44-2ba597c8bbaa\n",
      "Gradient of the quadratic error function for linear regression with respect to the weight vector for a single data example, equal to the product of the difference between predicted and true output for the i-th example and the corresponding feature vector.\n",
      "-----\n",
      "\n",
      "4e5e2e26-0ef0-43c1-8ee5-6783c6ec2405\n",
      "Dataset of input-output pairs with inputs as feature vectors and outputs as corresponding labels, used for defining probability and error functions in a classification model.\n",
      "-----\n",
      "\n",
      "90d66d2f-b61c-4c45-ac82-0fd6d122c5fd\n",
      "Function mapping an n-dimensional real vector to an (m+1)-dimensional real vector, with output vector composed of scalar 1 followed by m basis function values evaluated at the input vector.\n",
      "-----\n",
      "\n",
      "8e243264-f95d-4141-aa98-5b2ff5ad619a\n",
      "L1 regularization uses the sum of the absolute values of model parameters, is associated with the L1 norm, and is distinct from L2 regularization.\n",
      "-----\n",
      "\n",
      "38f87889-b427-44f6-901c-e3a30e641f99\n",
      "The k-th component of the indicator vector for a categorical (multinoulli) variable equals one if and only if the outcome is the k-th category.\n",
      "-----\n",
      "\n",
      "b948cc6e-99aa-4d6b-8ea9-2d05fd2423e5\n",
      "Model for class k outputs the probability that an input belongs to class k, defined by the softmax function.\n",
      "-----\n",
      "\n",
      "d3daed8e-1cb6-46e2-a73d-c7e8e2c9c96b\n",
      "The update rule computes the next parameter vector by subtracting the product of the step size, the inverse Hessian at the current step, and the gradient at the current parameter vector from the current parameter vector.\n",
      "-----\n",
      "\n",
      "ca38986e-11ba-438f-a984-c2cd6e21bff2\n",
      "The target expression refers to the multinomial logistic regression model, defined as a set of class-indexed models that output the probability of an input belonging to each class via the softmax function.\n",
      "-----\n",
      "\n",
      "76f4b7cb-db17-4325-b240-00b1e2af08e5\n",
      "Error function for multiclass classification: negative sum over all training examples and classes of the product of the true label indicator for each class and the logarithm of the predicted probability for that class, given model parameters and input data.\n",
      "-----\n",
      "\n",
      "0c6f64f8-580f-435a-8048-c26d481cfd78\n",
      "Number of classes, equal to the dimension of the vector output by the softmax function applied to class scores.\n",
      "-----\n",
      "\n",
      "a601c704-edc5-4d64-80fd-d165ba3de124\n",
      "Four-dimensional indicator vector for a multinomial variable with the third component as one and all others zero, indicating selection of the third category among four possible categories.\n",
      "-----\n",
      "\n",
      "290b1886-a80b-4736-ad17-f2e44adf9814\n",
      "Index indicating the position of a weight in the weight vector for a specific basis function.\n",
      "-----\n",
      "\n",
      "5d3ea627-1c51-4d14-8128-84ef42c1f3f0\n",
      "Defines the probability that an input example belongs to a class in multinomial logistic regression as the softmax of a linear transformation of the input: the numerator is the exponential of the dot product between the class weight vector and the input feature mapping; the denominator is the sum of such exponentials over all classes. This equals the conditional probability of the class label given the input and weight vectors.\n",
      "-----\n",
      "\n",
      "91da4cb5-8eab-44d2-8879-991f96d5f727\n",
      "Defines the probability that an input vector belongs to class k, given the input and model parameters, as the k-th output of a model computed by exponentiating a linear function of the input (parameterized by the weight vector for class k) and normalizing by the sum of exponentiated linear functions over all classes; this is the softmax function used in multiclass classification models.\n",
      "-----\n",
      "\n",
      "518298ff-5719-4b64-8054-4945c2e690bf\n",
      "Diagonal matrix with entries equal to the product of the logistic function evaluated at each training example and one minus that value, for all dataset examples.\n",
      "-----\n",
      "\n",
      "78a83656-0d88-4b33-bae7-998e21e57ada\n",
      "Set of four equations defining logistic regression: (1) model output as sigmoid of linear input features, representing probability of positive class given input and weights; (2) label probability given input and weights as Bernoulli distribution parameterized by model output; (3) loss function as negative log-likelihood (cross-entropy) for binary label; (4) gradient of loss with respect to weights as difference between model output and label, multiplied by feature vector.\n",
      "-----\n",
      "\n",
      "9f6d1d24-7def-4856-b308-824816a2fd5b\n",
      "Index of a class in a classification problem with total classes denoted by an uppercase variable; model provides the probability that an example belongs to this class.\n",
      "-----\n",
      "\n",
      "be08a562-0456-47d4-b2bb-e23167c3ee42\n",
      "A diagonal matrix with diagonal entries equal to the regularization factor, except the upper-left element, which may differ because the corresponding weight is not regularized.\n",
      "-----\n",
      "\n",
      "5254db3f-5eea-4d9e-a71a-2ff51c82f021\n",
      "Weight parameter for the i-th input feature and j-th basis function in the inner layer of a generalized linear model with basis functions, where each basis function has a distinct set of weights.\n",
      "-----\n",
      "\n",
      "a890f4b4-7c50-42bc-94e2-80d286bbb6ce\n",
      "Function with subscript j mapping n-dimensional real vectors to real numbers; represents the j-th basis function among m basis functions for feature mapping in generalized linear models.\n",
      "-----\n",
      "\n",
      "24c4f5e6-c8f3-44ee-985d-9995df215303\n",
      "The Hessian matrix of a scalar-valued function of n variables is a square matrix with entries given by the second-order partial derivatives of the function with respect to each pair of variables; the entry in the i-th row and j-th column is the second partial derivative with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "713bcf6b-ccfd-4675-abe4-0fe0be1aced1\n",
      "The target expression represents the model for class k in multinomial logistic regression, outputting the probability that an input belongs to class k.\n",
      "-----\n",
      "\n",
      "9a210d5a-5fae-4ca5-9f1f-8bbd43180bbc\n",
      "Defines the loss function for a single example in K-class classification as the negative sum over all classes of the true label indicator multiplied by the logarithm of the model's predicted probability for each class, evaluated at the input and model parameters.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in math_expression_descriptions_opt:\n",
    "    print(x.math_expression_description_id)\n",
    "    print(x.text)\n",
    "    print('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72734a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import unzip\n",
    "\n",
    "from math_rag.application.models.embedders import EmbedderInput\n",
    "\n",
    "\n",
    "inputs, input_id_to_item = InputCreatorUtil.create(\n",
    "    math_expression_descriptions_opt, lambda x: EmbedderInput(text=x.text)\n",
    ")\n",
    "outputs = await default_embedder.concurrent_embed(inputs)\n",
    "descriptions, embeddings = unzip(\n",
    "    (input_id_to_item[output.input_id], output.embedding) for output in outputs\n",
    ")\n",
    "descriptions, embeddings = list(descriptions), list(embeddings)\n",
    "await math_expression_description_opt_repository.insert_many(descriptions)\n",
    "await math_expression_description_opt_embedding_repository.upsert_many(descriptions, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dd1ea",
   "metadata": {},
   "source": [
    "### 5. MathExpressionGroup, requires: MathExpressionDescription, MathExpressionContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88992ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_descriptions = await math_expression_description_opt_embedding_repository.group(\n",
    "    grouper_service.group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80ad33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import Record\n",
    "\n",
    "\n",
    "grouped_records: list[list[Record]] = []\n",
    "\n",
    "for descriptions in grouped_descriptions:\n",
    "    ids = [x.id for x in descriptions]\n",
    "    records = await math_expression_description_opt_embedding_repository.client.retrieve(\n",
    "        collection_name=math_expression_description_opt_embedding_repository.collection_name,\n",
    "        ids=[str(id) for id in ids],\n",
    "        with_payload=True,\n",
    "        with_vectors=True,\n",
    "    )\n",
    "\n",
    "    for record in records:\n",
    "        # remove some data for a clener diagram\n",
    "        record.payload['text'] = record.payload['text'][:50]\n",
    "        record.payload.pop('math_expression_description_id')\n",
    "        record.payload.pop('math_expression_index_id')\n",
    "        record.payload.pop('timestamp')\n",
    "\n",
    "    grouped_records.append(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56eb769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "os.environ['NUMBA_CPU_FEATURES'] = str()  # avoid kernel crash on arm\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b2d88",
   "metadata": {},
   "source": [
    "#### Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ac99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    centers=5,\n",
    "    n_features=10,\n",
    "    cluster_std=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    metric='euclidean',\n",
    "    random_state=None,\n",
    ")\n",
    "X_umap = reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cfb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": {
          "bdata": "BAQDAQIBAAQAAwQDBAACAQQDAwAEAgMBAAACAAQDAgEEBAICAAAABAEAAwIEBAMEAgACBAMBAQEBAgQBAAABAQAEAQIBAwMDAQQCAwMDBAMBAgQCAAICAwABBAQEAgABAAQDBAQCAwACAQMAAAQEBAIEAAQDAgIEBAQCAgIEAAIAAgMBAQMDAQMABAECAAECAAICAgIDAgQEAwADAAEDAAQCAQQCBAABAAEEAgMEAAQCAQADBAAAAAQEAgMBAQMDAAMDAAMAAQADBAQCAgADBAMEAQQCBAIEAAACAgQABAMEAAECBAMCAwMAAAQDAQEBAAIEAgEEBAMABAICAAICBAMDAgEEAQQAAAMBAQMEAQQCAQMBAwMBBAAAAAIDAgEEAQAABAEAAQMAAAADAwEDAAIDAwECBAAAAAICAAACAgEAAwAEAwIBAAEDAwECAwIBAQQBAgIDAAMEAAMCAAIEAgQCAQEDAwIBAQIBAwMCAQADAwMCAQMBAAMCAQEBAQMEAAQBAwIAAAQCAQEDAwEBAgADAQIDAAECAAIBAwECAQQAAAQBAQMEBAIDBAQEBAECAwIBAgIBAQQCAQACBAEDAAMBBAQAAAQDBAIEAAABAwEAAQQDAAIBAAABAgEAAwMDBAECAQQCAwICAwIAAAQDAwAEAQA=",
          "dtype": "i1",
          "shape": "500, 1"
         },
         "hovertemplate": "UMAP1=%{x}<br>UMAP2=%{y}<br>cluster=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "BAQDAQIBAAQAAwQDBAACAQQDAwAEAgMBAAACAAQDAgEEBAICAAAABAEAAwIEBAMEAgACBAMBAQEBAgQBAAABAQAEAQIBAwMDAQQCAwMDBAMBAgQCAAICAwABBAQEAgABAAQDBAQCAwACAQMAAAQEBAIEAAQDAgIEBAQCAgIEAAIAAgMBAQMDAQMABAECAAECAAICAgIDAgQEAwADAAEDAAQCAQQCBAABAAEEAgMEAAQCAQADBAAAAAQEAgMBAQMDAAMDAAMAAQADBAQCAgADBAMEAQQCBAIEAAACAgQABAMEAAECBAMCAwMAAAQDAQEBAAIEAgEEBAMABAICAAICBAMDAgEEAQQAAAMBAQMEAQQCAQMBAwMBBAAAAAIDAgEEAQAABAEAAQMAAAADAwEDAAIDAwECBAAAAAICAAACAgEAAwAEAwIBAAEDAwECAwIBAQQBAgIDAAMEAAMCAAIEAgQCAQEDAwIBAQIBAwMCAQADAwMCAQMBAAMCAQEBAQMEAAQBAwIAAAQCAQEDAwEBAgADAQIDAAECAAIBAwECAQQAAAQBAQMEBAIDBAQEBAECAwIBAgIBAQQCAQACBAEDAAMBBAQAAAQDBAIEAAABAwEAAQQDAAIBAAABAgEAAwMDBAECAQQCAwICAwIAAAQDAwAEAQA=",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "f4"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f4"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "UMAP Projection of 5 Random Clusters"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP1"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'UMAP1': X_umap[:, 0],\n",
    "        'UMAP2': X_umap[:, 1],\n",
    "        'cluster': y,\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='cluster',\n",
    "    hover_data=['cluster'],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff01fd",
   "metadata": {},
   "source": [
    "#### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51f69ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [r for grp in grouped_records for r in grp]\n",
    "vectors = [r.vector for r in records]\n",
    "cluster_labels = [i for i, grp in enumerate(grouped_records) for _ in grp]\n",
    "\n",
    "# figure out which payload keys exist across all records\n",
    "payload_keys = set().union(*(r.payload.keys() for r in records))\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    metric='cosine',\n",
    "    random_state=None,\n",
    ")\n",
    "X_umap = reducer.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00c622bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "L1 regularization uses the sum of the absolute val",
           "0d3764d1-5a53-4ad8-abd4-18d3ee4e6a35",
           "022cec21-9bcb-417c-8454-a3294db80000",
           0
          ],
          [
           "Regularization technique penalizing the sum of the",
           "20f48658-4097-4962-bd0b-e5cd527545d1",
           "153f610b-022d-4b73-9682-442f3a8f6173",
           0
          ],
          [
           "A type of regularization contrasted with another m",
           "4dd73954-81d8-49ba-bf2a-10a76d05c9f9",
           "20bccb0c-9644-4b8d-8abc-6b21032f0612",
           0
          ],
          [
           "Regularization in logistic regression using the su",
           "4167c11e-d00f-438c-b008-5c240b4a3e16",
           "6375b67e-5825-44ac-ac80-762718d1501d",
           0
          ],
          [
           "A regularization technique in optimization and mac",
           "c6963fb9-e146-4d19-8379-15462babcf70",
           "80f30733-0070-405d-a08c-80ec3b10b9f7",
           0
          ],
          [
           "L1 regularization norm, non-differentiable, requir",
           "bbd8a740-aeb9-445a-b9f6-93521fb9298f",
           "85d670e9-2881-492f-a8d3-bf4fd1766098",
           0
          ],
          [
           "L2 norm used as a regularization technique in logi",
           "5ccfc3db-3551-4cc2-88ff-9ef737c4f245",
           "e5de7dd8-faab-4240-835b-e0bd16892de4",
           0
          ],
          [
           "Squared L2 norm regularization term added to the l",
           "a607c8b5-621a-4486-bc54-452cfdc5bb0f",
           "f1793eff-7df7-4045-af54-250d61d4ba27",
           0
          ],
          [
           "Each parameter in the parameter vector, indexed by",
           "ccd7a72c-4dba-45cf-95ae-6e923686ce27",
           "4dd7d644-9319-4382-9423-8d36117c6653",
           1
          ],
          [
           "The parameter vector contains elements representin",
           "66d76a0e-dec1-4e10-b174-90695768ce03",
           "ca2f7539-8890-4197-a412-528d997c57c1",
           1
          ],
          [
           "The target expression is the function approximated",
           "b2c69de8-4363-493e-b974-850e086c5a0b",
           "1188fa7c-63e3-4b0e-8815-71e91ee0ed4a",
           2
          ],
          [
           "Function minimized, approximated quadratically at ",
           "bbcc2c89-208a-41f0-9f65-1221a5bbedaa",
           "1391d9b4-906b-49b0-8918-90779869073f",
           2
          ],
          [
           "Function minimized in optimization, locally approx",
           "efb7321d-8ae6-4d4a-8729-3ae2a36997f2",
           "150645cb-e2a9-4f64-a9fd-be9826f6a709",
           2
          ],
          [
           "Initial point for starting minimization of a singl",
           "78752d0a-4583-4f17-83d9-ae5e0bd7eff0",
           "2b69ebdf-a619-42d7-aae8-0e8d7ca3afe9",
           2
          ],
          [
           "Point reached after the first update in an iterati",
           "b3c61b85-5bd7-4202-99bd-1519dcb15984",
           "2c1c605a-b21d-4202-8e15-eee4fbc8c8ec",
           2
          ],
          [
           "Quadratic function approximating the original func",
           "d3a036be-c870-4801-9c89-3fdc24b14a54",
           "3bd9ce75-bb4c-45e2-9648-3a1c1225ba0f",
           2
          ],
          [
           "Function minimized using Newton's method, evaluate",
           "7257213b-65d3-45e6-a69b-30e2f7586662",
           "4f1a23b7-c900-46c5-9a27-00ffc60e0926",
           2
          ],
          [
           "Black curve representing a single-variable functio",
           "d354b05a-be9e-4753-b72e-24bda4a6d4d8",
           "5023981d-8720-49a1-b55d-d793f70eb10d",
           2
          ],
          [
           "Function minimized by quadratic approximation at e",
           "b3f723b1-72a2-4c51-a538-09a2608bbf25",
           "a0e192a8-41cf-4aa9-8ae3-71943f6ce58f",
           2
          ],
          [
           "Point where the quadratic approximation of a funct",
           "d118ab17-ae17-4c87-87db-8e7c1150975a",
           "d180b86f-630d-437b-b57a-7eed5f80d466",
           2
          ],
          [
           "Parameter update rule in gradient descent: the par",
           "22cc79a2-bccd-432c-92b2-6046877e949f",
           "ae84c9e2-6635-46ca-99a9-6f5265f79b36",
           3
          ],
          [
           "The update rule computes the next parameter vector",
           "6beaaecf-dd28-4988-85be-f664ca5a9d05",
           "bfbf1885-f01b-4d35-b157-ce14d3d50e9c",
           3
          ],
          [
           "Iterative update rule for gradient descent: the pa",
           "8b29aaa4-0045-4ce8-917e-d6b3c76f6832",
           "f1ba93b9-8a8f-4c21-b378-3582e9a07085",
           3
          ],
          [
           "Weight update rule for stochastic gradient descent",
           "46b28bcd-0afb-41a0-a610-2cfbb3b57e37",
           "cc8525a8-f3e2-4fe2-ac0a-0c449e755c3c",
           4
          ],
          [
           "Gradient of the quadratic error function for linea",
           "9c00942e-79f2-4f89-921c-bbf6ca186492",
           "f65fe532-c292-4fb1-9339-153f9d5740b7",
           4
          ],
          [
           "Diagonal matrix with entries equal to the product ",
           "e111b44a-58ab-472c-820d-ded943d3dfec",
           "3b0de53f-30ee-4e16-a22e-21e73d2f19d3",
           5
          ],
          [
           "Regularization factor applied to the diagonal elem",
           "4ad0d832-3f0a-4893-84cb-48aa59b9cd8c",
           "713d7528-e0fd-4a05-a1d1-c60888f38ad2",
           5
          ],
          [
           "A diagonal matrix with diagonal entries equal to t",
           "ac655c66-7e7f-4b79-a689-2e727af063cb",
           "ff277387-fa8e-43e6-b6ef-d370d20b5f62",
           5
          ],
          [
           "Index indicating the position of a weight in the w",
           "62b038c9-de0d-44b7-b008-7f3b47a6c9e8",
           "028c81db-abda-4d5a-aa8f-5144530afb89",
           6
          ],
          [
           "Index identifying a specific basis function within",
           "6827de4a-a1d2-410f-8f14-4c17942c2bc8",
           "128f1571-02a4-4a92-b4bd-99f65e039f05",
           6
          ],
          [
           "Index identifying a specific basis function in the",
           "831caebe-f18e-44a4-8b61-e41afb65c6f3",
           "c81d2fa3-212b-4f60-822e-2967bf43f743",
           6
          ],
          [
           "Second-order Taylor expansion of a multivariate fu",
           "337e34e5-fa76-4cf1-961e-f656f80720d0",
           "53d4fc7c-9744-4ee4-90b2-a86ef79d7a1b",
           7
          ],
          [
           "Function evaluated at x, approximated by a Taylor ",
           "a6d63c97-fa55-4a11-8b45-ddf79497dc70",
           "c20d5bd8-b5dc-453e-af0b-0699ea2312b9",
           7
          ],
          [
           "Taylor series expansion of a differentiable functi",
           "835c8757-7e6d-4690-a21e-d4f0d5b4aa4c",
           "c23daa85-9b8d-4113-9b32-9690bf0647d4",
           7
          ],
          [
           "point where a function is expanded in its Taylor s",
           "e2a792d8-48e7-4284-adf3-4a8d7d310675",
           "dc6c3f13-7f1d-4ff9-a3b5-c2400c28589a",
           7
          ],
          [
           "Error function for multiclass classification: nega",
           "4cf2d682-7485-4b03-a8aa-c1a1cebe4031",
           "c24b5d17-b16a-45a0-8bb9-febfd84a7216",
           8
          ],
          [
           "Log-likelihood of observed labels given input data",
           "db9cb66b-d857-4bca-8dc2-737abbcdeb0c",
           "e7ef545d-d51f-4bf4-b3b6-fc45d32c12f2",
           8
          ],
          [
           "Defines the loss function for a single example in ",
           "f8814e86-5a10-4b26-b6ce-1ea7d145175c",
           "fb48e310-f52b-4597-a86e-1fa62cca1ca5",
           8
          ],
          [
           "Parameter representing the probability of success ",
           "fd293a59-9aea-4ae4-bacb-0fb76bb91195",
           "2b083c86-c834-4f5e-a425-0e09abcf8ed8",
           9
          ],
          [
           "Probability mass function of a Bernoulli random va",
           "4c6e1e04-4f80-4e03-9b9c-075629d820f0",
           "372145b3-dc0b-4acd-a002-bab3fea62809",
           9
          ],
          [
           "Value representing the first possible outcome of a",
           "c97b3c8f-475b-43d1-a5a4-92704dca867a",
           "d4156410-fb14-494a-b19e-ffb41a9fc798",
           9
          ],
          [
           "Index identifying a specific class among possible ",
           "7837e1ce-6af1-4d04-81c5-6e6f39fcdbbc",
           "2d3cae15-ad5d-4f3a-9342-e2de5e71fffb",
           10
          ],
          [
           "Index identifying a specific class among multiple ",
           "59330188-b6a2-4fdd-bed7-e9b71ba783d0",
           "3fe8444d-4b22-4ed2-9b09-53afccacee86",
           10
          ],
          [
           "Index identifying a specific class in a multi-clas",
           "807ca308-12de-4953-84ee-3de7c417b315",
           "c436f5e8-3e68-45c0-8d90-09143dd96ba6",
           10
          ],
          [
           "Index identifying a specific class in multiclass c",
           "14fd6bb3-9245-4446-8b11-fc4f27a594e5",
           "f71db549-20d3-44f8-b6eb-9ba5b1b94e21",
           10
          ],
          [
           "Vector of weights used to combine outputs of basis",
           "0fa1d9ef-a7ce-4f80-9da7-9fe09f3a3f56",
           "18817f2d-386e-4f74-9359-a8c4d8973ea6",
           11
          ],
          [
           "Weight matrix for the first model layer; each basi",
           "69838442-4051-4a20-a33f-852f8ed714b8",
           "d7f08f97-5551-4745-8d98-c873bc7f4b14",
           11
          ],
          [
           "For every non-zero vector, the quadratic form of t",
           "cf55bb12-5e0b-4ed4-be3e-759b4917afb3",
           "2d539c70-fde6-4ca4-8788-02e1e22d1c8e",
           12
          ],
          [
           "Variable representing a non-zero vector used to te",
           "e6ce2cd5-58a0-4e52-bf9f-6dd8372e35e1",
           "85a0799e-f5f0-4e46-bdf3-89e04a519da9",
           12
          ],
          [
           "For every non-zero vector, the quadratic form invo",
           "1d93b4a7-62b0-422a-951e-89af34c0c98e",
           "ce202d52-922d-43c5-876d-af6e6ba79439",
           12
          ],
          [
           "Function of a vector variable evaluated at a vecto",
           "2476b01b-af62-40d5-a1cd-f9e516d03cfb",
           "daa9f268-0933-492b-ab10-ad7081dd4b5a",
           12
          ],
          [
           "Symbol denoting a non-zero vector used to test the",
           "290109b9-696e-45f0-8c50-8709e8b5bec7",
           "ec7d8772-1a73-428c-95cd-8142fd58e2a3",
           12
          ],
          [
           "Function of a vector variable; convexity determine",
           "e062f305-0abc-445d-a95c-2adf6560ce62",
           "ef8ecc58-dfd4-4c6c-a4b1-9941cd38259d",
           12
          ],
          [
           "Hessian matrix of a vector-valued function evaluat",
           "e5d9cae1-2143-40fe-a43e-f1e5efcce360",
           "02eecc6e-bb74-463f-8d86-f3499502a655",
           13
          ],
          [
           "Point in n-dimensional space where function f is e",
           "6b77bf2f-3926-41b1-bd8c-9c15a71c000b",
           "919a63dd-23fa-4bff-8e54-ba9e7cdb2ce1",
           13
          ],
          [
           "A set of m basis functions, each a nonlinear funct",
           "fc682491-4c51-47ae-8778-edbc9b8617ad",
           "38ff9cb5-1d3c-417c-ae6b-1119f435f1fa",
           14
          ],
          [
           "Function with subscript j mapping n-dimensional re",
           "cbfcae35-69df-46f4-bdca-32a27be7f42a",
           "d47dced6-960d-409a-9ee8-babd96afe08a",
           14
          ],
          [
           "Scalar product of the k-th class weight vector and",
           "56b70ca9-d561-4a9b-a98f-11c03bd43997",
           "3d888f09-3b57-447b-a30f-546c6a2a8e9f",
           15
          ],
          [
           "Weight vector for the k-th class in multinomial lo",
           "b1674729-2acd-4d61-9816-235c3e7c724f",
           "775052ea-fefd-4464-bc62-40fdb663e1dd",
           15
          ],
          [
           "The dot product of the weight vector for class k a",
           "b725f8bd-f5ab-4c52-b7e2-de3fbfcf1651",
           "7fdede2a-199e-4c79-8b60-8af56fdffbb4",
           15
          ],
          [
           "The k-th weight vector in softmax regression, a co",
           "175bd24b-d78a-48ff-b2bd-d59e8d5e06be",
           "ea4f6a47-08ca-4397-a290-541924c281f7",
           15
          ],
          [
           "Hessian matrix in Newton's optimization method det",
           "6ff3b96b-6943-4473-b922-f0e05a62fa29",
           "57b216be-43f4-4ce8-b019-d254bfef9c7e",
           16
          ],
          [
           "Hessian matrix that is positive semi-definite but ",
           "c9d816d2-871d-41bf-b58f-396d77968950",
           "6ece0b33-af95-4162-8e25-dad76feb5ba5",
           16
          ],
          [
           "Hessian matrix that is positive semi-definite but ",
           "c37189bf-9c75-41f9-898e-556a97b38bfc",
           "9fb8626b-7318-4aad-a3b1-bb5b5545caa7",
           16
          ],
          [
           "Hessian matrix in Newton's optimization method, re",
           "ecf4131f-fce8-42df-8716-01752704c98a",
           "a60bc4ca-5801-4e9e-b133-d3d621231999",
           16
          ],
          [
           "The number of weight vectors in the matrix equals ",
           "fd7b587e-fa05-4ac3-8d82-1664bd40fd97",
           "2de553ad-3ad4-4368-bc68-41b24ead379c",
           17
          ],
          [
           "Column vector with K elements, each representing a",
           "6fd53e2a-0ae6-47f1-a7d3-275f8cf25a24",
           "9507e4c8-8bd5-4b78-a3e0-87f09897c8b8",
           17
          ],
          [
           "Number of classes, equal to the dimension of the v",
           "5b3a9e2e-eb07-4e6c-bdef-72eb190789ff",
           "9bad377f-b203-4e6b-904f-6a14c772e254",
           17
          ],
          [
           "A matrix of K column vectors, each representing a ",
           "687d2a36-e215-4a66-80b8-55479d2d4bda",
           "cd7c0953-979f-4875-ad63-c6a869aa6f0d",
           17
          ],
          [
           "Total number of classes in a classification proble",
           "522bd5f5-52aa-4239-a491-65a69641a21f",
           "d3394895-4237-4aa2-97f9-0469ed3011d6",
           17
          ],
          [
           "Total number of classes in a multinomial logistic ",
           "fda9707f-6d32-4975-a038-ffc704b5e3f4",
           "eb7564eb-068e-43f4-b94e-eac0b7a501b9",
           17
          ],
          [
           "total number of classes in multinomial logistic re",
           "9c41569f-3ec3-4077-bf92-03767c7de0a8",
           "f437c2f4-610c-45d9-8f73-f65dad6226e2",
           17
          ],
          [
           "Index of a categorical variable's outcome; the k-t",
           "8554eabd-a4ae-4ff7-8341-bbf35c5c1abf",
           "0e26b156-1bef-4e3b-a6bd-02e2c0e27b77",
           18
          ],
          [
           "The k-th component of the indicator vector for a c",
           "541e1285-1bf4-4cf1-b805-e4df7dfc9ea9",
           "178fca1d-d383-411d-9465-f3bf62549772",
           18
          ],
          [
           "The k-th indicator variable in the outcome vector ",
           "4c1d315f-a52f-454e-8f52-df21af0dcdf5",
           "1fe84871-7155-4f6f-bcce-c502ec7e09ca",
           18
          ],
          [
           "Probability that the k-th indicator variable in a ",
           "a4b691f9-d2ae-4057-ba28-328e49929e16",
           "59ac569a-eecd-4201-ab8e-60bc4fbab5fb",
           18
          ],
          [
           "The indicator vector for a categorical variable ha",
           "2ec54799-905b-4654-be12-552f1b4f14d0",
           "a7951402-4007-45ef-9e10-6140697656e5",
           18
          ],
          [
           "Probability that the k-th indicator variable equal",
           "4b012232-7190-4a6f-b072-5a4614b5d0d0",
           "fcdd415d-a3d2-4280-bcfc-95330e7e191b",
           18
          ],
          [
           "Function mapping an n-dimensional real vector to a",
           "539044c5-30b2-418f-8aec-bd600459d05d",
           "2e5bf442-dc92-463d-9196-51c20b208187",
           19
          ],
          [
           "Function mapping an n-dimensional real vector to a",
           "27ed1088-a1bd-4394-8d17-abccae5d38a6",
           "3cd5d712-3606-44dc-a8a6-bcb5425141dc",
           19
          ],
          [
           "Function mapping an n-dimensional real vector to a",
           "48a813db-d237-4ebd-ab28-f731039f94c4",
           "84a474e3-28c0-42b9-b400-3ddbd76bbf53",
           19
          ],
          [
           "Hessian matrix in logistic regression cross entrop",
           "b6b3c09b-e061-4a3a-9a9f-5652a7de03f0",
           "08e77b3a-ca4b-4c10-82c6-402ff6bbf373",
           20
          ],
          [
           "Multivariate function mapping n-dimensional vector",
           "ec07edce-bf3d-48a8-aa7c-b04407743114",
           "10a9d410-a8e3-4a02-804e-c18c9e34721d",
           20
          ],
          [
           "The Hessian matrix of a scalar-valued function of ",
           "2c58da71-f506-4d66-89ef-0c9852bd692e",
           "1ab3ab2e-411a-40fe-903c-7509b7e7d2a3",
           20
          ],
          [
           "Hessian matrix associated with cross-entropy error",
           "c2a504ce-e118-4183-b7bb-70ea9a9f48a6",
           "1bce1b77-b9ab-4d2c-b392-8d81e6b9a5df",
           20
          ],
          [
           "Decomposition of the Hessian matrix for the cross-",
           "1465480a-632e-49ad-9237-f4744149cf72",
           "24f31dd3-4fef-43ab-98b9-af03bade4111",
           20
          ],
          [
           "Hessian matrix for cross-entropy error in logistic",
           "a2d3dc5f-69a8-4a0c-aa62-1dca31166910",
           "26b51406-8cbc-400b-ac46-3b43fd530dde",
           20
          ],
          [
           "The Hessian matrix is a square matrix with dimensi",
           "33e02055-2d36-4821-9d2c-10d2c0f149d0",
           "33c58f3a-fa60-4097-8fd4-c489777e9a8f",
           20
          ],
          [
           "The Hessian matrix of the cross-entropy error func",
           "535361da-535d-44df-9dd1-e8c5965e4dd5",
           "8c95418a-35bb-4174-957f-32362c1f9edf",
           20
          ],
          [
           "Hessian matrix of the cross-entropy error in logis",
           "e11f041e-6ba9-4e83-bff4-bffccdb6a2fa",
           "bac5803f-44b5-41a1-ab58-807bef410144",
           20
          ],
          [
           "Hessian matrix of the cross-entropy error function",
           "9c296e84-c6c8-4f11-8afd-e7e34526fe6c",
           "bf440dc5-86f1-48d2-a19b-fbce378d4f04",
           20
          ],
          [
           "Hessian matrix of the cross-entropy error function",
           "26499715-184e-4000-ba6b-41d5870059d6",
           "dc48c8b4-295f-4ebd-a79f-87ec0762a6ba",
           20
          ],
          [
           "Matrix representing the Hessian of the cross-entro",
           "23c307d0-55f9-4af6-ae57-a3aed2d09de6",
           "dfd3ca5b-be46-4dff-970a-7d200c6dba60",
           20
          ],
          [
           "Set of four equations defining logistic regression",
           "ef825ee0-b6c6-4b17-ae08-11f014f82c11",
           "34d55c22-631a-4bdb-be8e-bfe30dacff7b",
           21
          ],
          [
           "Set of four expressions specifying the linear regr",
           "98263ab1-c7fd-4aa7-ab7d-43c624ff8b27",
           "605152ba-fce7-4fd6-bc62-db20347d09ee",
           21
          ],
          [
           "Activation function in generalized linear models t",
           "7eef7c04-8db1-47ff-aab7-a6c02bd2f58e",
           "01da85d2-623d-4977-852a-0972bcddf92c",
           22
          ],
          [
           "Activation function applied to the scalar product ",
           "665dcb4e-ca2f-44e7-8ec1-b41a96d7a594",
           "0ac33d01-b617-4d8b-b18c-2a3e43cf23c5",
           22
          ],
          [
           "Mean parameter of an exponential family distributi",
           "67c044d3-4f19-48d8-8920-7cb61dfca3d0",
           "435b0c35-3037-47f7-aea6-190d431c251b",
           22
          ],
          [
           "Defines a generalized linear model function that t",
           "cc12c651-2e00-4250-a8e5-1cff8b42ba3e",
           "7ef9f98a-e511-44d9-a071-5ced65e55017",
           22
          ],
          [
           "Activation function in a generalized linear model ",
           "e9726689-8191-42f6-81f0-90f65fe8d069",
           "92802680-1b8d-4df1-871e-322e865b7bf9",
           22
          ],
          [
           "Activation function specifying the mean parameter ",
           "4a906f2b-ad03-4b12-958d-cdbcd44c4182",
           "eaf7a703-0e25-419b-b47c-4113e3495201",
           22
          ],
          [
           "Mean parameter of a distribution represented as a ",
           "1ceb5d2c-bf10-4d2c-b88d-c4c2063becef",
           "41770c7b-3c2e-4424-b51a-f49be4bb71f3",
           23
          ],
          [
           "The mean parameter of a distribution equals the do",
           "bf464be2-1d7b-40f9-8793-18a86a5cc8fc",
           "718f9c64-06b4-4052-b438-7401128d391c",
           23
          ],
          [
           "Defines the probability that an input vector belon",
           "172857b8-1a24-4a8e-9185-df275f5678df",
           "02feb7d5-6056-41fe-839c-2d877b3274c5",
           24
          ],
          [
           "Defines the probability that an input example belo",
           "4882e4ae-fb99-4b42-8e7d-882b7eb1cfef",
           "5e8143af-2a8d-4c0a-bf23-31536364b24d",
           24
          ],
          [
           "Set of four expressions for multinomial logistic r",
           "f5ce45b2-9f23-423a-85e2-34a8b5c7e8d8",
           "adc929fb-1233-4842-8a7e-647f15df431b",
           24
          ],
          [
           "The target expression represents the model for cla",
           "b81943d0-007b-4490-8b92-b3be8ac03355",
           "041f5adf-eb41-4a9c-b9c3-e004f0390544",
           25
          ],
          [
           "The target expression represents a set of models i",
           "5bbf0113-9153-4079-81a8-1fa3df67cd21",
           "6a63a376-81c6-4b8e-855e-1af1a1ad4735",
           25
          ],
          [
           "The target expression refers to the multinomial lo",
           "0d9f20ec-6596-4f25-bc42-494f90e2598f",
           "71045db6-ed78-411a-8dc6-04e7b49c31df",
           25
          ],
          [
           "The target expression represents the model for cla",
           "a5ba6803-e4d1-4eac-a72c-c0c94f3fe7de",
           "a3a63853-3912-4f62-8dbf-53fee564bdb1",
           25
          ],
          [
           "The target expression refers to the input example ",
           "d82b2105-3088-4b32-b2ae-297ce42187a6",
           "fec89daa-2650-46b2-b648-c44aed45c084",
           25
          ]
         ],
         "hovertemplate": "UMAP_1=%{x}<br>UMAP_2=%{y}<br>text=%{customdata[0]}<br>math_expression_id=%{customdata[1]}<br>id=%{customdata[2]}<br>cluster=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "AAAAAAAAAAABAQICAgICAgICAgIDAwMEBAUFBQYGBgcHBwcICAgJCQkKCgoKCwsMDAwMDAwNDQ4ODw8PDxAQEBAREREREREREhISEhISExMTFBQUFBQUFBQUFBQUFRUWFhYWFhYXFxgYGBkZGRkZ",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "2O4qQX0yMUG0qzFBd8wsQVVSNEEvECxBE7MsQdSzK0G8RLhAAgS0QLIpT0EY80tBhlBNQUTPTEEe2kdBbcpHQSWfUkEBSk5BtftIQVopSkFWSztBw8I/QbWaQEFLyTlB+cooQRiXN0HtUS9BZw0tQW/aE0E1fw5BHs0QQXIkUkEhu1JBa4lTQeKkVEHlf/JA2j71QPC87kBuxaNA2RypQCw7pkBy0AlBPz0FQTgWCUGtawdBfrQVQZZ1FUGZkEdBMppKQYG4SkEctD1B3ExMQXg7SEH+Ej5BvzY7QbA6HUGBZSRBpzfmQMvN3ECtluVAHnPmQA6uQEG20EdBsLhEQdhrQUERoetAvSisQNW+80CauudAzrTxQKAg90CREv5Ao8KiQCEDpEDWpp1AvO2oQINvp0Apv6NA9wArQUbnKEESji9BPMw6QemENUH3tDtBIpA+QVHqQEFCQD1B4HY+QX2iQEHpNz9BjYxFQdPEQkEHP0VBs/j7QKflEUFprxVBJEwWQVMtEkFdhRhBQ1IXQU2dFEHZ3hVBjHsZQYAE1ECOvuZAtrfkQBzTz0ANANRAc3ncQL2P0UAnD9ZA",
          "dtype": "f4"
         },
         "xaxis": "x",
         "y": {
          "bdata": "iDAYQV8/F0G1qxtBNkIVQYm3GEFYuBpBteUWQZoGFEFOjqpAIqy2QAQGHUEpdx9B19wcQZruI0EA8iFBz0wbQZy8H0HCSiRBUr4cQVH/IEF3nRdBKucWQVj5FEHDdRRBqov7QKZSqUCfrwtBLOMGQeQfrkADlKpAtLKsQNdwDkErDxVBFhUQQd55EUHIMb9AAoDCQGj2vUCH3qxAs2ekQIHRq0A5SKJAzd+iQFhYo0Cs9aBAieC5QG4XvUARhttA1aPaQOWA30AOQfJAiRDYQFWF40BymdNAXTbbQEWsykAiAc9Af0maQOi1k0CLIZxASBWTQAVmzUDgI9BAL6rOQPxtx0B9G4VAVP/BQD8njkDxJoZAUkOMQLYUkUCZNYhAy0vBQFcQwECMt7pAOpG3QGc0xEA37bJA/aDWQEj+00BOntxA+AWtQLZ13EBdj81ADBu3QLhxq0Cgu61AUsLBQKfyrkCHSKlAAVO5QA70rkATp7RABPHEQJc70kAFydxAdSzWQFtK50AS4tBA1svXQG1F4kD4TN9AVX3bQJp3okAb1alAF/yuQMuqqkALg6pAAmmzQKNsqUBzNa9A",
          "dtype": "f4"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP_1"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP_2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = []\n",
    "for x, y, label, record in zip(X_umap[:, 0], X_umap[:, 1], cluster_labels, records):\n",
    "    row = {\n",
    "        'UMAP_1': x,\n",
    "        'UMAP_2': y,\n",
    "        'cluster': label,\n",
    "        'id': record.id,\n",
    "    }\n",
    "    row.update(record.payload or {})  # add all payload fields\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='UMAP_1',\n",
    "    y='UMAP_2',\n",
    "    color='cluster',\n",
    "    hover_data=list(payload_keys) + ['id', 'cluster'],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f8f4d",
   "metadata": {},
   "source": [
    "#### Continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b98c0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathExpressionGroup\n",
    "\n",
    "\n",
    "grouped_math_expression_ids = [\n",
    "    [description.math_expression_id for description in descriptions]\n",
    "    for descriptions in grouped_descriptions\n",
    "]\n",
    "\n",
    "for math_expression_ids in grouped_math_expression_ids:\n",
    "    # group requires at least two elements\n",
    "    if len(math_expression_ids) < 2:\n",
    "        continue\n",
    "\n",
    "    math_expression_group = MathExpressionGroup(math_expression_index_id=index.id)\n",
    "    await math_expression_group_repository.insert_one(math_expression_group)\n",
    "    await math_expression_group_graph_repository.insert_one_node(math_expression_group)\n",
    "\n",
    "    # add all candidates to a group, remove some of them in the next step\n",
    "    await math_expression_repository.update_group_id(math_expression_ids, math_expression_group.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3e827",
   "metadata": {},
   "source": [
    "### 6. MathExpressionGroupRelationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "from math_rag.application.models.assistants.inputs import MathExpressionComparator as AssistantInput\n",
    "from math_rag.application.utils import GroupPrunerUtil\n",
    "from math_rag.core.models import MathExpressionGroupRelationship\n",
    "\n",
    "\n",
    "math_expression_groups = await math_expression_group_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")\n",
    "\n",
    "for math_expression_group in math_expression_groups:\n",
    "    math_expressions = await math_expression_repository.find_many(\n",
    "        filter=dict(math_expression_group_id=math_expression_group.id)\n",
    "    )\n",
    "    math_expression_ids = [math_expression.id for math_expression in math_expressions]\n",
    "    math_expression_contexts = await math_expression_context_repository.find_many(\n",
    "        filter=dict(math_expression_id=math_expression_ids)\n",
    "    )\n",
    "    pairs = list(combinations(zip(math_expressions, math_expression_contexts), 2))\n",
    "\n",
    "    print(len(math_expression_ids))\n",
    "    print(len(pairs))\n",
    "    print('----')\n",
    "\n",
    "    if not pairs:\n",
    "        continue\n",
    "\n",
    "    inputs: list[AssistantInput] = []\n",
    "    input_id_to_candidate_pair: dict[UUID, tuple[UUID, UUID]] = {}\n",
    "\n",
    "    for pair, other_pair in pairs:\n",
    "        math_expression, math_expression_context = pair\n",
    "        other_math_expression, other_math_expression_context = other_pair\n",
    "        input = AssistantInput(\n",
    "            katex=math_expression.katex,\n",
    "            context=math_expression_context.text,\n",
    "            other_katex=other_math_expression.katex,\n",
    "            other_context=other_math_expression_context.text,\n",
    "        )\n",
    "        inputs.append(input)\n",
    "        input_id_to_candidate_pair[input.id] = (math_expression.id, other_math_expression.id)\n",
    "\n",
    "    outputs = await math_expression_comparator_assistant.concurrent_assist(inputs)\n",
    "\n",
    "    candidates = math_expression_ids\n",
    "    candidate_pair_to_is_connected = {\n",
    "        input_id_to_candidate_pair[output.input_id]: output.is_identical for output in outputs\n",
    "    }\n",
    "\n",
    "    math_expression_ids = [math_expression.id for math_expression in math_expressions]\n",
    "    math_expression_ids_to_group = GroupPrunerUtil.prune(candidates, candidate_pair_to_is_connected)\n",
    "    math_expression_ids_to_ungroup = list(\n",
    "        set(math_expression_ids) - set(math_expression_ids_to_group)\n",
    "    )\n",
    "\n",
    "    if not math_expression_ids_to_group:\n",
    "        continue\n",
    "\n",
    "    math_expression_group_relationships = [\n",
    "        MathExpressionGroupRelationship(\n",
    "            math_expression_index_id=index.id,\n",
    "            math_expression_id=math_expression_id,\n",
    "            math_expression_group_id=math_expression_group.id,\n",
    "        )\n",
    "        for math_expression_id in math_expression_ids_to_group\n",
    "    ]\n",
    "    await math_expression_repository.update_group_id(math_expression_ids_to_ungroup, None)\n",
    "\n",
    "    # insert updated math expressions to the graph database\n",
    "    math_expressions_updated = await math_expression_repository.find_many(\n",
    "        filter=dict(id=math_expression_ids)\n",
    "    )\n",
    "    await math_expression_graph_repository.insert_many_nodes(math_expressions_updated)\n",
    "\n",
    "    math_expression_group_relationships = [\n",
    "        MathExpressionGroupRelationship(\n",
    "            math_expression_index_id=index.id,\n",
    "            math_expression_id=math_expression_id,\n",
    "            math_expression_group_id=math_expression_group.id,\n",
    "        )\n",
    "        for math_expression_id in math_expression_ids_to_group\n",
    "    ]\n",
    "\n",
    "    # print(len(math_expression_ids))\n",
    "    # print(len(math_expression_ids_to_group))\n",
    "    # print(len(math_expression_ids_to_ungroup))\n",
    "    # print(len(math_expression_group_relationships))\n",
    "    # print('----')\n",
    "\n",
    "    await math_expression_group_graph_repository.insert_many_rels(\n",
    "        math_expression_group_relationships, rel_to_cls=MathExpression\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ee39e",
   "metadata": {},
   "source": [
    "## Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f44bd",
   "metadata": {},
   "source": [
    "### 1. MathArticleChunk, requires: MathExpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e063fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expressions = await math_expression_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b9bfa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathArticleChunk\n",
    "\n",
    "\n",
    "index_to_katex = {\n",
    "    math_expression.index: math_expression.katex for math_expression in math_expressions\n",
    "}\n",
    "chunk_templates = TemplateChunkerUtil.chunk(template, max_window_size=2048, max_padding=256)\n",
    "math_article_chunks: list[MathArticleChunk] = []\n",
    "\n",
    "for i, chunk_template in enumerate(chunk_templates):\n",
    "    indexes = TemplateIndexFinderUtil.find(chunk_template)\n",
    "    formatted_chunk, _ = TemplateFormatterUtil.format(\n",
    "        chunk_template, index_to_katex, omit_wrapper=False\n",
    "    )\n",
    "    # print(_)\n",
    "    math_article_chunk = MathArticleChunk(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_index_id=index.id,\n",
    "        index=i,\n",
    "        indexes=indexes,\n",
    "        text=formatted_chunk,\n",
    "    )\n",
    "    math_article_chunks.append(math_article_chunk)\n",
    "\n",
    "await math_article_chunk_repository.insert_many(math_article_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f6fdc",
   "metadata": {},
   "source": [
    "### 2. MathExpressionRelationship, requires: MathExpression, MathArticleChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89922f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expressions = await math_expression_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")\n",
    "math_article_chunks = await math_article_chunk_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5259e873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2735"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_expected_chunks = sum(\n",
    "    len(math_article_chunk.indexes) - 1 for math_article_chunk in math_article_chunks\n",
    ")\n",
    "num_expected_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "542a3754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    },
    {
     "ename": "MathExpressionNodeDoesNotExist",
     "evalue": "(MathExpressionNodeDoesNotExist(...), \"{'uid': '57072083-3dd6-4d35-8bd1-0ddbcbcd5627'}\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMathExpressionNodeDoesNotExist\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(math_expression_relationships))\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m math_expression_relationship_repository\u001b[38;5;241m.\u001b[39minsert_many(math_expression_relationships)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m math_expression_graph_repository\u001b[38;5;241m.\u001b[39minsert_many_rels(math_expression_relationships, rel_to_cls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/workspaces/math_rag/infrastructure/repositories/graphs/graph_repository.py:157\u001b[0m, in \u001b[0;36mGraphRepository.insert_many_rels\u001b[0;34m(self, rels, rel_to_cls)\u001b[0m\n\u001b[1;32m    155\u001b[0m source_node_set \u001b[38;5;241m=\u001b[39m cast(AsyncNodeSet, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_node_cls\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[1;32m    156\u001b[0m source_node_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(rel_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_node_id_field)\n\u001b[0;32m--> 157\u001b[0m source_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m source_node_set\u001b[38;5;241m.\u001b[39mget(uid\u001b[38;5;241m=\u001b[39msource_node_id)\n\u001b[1;32m    159\u001b[0m target_node_cls \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m     TargetTypeResolverUtil\u001b[38;5;241m.\u001b[39mresolve(source_cls\u001b[38;5;241m=\u001b[39mrel_to_cls)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rel_to_cls\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_node_cls\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    164\u001b[0m target_node_set \u001b[38;5;241m=\u001b[39m cast(AsyncNodeSet, target_node_cls\u001b[38;5;241m.\u001b[39mnodes)\n",
      "File \u001b[0;32m/workspaces/.venv/lib/python3.12/site-packages/neomodel/async_/match.py:1431\u001b[0m, in \u001b[0;36mAsyncNodeSet.get\u001b[0;34m(self, lazy, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MultipleNodesReturned(\u001b[38;5;28mrepr\u001b[39m(kwargs))\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_class\u001b[38;5;241m.\u001b[39mDoesNotExist(\u001b[38;5;28mrepr\u001b[39m(kwargs))\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mMathExpressionNodeDoesNotExist\u001b[0m: (MathExpressionNodeDoesNotExist(...), \"{'uid': '57072083-3dd6-4d35-8bd1-0ddbcbcd5627'}\")"
     ]
    }
   ],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionRelationshipDetector as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionRelationship\n",
    "\n",
    "\n",
    "for math_article_chunk in math_article_chunks:\n",
    "    if len(math_article_chunk.indexes) < 2:\n",
    "        continue\n",
    "\n",
    "    start_indexes = math_article_chunk.indexes[:-1]\n",
    "    last_index = math_article_chunk.indexes[-1]\n",
    "    index_pairs = [(index, last_index) for index in start_indexes]\n",
    "\n",
    "    inputs: list[AssistantInput] = []\n",
    "    input_id_to_math_expression_id_pair: dict[UUID, tuple[UUID, UUID]] = {}\n",
    "    input_id_to_math_expression_index_pair: dict[UUID, tuple[int, int]] = {}\n",
    "\n",
    "    for source_index, target_index in index_pairs:\n",
    "        input = AssistantInput(\n",
    "            chunk=math_article_chunk.text, source=source_index, target=target_index\n",
    "        )\n",
    "        inputs.append(input)\n",
    "\n",
    "        source_math_expression = next(\n",
    "            (x for x in math_expressions if x.index == source_index), None\n",
    "        )\n",
    "        target_math_expression = next(\n",
    "            (x for x in math_expressions if x.index == target_index), None\n",
    "        )\n",
    "\n",
    "        if source_math_expression is None or target_math_expression is None:\n",
    "            raise ValueError()\n",
    "\n",
    "        input_id_to_math_expression_id_pair[input.id] = (\n",
    "            source_math_expression.id,\n",
    "            target_math_expression.id,\n",
    "        )\n",
    "        input_id_to_math_expression_index_pair[input.id] = source_index, target_index\n",
    "\n",
    "    outputs = await math_expression_relationship_detector_assistant.concurrent_assist(inputs)\n",
    "    math_expression_relationships = [\n",
    "        MathExpressionRelationship(\n",
    "            math_article_chunk_id=math_article_chunk.id,\n",
    "            math_expression_index_id=index.id,\n",
    "            math_expression_source_id=input_id_to_math_expression_id_pair[output.input_id][0],\n",
    "            math_expression_target_id=input_id_to_math_expression_id_pair[output.input_id][1],\n",
    "            math_expression_source_index=input_id_to_math_expression_index_pair[output.input_id][0],\n",
    "            math_expression_target_index=input_id_to_math_expression_index_pair[output.input_id][1],\n",
    "        )\n",
    "        for output in outputs\n",
    "        if output.relationship_exists\n",
    "    ]\n",
    "\n",
    "    print(len(outputs))\n",
    "    print(len(math_expression_relationships))\n",
    "\n",
    "    await math_expression_relationship_repository.insert_many(math_expression_relationships)\n",
    "    await math_expression_graph_repository.insert_many_rels(\n",
    "        math_expression_relationships, rel_to_cls=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b852a9",
   "metadata": {},
   "source": [
    "### 3. MathExpressionRelationshipDescription, requires: MathArticleChunk, MathExpressionRelationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2b89f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_expression_relationships = await math_expression_relationship_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")\n",
    "len(math_expression_relationships)\n",
    "# 2389 < 2735 because llm decided that some of them are not connected\n",
    "# gpt 4o: 2389\n",
    "# gpt 4o nano: 2692 (bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87f5b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionRelationshipDescriptionWriter as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionRelationshipDescription\n",
    "\n",
    "\n",
    "math_article_chunk_ids = [\n",
    "    math_expression_relationship.math_article_chunk_id\n",
    "    for math_expression_relationship in math_expression_relationships\n",
    "]\n",
    "math_article_chunks = await math_article_chunk_repository.find_many(\n",
    "    filter=dict(id=math_article_chunk_ids)\n",
    ")\n",
    "\n",
    "inputs: list[AssistantInput] = []\n",
    "input_id_to_math_expression_relationship_id: dict[UUID, UUID] = {}\n",
    "\n",
    "for math_article_chunk, math_expression_relationship in zip(\n",
    "    math_article_chunks, math_expression_relationships\n",
    "):\n",
    "    input = AssistantInput(\n",
    "        chunk=math_article_chunk.text,\n",
    "        source=math_expression_relationship.math_expression_source_index,\n",
    "        target=math_expression_relationship.math_expression_target_index,\n",
    "    )\n",
    "    inputs.append(input)\n",
    "    input_id_to_math_expression_relationship_id[input.id] = math_expression_relationship.id\n",
    "\n",
    "outputs = await math_expression_relationship_description_writer_assistant.concurrent_assist(inputs)\n",
    "descriptions = [\n",
    "    MathExpressionRelationshipDescription(\n",
    "        math_expression_index_id=index.id,\n",
    "        math_expression_relationship_id=input_id_to_math_expression_relationship_id[\n",
    "            output.input_id\n",
    "        ],\n",
    "        text=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]\n",
    "await math_expression_relationship_description_repository.insert_many(descriptions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
