{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32ba622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from math_rag.application.containers import ApplicationContainer\n",
    "    from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "    application_container: ApplicationContainer\n",
    "    infrastructure_container: InfrastructureContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c020d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 16:44:29,387 - INFO - datasets - config.py:54 - PyTorch version 2.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "RESET = False\n",
    "%load_ext hooks.notebook_hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386180b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92db620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expression_description_writer_assistant = (\n",
    "    application_container.math_expression_description_writer_assistant()\n",
    ")\n",
    "math_expression_description_optimizer_assistant = (\n",
    "    application_container.math_expression_description_optimizer_assistant()\n",
    ")\n",
    "math_expression_comparator_assistant = application_container.math_expression_comparator_assistant()\n",
    "math_expression_relationship_description_writer_assistant = (\n",
    "    application_container.math_expression_relationship_description_writer_assistant()\n",
    ")\n",
    "math_expression_relationship_detector_assistant = (\n",
    "    application_container.math_expression_relationship_detector_assistant()\n",
    ")\n",
    "\n",
    "default_embedder = application_container.default_embedder()\n",
    "math_expression_description_opt_embedding_repository = (\n",
    "    infrastructure_container.math_expression_description_opt_embedding_repository()\n",
    ")\n",
    "math_expression_description_opt_repository = (\n",
    "    infrastructure_container.math_expression_description_opt_repository()\n",
    ")\n",
    "math_expression_description_repository = (\n",
    "    infrastructure_container.math_expression_description_repository()\n",
    ")\n",
    "math_expression_group_repository = infrastructure_container.math_expression_group_repository()\n",
    "math_expression_group_graph_repository = (\n",
    "    await infrastructure_container.math_expression_group_graph_repository()\n",
    ")\n",
    "math_expression_repository = infrastructure_container.math_expression_repository()\n",
    "grouper_service = application_container.grouper_service()\n",
    "\n",
    "math_expression_graph_repository = await infrastructure_container.math_expression_graph_repository()\n",
    "math_expression_relationship_repository = (\n",
    "    infrastructure_container.math_expression_relationship_repository()\n",
    ")\n",
    "math_expression_relationship_description_repository = (\n",
    "    infrastructure_container.math_expression_relationship_description_repository()\n",
    ")\n",
    "katex_corrector_service = application_container.katex_corrector_service()\n",
    "math_expression_context_repository = infrastructure_container.math_expression_context_repository()\n",
    "math_article_chunk_repository = infrastructure_container.math_article_chunk_repository()\n",
    "math_expression_index_repository = infrastructure_container.math_expression_index_repository()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb39d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 16:44:33,634 - INFO - googleapiclient.discovery_cache - __init__.py:49 - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from math_rag.core.models import MathArticle\n",
    "\n",
    "\n",
    "google_drive_repository = infrastructure_container.google_drive_repository()\n",
    "math_article_parser_service = infrastructure_container.math_article_parser_service()\n",
    "\n",
    "file_id = google_drive_repository.get_file_id(\n",
    "    Path('ml/lectures/L07-LogisticRegression2/2024_08_10_2174b40686820b4cb591g.tex')\n",
    ")\n",
    "\n",
    "if not file_id:\n",
    "    raise ValueError()\n",
    "\n",
    "file_content = google_drive_repository.get_file_by_id(file_id)\n",
    "\n",
    "math_article = MathArticle(\n",
    "    math_expression_dataset_id=None,\n",
    "    math_expression_index_id=None,\n",
    "    name='article',\n",
    "    bytes=file_content.getvalue(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081db754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('42c8b539-84a8-445b-b9a1-547022e6c47d')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import UUID\n",
    "\n",
    "from math_rag.application.utils import InputCreatorUtil\n",
    "from math_rag.core.models import MathExpressionIndex\n",
    "from math_rag.infrastructure.utils import (\n",
    "    TemplateChunkerUtil,\n",
    "    TemplateContextChunkerUtil,\n",
    "    TemplateFormatterUtil,\n",
    "    TemplateIndexFinderUtil,\n",
    ")\n",
    "\n",
    "\n",
    "index = MathExpressionIndex()\n",
    "index.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81928c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = MathExpressionIndex(id=UUID('d7672957-dc0d-4f9d-8fbb-da2d91cb0dc2'))\n",
    "# index.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 16:44:36,226 - WARNING - neo4j.notifications - result.py:337 - Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (a) { ... }} {position: line: 3, column: 13, offset: 35} for query: '\\n            MATCH (a)\\n            CALL { WITH a DETACH DELETE a }\\n            IN TRANSACTIONS OF 5000 rows\\n        '\n",
      "2025-07-13 16:44:36,231 - WARNING - neo4j.notifications - result.py:337 - Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (a) { ... }} {position: line: 3, column: 13, offset: 35} for query: '\\n            MATCH (a)\\n            CALL { WITH a DETACH DELETE a }\\n            IN TRANSACTIONS OF 5000 rows\\n        '\n"
     ]
    }
   ],
   "source": [
    "# index_id_to_remove = index.id\n",
    "# common_filter = dict(math_expression_index_id=index_id_to_remove)\n",
    "\n",
    "# await math_expression_index_repository.delete_one(filter=dict(id=index_id_to_remove))\n",
    "\n",
    "# await math_expression_repository.delete_many(filter=common_filter.copy())\n",
    "# await math_expression_context_repository.delete_many(filter=common_filter.copy())\n",
    "# await math_expression_description_repository.delete_many(filter=common_filter.copy())\n",
    "# await math_expression_description_opt_repository.delete_many(filter=common_filter.copy())\n",
    "# await math_expression_group_repository.delete_many(filter=common_filter.copy())\n",
    "# await math_article_chunk_repository.delete_many(filter=common_filter.copy())\n",
    "# await math_expression_relationship_repository.delete_many(filter=common_filter.copy())\n",
    "# await math_expression_relationship_description_repository.delete_many(filter=common_filter.copy())\n",
    "# await math_expression_description_opt_embedding_repository.clear()\n",
    "# await math_expression_graph_repository.clear()\n",
    "# await math_expression_group_graph_repository.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0146e",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816adeb6",
   "metadata": {},
   "source": [
    "### 1. MathExpression, requires: MathArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aad9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathExpression\n",
    "\n",
    "\n",
    "math_nodes, _, template = math_article_parser_service.parse_for_index(math_article)\n",
    "math_nodes.sort(key=lambda x: x.position)\n",
    "\n",
    "katexes = [math_node.latex.strip('$') for math_node in math_nodes]\n",
    "valid_katexes = await katex_corrector_service.correct(katexes, max_num_retries=3)\n",
    "math_expressions = [\n",
    "    MathExpression(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_dataset_id=None,\n",
    "        math_expression_group_id=None,\n",
    "        math_expression_index_id=index.id,\n",
    "        latex=node.latex,\n",
    "        katex=katex.strip(),\n",
    "        index=i,\n",
    "        position=node.position,\n",
    "        is_inline=node.is_inline,\n",
    "    )\n",
    "    for i, (node, katex) in enumerate(zip(math_nodes, valid_katexes))\n",
    "]\n",
    "await math_expression_repository.insert_many(math_expressions)\n",
    "await math_expression_graph_repository.insert_many_nodes(math_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff89ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jan Šnajder, lectures, v2.0\n",
      "\n",
      "Last time we introduced the logistic regression algorithm. We defined the model and derived the cross-entropy error function as the negative probability of the labels in the training set. We established that minimizing that error had no solution in closed form, so we turned to iterative procedures. We have considered the simplest such procedure, the gradient descent algorithm, and we applied it to logistic regression, in standard (batch) and stochastic variant. In the end, we talked about regularization, specifically [math_placeholder | 0] regularization, which we incorporated quite straightforwardly into the optimization process.\n",
      "\n",
      "Today we'll talk a bit more about logistic regression. First, we'll consider some more efficient (read: faster) alternatives to gradient descent. Second, we'll consider the extension of binary logistic regression to multiclass logistic regression. Third, we'll look at all the models discussed thus far and see what they have in common and how they can be generalized. Finally, we'll talk about adaptive basis functions, which is a way to learn the feature mapping function directly from data, instead of defining it manually.\n",
      "\n",
      "Unlike previous lectures, this one won't go into details, because we do not have the time for this. My goal is to give you enough information to know where to look for more, should you feel motivated to do so or if you need it\n",
      "\n",
      "\n",
      "Last time we established that the gradient descent needs to be coupled with line search, otherwise there is no guarantee for global convergence. This means that, depending on where the descent initially starts, it can happen that the gradient descent does not converge but diverge (effectively it starts to ascend instead of descend). Line search prevents this from happening. However, line search can result in a zig-zag descent. Let's recall the figure we discussed last time:\n",
      "\n",
      "\n",
      "[image_placeholder | 0]\n",
      "\n",
      "\n",
      "The blue trajectory corresponds to the best scenario, and the red to the worst-case scenario for a gradient descent with line search. Which one will play out depends on the choice of the starting point of the search. As we can see, it can happen that the descent zig-zags quite a lot, which means that the optimization will consume a lot of iterations. Obviously, the problem arises because the descent direction is not as good as it could be. Imagine descending into a pit from the starting point for the red trajectory. It is hard to imagine that you will descend\n",
      "as pointed to by the line. It is more likely that the gravity would pull you and your descend direction would be steeper (i.e., at a smaller angle to the blue line). This reasoning of ours is based on the curvature of the isocontours (that is, the curvature of the surface down which we descend). In other words, the curvature of the surface gives us, in addition to the gradient, additional information about where the minimum is likely to be located (at least when it comes to convex functions).\n",
      "\n",
      "The above observations apply to the standard (batch) gradient. In stochastic gradient descent we typically do not use line search. But there the descent will zig-zag quite a lot anyway, since each step is taken based on the gradient calculated for a single example. In the following, we focus on non-stochastic, i.e., batch gradient descent.\n",
      "\n",
      "Based on the above consideration, we can conclude that the batch gradient descent could be improved if we take into account not only the slope (gradient) but also the curvature (the change in gradient, i.e., the second derivative) of the error function. Such optimization methods are referred to as second-order optimization, as opposed to first-order optimization methods, such as gradient descent. The basic second-order optimization method is the Newton's method.\n",
      "\n",
      "\n",
      "Consider minimization of function [math_placeholder | 1]. We know that the parameter update in gradient descent is as follows:\n",
      "\n",
      "[math_placeholder | 2]\n",
      "\n",
      "If we introduce an index for the iterations, then we can write this as an equation:\n",
      "\n",
      "[math_placeholder | 3]\n",
      "\n",
      "The idea with Newton's method is to take the point [math_placeholder | 4] (the current minimum) and compute at it the quadratic approximation of the function [math_placeholder | 5], and then move to the minimizer of this quadratic approximation (which is known analytically). If [math_placeholder | 6] is a function of one variable, this would look like this:\n",
      "\n",
      "\n",
      "[image_placeholder | 1]\n",
      "\n",
      "\n",
      "The black curve is the function [math_placeholder | 7] that we minimize. We start from point [math_placeholder | 8]. At this point we do a quadratic approximation of the function [math_placeholder | 9], thus obtaining a parabola that is tangential to the function [math_placeholder | 10] at the point [math_placeholder | 11]. The search then moves to a point that minimizes the quadratic approximation of [math_placeholder | 12]. In the picture it is the point [math_placeholder | 13]. At that point we again do a quadratic approximation of the function [math_placeholder | 14] and move to the point that minimizes that approximation. The procedure is repeated until the update is small enough.\n",
      "\n",
      "The methods works just the same for functions of several variables, i.e., in a multidimensional parameter space. In a two-dimensional space, it looks like this:\n",
      "\n",
      "\n",
      "[image_placeholder | 2]\n",
      "\n",
      "\n",
      "So, the idea is to take the step exactly so that we land in the minimum of the quadratic approximation. This will work well if [math_placeholder | 15] is a convex function, which happens to be the case with logistic regression.\n",
      "\n",
      "Let us now deal with the technical details. We need to find a quadratic approximation of a function at some point. How can we do this? Recall, from calculus, that every differentiable function [math_placeholder | 16] can be expressed at some given point [math_placeholder | 17] in the form of a power series. More precisely, every differentiable function can be expanded into a Taylor series about the point [math_placeholder | 18], as follows:\n",
      "\n",
      "[math_placeholder | 19]\n",
      "\n",
      "As we only need a quadratic approximation, we will take only the first three terms of the Taylor series. This then is the second-order Taylor expansion:\n",
      "\n",
      "[math_placeholder | 20]\n",
      "\n",
      "Note that this now is an approximation; the equality between [math_placeholder | 21] on the left-hand side and the series on the right-hand side is valid only when the series is infinite.\n",
      "\n",
      "Our error function, [math_placeholder | 22], is a function of multiple variables (i.e., vector w). For a multivariate function [math_placeholder | 23], the quadratic expansion about the point [math_placeholder | 24] is:\n",
      "\n",
      "[math_placeholder | 25]\n",
      "\n",
      "where [math_placeholder | 26] is the Hessian matrix (hrv. Hesseova matrica) of function [math_placeholder | 27] at point [math_placeholder | 28]. The Hessian matrix is a square [math_placeholder | 29] matrix of second-order partial derivatives of function [math_placeholder | 30], which is a function that maps [math_placeholder | 31]-dimensional vectors into scalars. The Hessian matrix is defined as follows:\n",
      "\n",
      "[math_placeholder | 32]\n",
      "\n",
      "that is, an element of the Hessian matrix is defined as:\n",
      "\n",
      "[math_placeholder | 33]\n",
      "\n",
      "Note that the Hessian matrix is a symmetric matrix, as the order of partial differentiation does not affect the result (commutativity).\n",
      "\n",
      "Using the expansion of the quadratic function [math_placeholder | 34], one can now derive its minimum, which us exactly where we want to step to when descending (we omit the derivation). The parameter update is then:\n",
      "\n",
      "[math_placeholder | 35]\n",
      "\n",
      "If the Hessian matrix is computed exactly (i.e., it is not an approximation), then we can set [math_placeholder | 36], because then the step will land exactly in the minimum of the quadratic approximation.\n",
      "\n",
      "We see that the Newton's optimization method requires us to compute the inverse of the Hessian matrix. In general, the Hessian matrix [math_placeholder | 37] is positive semi-definite ( [math_placeholder | 38] for every non-zero vector [math_placeholder | 39] ) if and only if [math_placeholder | 40] is a convex function. However, for its inverse to exist, the matrix [math_placeholder | 41] has to be positive definite ( [math_placeholder | 42] for each non-zero vector [math_placeholder | 43] ). If [math_placeholder | 44] is positive semi-definite, but not positive definite, then [math_placeholder | 45] has no inverse and we cannot apply the Newton's method.\n",
      "\n",
      "\n",
      "Let us now apply Newton's procedure to logistic regression. The weights update rule is as follows:\n",
      "\n",
      "[math_placeholder | 46]\n",
      "\n",
      "One can easily derive that the Hessian matrix for the cross-entropy error equals:\n",
      "\n",
      "[math_placeholder | 47]\n",
      "\n",
      "where [math_placeholder | 48], i.e., a diagonal matrix that on its diagonal has the firstorder derivatives of the logistic output [math_placeholder | 49] for each of the [math_placeholder | 50] examples from the training set. Just in case, let's check the compatibility of the matrices we multiply here: [math_placeholder | 51] [math_placeholder | 52]. Everything is fine: the dimension of the matrix [math_placeholder | 53] is [math_placeholder | 54], where [math_placeholder | 55] is the number of features in the feature space (i.e., after mapping).\n",
      "\n",
      "Let's go back quickly to the problem of computing the inverse of the Hessian matrix [math_placeholder | 56]. As we already said, [math_placeholder | 57] is positive semi-definite if and only if the function is convex. The crossentropy error is convex, so the [math_placeholder | 58] matrix for logistic regression will be positive semi-definite. But this alone does not guarantee it is invertible. However, one can show that the Hessian matrix [math_placeholder | 59] of cross-entropy, which can be decomposed as [math_placeholder | 60], must be positive definite. In other words, the Hessian matrix of the cross entropy error always has an inverse, so we can always use the Newton's method to optimize the parameters of logistic regression. However, due to multicollinearity, [math_placeholder | 61] may be ill-conditioned and in this case the solution will be unstable, which means we have to either select a linearly independent subset of features or apply regularization (we'll see how soon).\n",
      "\n",
      "If we were now to plug in [math_placeholder | 62] for [math_placeholder | 63] in the weights update rule above, and rearrange the expression a bit, we would obtain the iteratively reweighted least squares (IRLS) algorithm (hrv. algoritam najmanjih kvadrata s iterativnim ažuriranjem težina). We won't go into the details; it suffices for you to know that this algorithm is used for faster optimization for logistic regression, and that it is in fact an application of the Newton's method, which is a second-order optimization procedure.\n",
      "\n",
      "\n",
      "The problem with Newton's method (and thus with IRLS) is that the computation of the Hessian (and especially its inverse) can be expensive (especially if [math_placeholder | 64], the number of dimensions of the feature space, is large). Note that one has to compute the Hessian matrix and its inverse in every step of the optimization procedure. It might well be the case that doing gradient descent is less taxing computationally, even if the descent zig-zags a lot!\n",
      "\n",
      "The alternative is that, instead of computing the exact Hessian matrix, we compute its approximation. This is what the so-called quasi-Newton methods do. These procedures using the gradient vectors (from the current and previous step) to approximate the Hessian matrix (or its inverse) in each step of the descent. The most common such method is the BFSG algorithm. Again, we won't go into details, we just want you to be aware that the method exists.\n",
      "\n",
      "Another problem is that, even if we approximate the Hessian matrix, storing it in memory can be problematic because its dimensions are [math_placeholder | 65]. In that case, the matrix can be \"compressed\" using a low-rank approximation method. The algorithm that works that way is the limited BFSG (L-BFSG) (hrv. ograničen BFSG). Again, it suffices for you to know that this algorithm exists.\n",
      "\n",
      "\n",
      "We already know about the benefits of regularization, and last time we talked about the additional benefit of regularization in logistic regression (when optimizing the parameters for linearly separable problems, we have [math_placeholder | 66] and logistic regression overfits easily). Extending the Newton's method to regularized cross-entropy error is simple. Let's look at [math_placeholder | 67] regulation. We already considered adding the [math_placeholder | 68] regularization term to the gradient:\n",
      "\n",
      "[math_placeholder | 69]\n",
      "\n",
      "We have the same for the Hessian matrix:\n",
      "\n",
      "[math_placeholder | 70]\n",
      "\n",
      "where [math_placeholder | 71] is a diagonal matrix with a regularization factor [math_placeholder | 72] on the diagonal (except for the upper-left element, because the weight [math_placeholder | 73] is not regularized).\n",
      "\n",
      "We see, thus, that [math_placeholder | 74] regularization is easily incorporated into the gradient descent and in the Newton's method, and it was just as easily incorporated into the least squares optimization method for the linear regression. All in all, [math_placeholder | 75] regularization is a tame beast.\n",
      "\n",
      "What about [math_placeholder | 76] regularization? So far, we haven't dealt with it at all, and we won't deal with it now, either. Let it be said that things get complicated here, which is expected because [math_placeholder | 77] norm is not differentiable, so we cannot calculate the gradient. We instead compute the subgradient (engl. podgradijent). We then typically use the coordinate descent (engl. koordinatni spust), where we optimize by each variable in term (dimension after dimension), or we use proximal or projection optimization methods. For now, you only need to know that optimization for [math_placeholder | 78] regularized logistic regression is possible, that there are algorithms for it, and that they are implemented in standard tools.\n",
      "\n",
      "\n",
      "Last time we talked about binary logistic regression: we classified into classes [math_placeholder | 79] and [math_placeholder | 80]. To get the probabilities, we used the sigmoid for the activation function:\n",
      "\n",
      "[math_placeholder | 81]\n",
      "\n",
      "But what if we have more than two classes, i.e. [math_placeholder | 82] ? We could apply the OVO or OVR decomposition scheme, but the problem is that the probabilities for the individual classes would not add up to 1 . Moreover, in statistical sense the estimates for the parameters for the individual models are less reliable than estimates for a model that considers all classes at once. Instead, it's better to use the multinomial logistic regression (MNR, occasionally MLR), also referred to as maximum entropy classifier (hrv. klasifikator maksimalne entropije).\n",
      "\n",
      "\n",
      "The idea is actually very simple: use a separate weight vector [math_placeholder | 83] for each of the [math_placeholder | 84] classes, but then pass the scalar product [math_placeholder | 85] through an appropriate activation function to be make sure that the probabilities of all classes add up to 1. A function that does exactly this is the softmax\n",
      "\n",
      "function. For some input example [math_placeholder | 86], the softmax function takes the values [math_placeholder | 87] for each of the [math_placeholder | 88] classes, i.e., a [math_placeholder | 89]-dimensional vector, and maps them to a [math_placeholder | 90]-dimensional vector whose components sum to 1 . Formally, softmax : [math_placeholder | 91], where [math_placeholder | 92] is the component of the output vector equal to:\n",
      "\n",
      "[math_placeholder | 93]\n",
      "\n",
      "The softmax function accomplishes two things: it normalizes all values to a total of 1 , but it also amplifies the larger values and attenuates the smaller values. The function is called softmax because it corresponds a \"soft\" variant of the max function (in the sense that, unlike the max function, it is continuous and differentiable). Let's look at an example.\n",
      "\n",
      "[math_placeholder | 94] EXAMPLE\n",
      "\n",
      "\n",
      "[image_placeholder | 3]\n",
      "\n",
      "\n",
      "[math_placeholder | 95]\n",
      "\n",
      "\n",
      "[image_placeholder | 4]\n",
      "\n",
      "\n",
      "[math_placeholder | 96]\n",
      "\n",
      "We will define the model [math_placeholder | 97] of multinomial logistic regression as a set of models [math_placeholder | 98], where each model [math_placeholder | 99] is responsible for class [math_placeholder | 100] out of [math_placeholder | 101] classes. We define each model [math_placeholder | 102] so that it outputs the probability of example [math_placeholder | 103] belonging to class [math_placeholder | 104], using the softmax function:\n",
      "\n",
      "[math_placeholder | 105]\n",
      "\n",
      "where [math_placeholder | 106] is a matrix [math_placeholder | 107] weight vectors [math_placeholder | 108]. Note that, by virtue of the softmax function, the model [math_placeholder | 109] for class [math_placeholder | 110] takes into account the outputs of the other [math_placeholder | 111] models for the remaining classes.\n",
      "\n",
      "This defines the model. Let us now derive the error function.\n",
      "\n",
      "\n",
      "In binary logistic regression, we defined derived the error function staring from the negative logarithm of the probability of the labels. The labels were binary, [math_placeholder | 112], that is, they were Bernoulli variables. Now, since the output of multiclass regression can take more than two [math_placeholder | 113] values, then we move to categorical variable, which is also also called multinomial, or, perhaps better, the multinoulli variable. We represent such a variable as a vector of indicator (binary) variables:\n",
      "\n",
      "[math_placeholder | 114]\n",
      "\n",
      "where [math_placeholder | 115] if the outcome of the variable is [math_placeholder | 116], otherwise [math_placeholder | 117]. For example, [math_placeholder | 118] indicates that the multinomial variable has taken the third state out of four possible states. [math_placeholder | 119] is valid (outcomes are mutually exclusive and complete). Let's denote the probability [math_placeholder | 120] as [math_placeholder | 121].\n",
      "\n",
      "We will now define the distribution of this variable. Recall, the distribution of the Bernoulli variable, which has only two values, [math_placeholder | 122] and [math_placeholder | 123], is defined via the [math_placeholder | 124] parameter as follows:\n",
      "\n",
      "[math_placeholder | 125]\n",
      "\n",
      "We can generalize this to [math_placeholder | 126] values as follows. First, we need [math_placeholder | 127] parameters, so we will define a parameter vector:\n",
      "\n",
      "[math_placeholder | 128]\n",
      "\n",
      "where parameters [math_placeholder | 129] satisfy [math_placeholder | 130] and [math_placeholder | 131], as they represent probabilities.\n",
      "\n",
      "Now, by analogy with the Bernoulli distribution, the distribution of a categorical variable can be defined as:\n",
      "\n",
      "[math_placeholder | 132]\n",
      "\n",
      "Note: as with binary logistic regression, the probability that example [math_placeholder | 133] belongs to class [math_placeholder | 134] is exactly what we are given by the model:\n",
      "\n",
      "[math_placeholder | 135]\n",
      "\n",
      "Now we can finally write the logarithm of the probability of the labels from [math_placeholder | 136] as:\n",
      "\n",
      "[math_placeholder | 137]\n",
      "\n",
      "The error function we wish to minimize is the negative logarithm of the probability of the labels:\n",
      "\n",
      "[math_placeholder | 138]\n",
      "\n",
      "We see that we arrived at the generalization of the cross-entropy error to [math_placeholder | 139] classes. Also, from this we can read off the loss function as:\n",
      "\n",
      "[math_placeholder | 140]\n",
      "\n",
      "The logic is the same as with binary logistic regression: if the label [math_placeholder | 141] of some example [math_placeholder | 142] for class [math_placeholder | 143] is equal to 1 , then we want the model prediction (softmax output) to be a high probability close to 1 , because then [math_placeholder | 144] and the loss will be zero. Otherwise, if the model for the example whose label is 1 gives a value close to 0 , then the logarithm will be a large negative number, its negation will be a large positive number, and consequently the loss will be large.\n",
      "\n",
      "\n",
      "As with binary logistic regression, we cannot minimize [math_placeholder | 145] in closed form, so we need to rely on iterative optimization. For gradient descent, one can show (although it is a bit clumsy) that the gradient of the error function is equal to:\n",
      "\n",
      "[math_placeholder | 146]\n",
      "\n",
      "This is the gradient of the weights specifically for class [math_placeholder | 147]. The idea is that we can update the weights for each class separately. From this we can directly derive the stochastic gradient descent (we update the weights for each example, for each class). We can also derive the standard (batch) gradient descent, where we accumulate updates for all examples, for each class separately. We can also derive the Newton method (the Hessian matrix), but we will skip that.\n",
      "\n",
      "\n",
      "You may have noticed that the gradient of the loss we derived for multinomial logistic regression is actually the same as that for binary logistic regression: \"model's error times the example vector\". When we use this for gradient descent, we update the weights as follows:\n",
      "\n",
      "[math_placeholder | 148]\n",
      "\n",
      "We have already (in the context of the perceptron, if you remember) said this rule is called the Widrow-Hoff rule, while the alternative name is least-mean-squares (LMS) algorithm (not to be confused with least squares, although there is obviously a connection).\n",
      "\n",
      "The LMS algorithm, that is, this kind of learning where we use stochastic gradient descent to minimize the error by updating the model's weights in the way defined above, allows for online learning. We mentioned online learning the last time. Recall: online learning is a type of learning where not all learning examples need to be available up front, rather they can become available one after the other, and the model weights will be updated as new examples arrive.\n",
      "\n",
      "Let's go back a few weeks, to linear regression. When we introduced linear regression, we were actually talking only about batch optimization, which is achieved by calculating the pseudoinverse of the design matrix. However, already then we could have resorted to a stochastic (i.e., online) weight update. Namely, instead of searching analytically for the minimum of the error function, we can calculate the gradient of the error function, and then apply the gradient descent. (We didn't do that at that at that time, probably because we were blinded by the sheer elegance of the closed-form solution). Well, let's do it now. The quadratic error function of linear regression is:\n",
      "\n",
      "[math_placeholder | 149]\n",
      "\n",
      "The gradient (for one example [math_placeholder | 150] ) is:\n",
      "\n",
      "[math_placeholder | 151]\n",
      "\n",
      "So, the rule for updating the weights is:\n",
      "\n",
      "[math_placeholder | 152]\n",
      "\n",
      "which, again, is the LMS rule! So, there seems to be some underlying principle at play here all three regression algorithms use the same rule (LMS) for online learning (i.e., for stochastic gradient descent). How can that be? Well, this is because all these models - linear regression, logistic regression, and multinomial regression - belong to the generalized linear models family.\n",
      "\n",
      "Now it's time to round out the story and give a unified perspective on the three algorithms.\n",
      "\n",
      "\n",
      "For starters, let's recall from last lecture that generalized linear models are models that wrap the scalar product of the weight vector and example vector into an activation function [math_placeholder | 153]. So:\n",
      "\n",
      "[math_placeholder | 154]\n",
      "\n",
      "Let's look at three generalized linear models we've considered so far. For each of them, let's consider four points: (1) how the model is defined, (2) what is the probability distribution to which their output corresponds, (3) how is the loss is defined, and (4) what is the gradient of the loss, which we need for gradient descent.\n",
      "\n",
      "First, let's look at the linear regression algorithm:\n",
      "\n",
      "[math_placeholder | 155]\n",
      "\n",
      "For batch learning we use the least squares method (the pseudoinverse), and for online learning we use the LMS rule.\n",
      "\n",
      "Let's look at the logistic regression algorithm:\n",
      "\n",
      "[math_placeholder | 156]\n",
      "\n",
      "For batch learning we use gradient descent, the Newton's (IRLS) or the quasi-Newton method (BFSG, L-BFSG). We use the LMS rule for online learning.\n",
      "\n",
      "Finally, let's look at the multinomial logistic regression:\n",
      "\n",
      "[math_placeholder | 157]\n",
      "\n",
      "We use the same (mutatis mutandis) optimization procedures for model learning as for logistic regression.\n",
      "\n",
      "Notice the commonalities. For all three algorithms, we derived the loss function from the negative logarithm of the probability of labels of the examples from the dataset. We did this by using the normal, Bernoulli, and multinoulli distribution distribution for linear, binary logistic, and multinoulli logistic regression, respectively. Furthermore, for all three algorithms we derived an identical rule (the LMS) for online weights update.\n",
      "\n",
      "The question is: how come we always get the same weights update rule? Also, what is the relationship between the logistic function and the Bernoulli variable, and between the softmax function and the multinoulli distribution? There seems to be a connection, because in both cases we obtained the cross-entropy error. The answer lies in the properties of the distributions we used to model the labels [math_placeholder | 158].\n",
      "\n",
      "\n",
      "The distributions we have encountered so far (Gaussian, Bernoulli, multinoulli), but also some others that are often used in machine learning (binomial, multinomial, Student's t-distribution, uniform, beta distribution, gamma distribution, Dirichlet's) belong to the so-called exponential family (hrv. eksponencijalna familija). What is an exponential family? The exponential family is a broad group of distributions that can be written in the following form:\n",
      "\n",
      "[math_placeholder | 159]\n",
      "\n",
      "The exponential family distributions have many properties that are important for machine learning, but mostly for probabilistic approaches to machine learning, so we won't go into further into that here.\n",
      "\n",
      "What is of interesting to us here is that it is that the exponential family is crucial for for generalized linear models. Specifically, for the distributions belonging to the exponential family (including Gaussian, Bernoulli, and multinoulli) there is a relationship between the distribution and its (possibly nonlinear) activation function [math_placeholder | 160]. This function is in this context is referred to as the mean function (hrv. funkcija sredine), because it defines the [math_placeholder | 161] parameter of a distribution, i.e., the distribution's mean. Thus, the activation function [math_placeholder | 162] therefore [math_placeholder | 163] as a function of [math_placeholder | 164]. You probably already guessed that for the Gaussian distribution the activation function is an identity function, since [math_placeholder | 165]. For the Bernoulli's distribution it is the logistic function, and for the multinoulli distribution it is the softmax function.\n",
      "\n",
      "After this inspiring topic, let's take a look at another no less inspiring thing ...\n",
      "\n",
      "\n",
      "In generalized linear models (for both regression and classification) we had the option to map examples to the feature space using a feature mapping function:\n",
      "\n",
      "[math_placeholder | 166]\n",
      "\n",
      "where [math_placeholder | 167] is a set of [math_placeholder | 168] basis functions (nonlinear functions of the input variables): [math_placeholder | 169]. For example, polynomial mapping for [math_placeholder | 170] and [math_placeholder | 171] :\n",
      "\n",
      "[math_placeholder | 172]\n",
      "\n",
      "We then easily incorporated such a mapping into any generalized linear model:\n",
      "\n",
      "[math_placeholder | 173]\n",
      "\n",
      "where [math_placeholder | 174] is a chosen activation function (i.e., the mean function, to make use of the term we just introduced).\n",
      "\n",
      "Although we haven't tried it, the basis functions [math_placeholder | 175] need not necessarily be potencies or factors of the input features, but rather these can really be any functions. One interesting possibility is to use functions that measure the similarity of an example with some prototype\n",
      "examples in the input space. This then is called a kernel machine, and we'll talk about that in two weeks.\n",
      "\n",
      "At any rate, the limiting factor is that these are fixed basis functions: their number and shape is predetermined. This is a problem because in most cases we do not know in advance which basis functions are good for our problem. In other words, we generally don't know which feature mapping will make our problem linearly separable in feature space.\n",
      "\n",
      "We can solve this problem by letting the basis functions adapt to our data (examples from the training set). Here we have two options: the first, used by the aforementioned kernel machines, is to select some examples from the training set as prototypes, and then make the basis functions measure the similarity between an input example and these prototypes. This adjusts the total number of basis functions depending on the data. Another possibility is to use a fixed number of basis functions, but let each of them adapt to the data. Let's look into that in a bit more detail.\n",
      "\n",
      "The idea of adaptive basis functions is to define them up to some parameters, which we can then adjust to the data. That is, we will define parameterized basis functions. Does that sound familiar? Of course. It is exactly the same as training machine learning models: we define a function up to some parameters, and the parameters are determined by optimizing the empirical error on the training set. But let's look first at how one could go about parameterizing the basis functions. One possibility is to say that each basis function is a small generalized linear model on its own! So, we will have a generalized linear model and within it we'll have generalized linear models as its basis functions:\n",
      "\n",
      "[math_placeholder | 176]\n",
      "\n",
      "Note that each basis function in our model should have its own weight vector, so weights [math_placeholder | 177] in the inner sum have two indices: [math_placeholder | 178] is the basis function index, while [math_placeholder | 179] is the index of the weight in the weight vector of basis function [math_placeholder | 180]. With superscripts [math_placeholder | 181] and [math_placeholder | 182] we indicated which weights are used first in the calculation of model prediction, and which are used second. The expression on the right is just a matrix notation of the model, where we managed to get rid of the sums and we combined the weights into a weight vector and weight matrix. Take some time to convince yourself that this notation is the same as the one with sums.\n",
      "\n",
      "Now that we have metabolized this, it's time for a big revelation. What kind of model is this actually? For each basis function there are weights from the matrix [math_placeholder | 183], which we multiply with all the input features and add them up. We then multiply these values again by weights [math_placeholder | 184] and add them up. We've built something most of you already know: a neural network!\n",
      "\n",
      "\n",
      "[image_placeholder | 5]\n",
      "\n",
      "\n",
      "This network of ours is two-layered, however nothing prevents us from going deeper: we can make the basis functions be combinations of other basis functions, and those again be combinations of yet another basis functions, etc.\n",
      "\n",
      "Obviously, neural networks are a more complex model than generalized linear models. This, of course, comes at a price: a more complex optimization procedure (due to non-convexity of\n",
      "the error function, since the loss in the output layer - which can still be the quadratic loss or cross entropy loss - now has a very complex dependence on the weights of the previous layers) and a greater possibility of model overfitting. Of course, various solutions have been proposed to tackle these issues, in particular within the now popular deep learning paradigm. We won't go any further, however, as this topic is receiving a rather comprehensive treatment in other courses.\n",
      "\n",
      "What matters here is to be aware of the connection: a neural network is an extension of a generalized linear model in which the basis functions are adaptive, i.e., the feature mapping function is also learned from the data.\n",
      "\n",
      "\n",
      "\n",
      "  Newton's method is a second-order optimization method that converges faster than the gradient descent, and is based on the calculation of the Hessian matrix. The logistic regression variant is called IRLS\n",
      "  The calculation of the Hessian matrix is expensive to compute in both time and space, so we may resort to a quasi-Newton method, such as L-BSFG\n",
      "  Multinomial logistic regression is a generalization of logistic regression to more than two classes, with softmax function as the activation function\n",
      "  Common to generalized linear models is that their outputs are variables from the exponential family distributions\n",
      "  Instead of using fixed basis function, we can use parameterized adaptive basis functions, which brings us to neural networks\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd377fa0",
   "metadata": {},
   "source": [
    "### 2. MathExpressionContext, requires MathExpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6a6eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "above consideration, we can conclude that the batch gradient descent could be improved if we take into account not only the slope (gradient) but also the curvature (the change in gradient, i.e., the second derivative) of the error function. Such optimization methods are referred to as second-order optimization, as opposed to first-order optimization methods, such as gradient descent. The basic second-order optimization method is the Newton's method.\n",
      "\n",
      "\n",
      "Consider minimization of function [math_placeholder | 1]. We know that the parameter update in gradient descent is as follows:\n",
      "\n",
      "[math_placeholder | 2]\n",
      "\n",
      "If we introduce an index for the iterations, then we can write this as an equation:\n",
      "\n",
      "[math_placeholder | 3]\n",
      "\n",
      "The idea with Newton's method is to take the point [math_placeholder | 4] (the current minimum) and compute at it the quadratic approximation of the function [math_placeholder | 5], and then move to the minimizer of this quadratic approximation (which is known analytically). If [math_placeholder | 6]\n"
     ]
    }
   ],
   "source": [
    "context_templates = TemplateContextChunkerUtil.chunk(template, max_context_size=1000)\n",
    "assert len(context_templates) == len(math_expressions)\n",
    "\n",
    "print(context_templates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e5d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathExpressionContext\n",
    "\n",
    "\n",
    "index_to_katex = {\n",
    "    math_expression.index: math_expression.katex for math_expression in math_expressions\n",
    "}\n",
    "math_expression_contexts: list[MathExpressionContext] = []\n",
    "\n",
    "for math_expression, context_template in zip(math_expressions, context_templates):\n",
    "    formatted_context, _ = TemplateFormatterUtil.format(\n",
    "        context_template, index_to_katex, omit_wrapper=False\n",
    "    )\n",
    "    math_expression_context = MathExpressionContext(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_id=math_expression.id,\n",
    "        math_expression_index_id=index.id,\n",
    "        text=formatted_context,\n",
    "    )\n",
    "    math_expression_contexts.append(math_expression_context)\n",
    "\n",
    "await math_expression_context_repository.insert_many(math_expression_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315a5bb",
   "metadata": {},
   "source": [
    "### 3. MathExpressionDescription, requires MathExpression, MathExpressionContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c764cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionDescriptionWriter as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionDescription\n",
    "from math_rag.infrastructure.constants.services import MATH_TEMPLATE\n",
    "\n",
    "\n",
    "inputs: list[AssistantInput] = []\n",
    "input_id_to_math_expression_id: dict[UUID, UUID] = {}\n",
    "\n",
    "for math_expression, math_expression_context in zip(math_expressions, math_expression_contexts):\n",
    "    input = AssistantInput(\n",
    "        katex=MATH_TEMPLATE.format(katex=math_expression.katex, index=math_expression.index),\n",
    "        context=math_expression_context.text,\n",
    "    )\n",
    "    inputs.append(input)\n",
    "    input_id_to_math_expression_id[input.id] = math_expression.id\n",
    "\n",
    "outputs = await math_expression_description_writer_assistant.concurrent_assist(inputs)\n",
    "math_expression_descriptions = [\n",
    "    MathExpressionDescription(\n",
    "        math_expression_index_id=index.id,\n",
    "        math_expression_id=input_id_to_math_expression_id[output.input_id],\n",
    "        text=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]\n",
    "await math_expression_description_repository.insert_many(math_expression_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "605301ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f0a70670-5382-4058-aa2a-11e8de25fd19\n",
      "The number of examples in the training set.\n",
      "-----\n",
      "\n",
      "575cc528-f0d7-4adb-a103-4c736ca7d0ee\n",
      "The number of dimensions of the feature space.\n",
      "-----\n",
      "\n",
      "3b3c1871-78ed-49b7-ba60-5cf60782112f\n",
      "the point about which the Taylor series expansion of a differentiable function is taken\n",
      "-----\n",
      "\n",
      "e077e3f3-c74e-47a5-a630-c334e4e6e2bb\n",
      "the general differentiable function being considered for Taylor series expansion at a point\n",
      "-----\n",
      "\n",
      "e5fb6dd0-b1c1-4dea-995c-7c7c7296eac9\n",
      "the point about which the Taylor series expansion of a differentiable function is performed\n",
      "-----\n",
      "\n",
      "d191234c-ea79-490d-83e4-d4cea46f3580\n",
      "the function being minimized in the context of Newton's method, evaluated at a variable x\n",
      "-----\n",
      "\n",
      "fb890d36-4a5f-45f1-97a2-33e2a8250eac\n",
      "a square matrix with both the number of rows and columns equal to n\n",
      "-----\n",
      "\n",
      "57cedca5-a07e-46a8-a814-9c63d020e2c2\n",
      "The number of classes in a classification problem being greater than two.\n",
      "-----\n",
      "\n",
      "e72fe7f3-1020-4674-b66e-4f656c07399b\n",
      "the total number of classes in a classification problem, as referenced in the context of the softmax function\n",
      "-----\n",
      "\n",
      "f12212c9-b711-4ef0-9d2a-d4f2910a6487\n",
      "A mapping that takes an n-dimensional real vector as input and produces an n-dimensional real vector as output.\n",
      "-----\n",
      "\n",
      "6a139fd0-2530-4eb2-88ee-3c435d363a42\n",
      "the Hessian matrix of the function of interest, evaluated at the point denoted by the subscript t\n",
      "-----\n",
      "\n",
      "bf922eb8-7390-491b-95b6-f82ae32360cc\n",
      "the Hessian matrix associated with the cross-entropy error function in logistic regression, which is positive semi-definite and, under certain conditions, positive definite and invertible\n",
      "-----\n",
      "\n",
      "3d84377f-34ba-46d6-a2ae-830b9c3ee56f\n",
      "the total number of classes in the multinomial logistic regression model\n",
      "-----\n",
      "\n",
      "85ac7049-1b86-4e78-88fb-c587439a902d\n",
      "the output of the logistic function evaluated at a generic input vector, representing the predicted probability for that input in the context of logistic regression\n",
      "-----\n",
      "\n",
      "e4665be5-9045-468e-ac35-f1fa21677ffb\n",
      "The statement that, for every non-zero vector, the quadratic form with the Hessian matrix is greater than or equal to zero.\n",
      "-----\n",
      "\n",
      "b31742b1-1009-448c-b420-c17accc18c67\n",
      "the function being minimized, which depends on a single variable and is the subject of the quadratic approximation at the starting point\n",
      "-----\n",
      "\n",
      "32328d67-7f1a-4409-b361-e5b292dbc8f7\n",
      "The error function that depends on the vector variable w and is conditioned on the dataset D.\n",
      "-----\n",
      "\n",
      "dd634378-72b6-449f-aa96-f6cd838de3f4\n",
      "the initial point from which the minimization process starts in the context of minimizing a function of one variable using Newton's method\n",
      "-----\n",
      "\n",
      "7b553a8f-a347-4826-91e6-0d4275762b8d\n",
      "the Hessian matrix associated with the cross-entropy error function, whose dimension is (m+1) by (m+1), where m is the number of features in the feature space after mapping\n",
      "-----\n",
      "\n",
      "a8c4726e-e161-475c-bf19-73c1b430846b\n",
      "the norm of the parameter vector w increasing without bound, approaching infinity\n",
      "-----\n",
      "\n",
      "179b2530-f047-4c9f-b055-fe1cd7de5738\n",
      "The matrix referenced, which may be ill-conditioned due to multicollinearity, is the Hessian matrix of the cross-entropy error function in logistic regression.\n",
      "-----\n",
      "\n",
      "d4b1cc61-fd04-454a-a99e-166202a30d32\n",
      "the input example vector to the softmax function, representing a data point whose class probabilities are to be computed\n",
      "-----\n",
      "\n",
      "59bfb940-78b7-4150-a74e-9ce06bc9000e\n",
      "The number of possible output classes in the multiclass regression setting is greater than two.\n",
      "-----\n",
      "\n",
      "2395ef70-240f-47be-9ede-ed62e1410db8\n",
      "the symbol representing a non-zero vector used in the context of testing the positive semi-definiteness of the Hessian matrix\n",
      "-----\n",
      "\n",
      "6d2231cb-cc4f-40cd-b91c-44387adfa86f\n",
      "the Hessian matrix, which is referenced in the context of Newton's optimization method and discussed in terms of its positive semi-definiteness and positive definiteness properties\n",
      "-----\n",
      "\n",
      "cf7b23bd-c558-402f-9792-e6a609a703d1\n",
      "the number of basis functions in the set used for feature mapping in generalized linear models\n",
      "-----\n",
      "\n",
      "deeacc88-cbb2-466b-aee2-6ab11a16b38f\n",
      "This is a function that takes as input a vector from n-dimensional real space and outputs a real number; in other words, it maps n-dimensional real vectors to real scalars.\n",
      "-----\n",
      "\n",
      "b58e2409-c55f-4e30-b788-1fca9d2cbc80\n",
      "the function being minimized, which is approximated quadratically at each iteration to determine the next search point\n",
      "-----\n",
      "\n",
      "09feb954-e55a-4742-aecc-72bd8da5bb1b\n",
      "The total number of classes considered in the multinomial logistic regression model, where each class is associated with its own weight vector.\n",
      "-----\n",
      "\n",
      "276b5a23-68c5-41e1-859d-7779797f1e1b\n",
      "the value of the parameter vector at iteration t in an iterative optimization process\n",
      "-----\n",
      "\n",
      "ba45f30b-0ce1-41bc-961b-ef832622eb9e\n",
      "the Hessian matrix associated with the cross-entropy error in logistic regression, which has dimensions equal to the number of features plus one, squared\n",
      "-----\n",
      "\n",
      "e8c1f1fa-5661-4247-8620-fd51f5b175d0\n",
      "A general function of a vector variable, representing the objective function to be minimized in the context of optimization methods such as gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "65ce18fb-cd51-4230-87ab-2dcba4854d08\n",
      "The expression represents the decomposition of the Hessian matrix for the cross-entropy error in logistic regression, showing it as the product of the transpose of a matrix, a diagonal matrix, and the original matrix.\n",
      "-----\n",
      "\n",
      "f0b9017a-f1fc-4275-8ee3-9d646c637e32\n",
      "The regularized Hessian matrix, defined as the sum of the original Hessian matrix and a scaled identity matrix, where the scaling factor is the regularization parameter.\n",
      "-----\n",
      "\n",
      "beb0450f-62b3-468e-81f7-e8666c93c8fc\n",
      "the point reached after the first step of minimizing the quadratic approximation of the function, following the initial point in the iterative minimization process\n",
      "-----\n",
      "\n",
      "db3e5e24-8ba8-4d23-b417-72957fb3d84b\n",
      "the Hessian matrix whose positive definiteness is required for its inverse to exist in the context of Newton's optimization method\n",
      "-----\n",
      "\n",
      "aac728b1-fd11-490c-a0eb-ee2da4ac97a6\n",
      "the activation function that is applied to the scalar product of the weight vector and the feature vector in the definition of a generalized linear model\n",
      "-----\n",
      "\n",
      "3d8ba9cb-7ddb-4e4e-8575-efe40d1d9894\n",
      "The value representing the second possible outcome of a Bernoulli random variable, where the variable can take on either zero or one.\n",
      "-----\n",
      "\n",
      "0bd58e53-0014-44b8-808e-a56f461d0f3f\n",
      "the dimension of the output vector produced by the softmax function, which matches the number of classes and is equal to the number of input values to the softmax\n",
      "-----\n",
      "\n",
      "fa65176a-773f-4a75-9004-30e9803c628d\n",
      "the number of features in the feature space after mapping, as referenced in the context of the dimensions of the Hessian matrix\n",
      "-----\n",
      "\n",
      "44fa4563-06de-4bba-a64d-2d7c2daa2eab\n",
      "the dimensions of the Hessian matrix, where the matrix has m plus one rows and m plus one columns\n",
      "-----\n",
      "\n",
      "f75d9a87-588b-4778-af93-f5074f8bce70\n",
      "the index of a class in a classification problem with K classes, used to indicate which class an example belongs to\n",
      "-----\n",
      "\n",
      "24a1272f-257c-4818-818d-cfcefcc5926e\n",
      "A diagonal matrix whose diagonal entries are given by the product of the logistic function evaluated at each training example and one minus that value, for all examples in the training set.\n",
      "-----\n",
      "\n",
      "4e67ef8b-48cd-46c4-ad12-bd3685414f85\n",
      "the weight parameter that corresponds to the upper-left element of the parameter vector, which is excluded from regularization in the context of L2 regularization\n",
      "-----\n",
      "\n",
      "83c2d89c-3311-42d8-9df1-5b5a48211d2e\n",
      "the diagonal matrix whose diagonal entries are equal to the regularization factor, except possibly for the upper-left element, which may be zero because the corresponding weight is not regularized\n",
      "-----\n",
      "\n",
      "7185716c-65e6-498e-b294-58b20ea48a63\n",
      "the norm associated with L1 regularization, which is not differentiable and thus requires subgradient methods or specialized optimization techniques\n",
      "-----\n",
      "\n",
      "1cfb0ba4-2463-41c7-a051-236a6a90d0e5\n",
      "The general value of a function evaluated at the variable x, which is being approximated by a Taylor series expansion about the point a in the surrounding context.\n",
      "-----\n",
      "\n",
      "0549af99-079a-4cd8-ab82-7e531440d93d\n",
      "The expression assigns the value one to the parameter eta, indicating that the step size in the parameter update rule for Newton's method is set to one when the Hessian matrix is computed exactly.\n",
      "-----\n",
      "\n",
      "b5603743-8220-4d9f-bfeb-10dab2b8ae76\n",
      "The k-th component of the softmax function applied to an n-dimensional input vector, defined as the exponential of the k-th input divided by the sum of exponentials of all input components.\n",
      "-----\n",
      "\n",
      "bdb09bfa-398f-4db1-b5ba-e321c6dae35e\n",
      "the index indicating a specific component of the output vector produced by the softmax function, where the output is defined for each class or dimension\n",
      "-----\n",
      "\n",
      "d5e96d9e-2cda-4b87-a235-0dbc70d2624e\n",
      "The symbol in question refers to the Hessian matrix associated with the cross-entropy error function, which is discussed in the context of logistic regression and is noted to be positive semi-definite if the function is convex.\n",
      "-----\n",
      "\n",
      "875c14da-59b2-4447-824f-d704c7a0be25\n",
      "the input feature vector for which the probability of belonging to a particular class is being evaluated in the context of a categorical variable model\n",
      "-----\n",
      "\n",
      "e9241e79-3344-4702-9167-dfabbb8710db\n",
      "the collection of m basis functions, each representing a nonlinear function of the input variables, used in the feature mapping for generalized linear models\n",
      "-----\n",
      "\n",
      "3955fb55-0aee-465b-9aae-66d1a37d59a6\n",
      "the index used to denote a specific example in a dataset, typically ranging from 1 to N, where N is the total number of examples\n",
      "-----\n",
      "\n",
      "e5c13bc7-526d-4173-9279-cd46c5e14cfb\n",
      "The target expression refers to the Hessian matrix used in the context of optimizing the parameters of logistic regression, specifically in the application of Newton's method and the iteratively reweighted least squares (IRLS) algorithm.\n",
      "-----\n",
      "\n",
      "78141615-654c-4c43-a3c2-5c4881d517b2\n",
      "The target expression refers to the L one norm, which is used as a regularization term in logistic regression to encourage sparsity in the model parameters.\n",
      "-----\n",
      "\n",
      "40cafbef-37b1-456d-8634-9e4549b3928b\n",
      "the function of a vector variable whose convexity is being discussed in relation to the positive semi-definiteness of its Hessian matrix\n",
      "-----\n",
      "\n",
      "59e92971-5d6b-4817-862e-e2e5258724c3\n",
      "the weight vector associated with the k-th class in a multinomial logistic regression model\n",
      "-----\n",
      "\n",
      "ece6aa03-12eb-4ba4-a83a-a38b66ad8f79\n",
      "the mean parameter of a distribution in the exponential family, as defined by the mean function or activation function in the context of generalized linear models\n",
      "-----\n",
      "\n",
      "b6748ff0-9999-4b39-a4a1-544b9aa7d7f3\n",
      "The Hessian matrix associated with the cross-entropy error function in logistic regression, which is discussed in the context of its positive definiteness and invertibility.\n",
      "-----\n",
      "\n",
      "b63f4c41-3b4b-4a0a-8298-d94a5c232a2c\n",
      "The model associated with class indexed by k, which outputs the probability that an input belongs to class k, computed using the softmax function and dependent on the parameters of the model.\n",
      "-----\n",
      "\n",
      "7bfeb9ad-d2b5-4286-afba-11d48dbdf85d\n",
      "the point in n-dimensional space at which the function f is evaluated and at which the Hessian matrix is computed\n",
      "-----\n",
      "\n",
      "4dfa273f-bc91-41be-bd4a-9fc521bd5170\n",
      "the activation function that defines the mean parameter of a distribution from the exponential family in the context of generalized linear models\n",
      "-----\n",
      "\n",
      "7d6aa5aa-1c5f-4853-ae85-e88ae32d5d86\n",
      "the index representing a specific class in a multiclass classification problem, used to indicate which class's weights or outputs are being referenced or updated\n",
      "-----\n",
      "\n",
      "4a958b9d-3d76-415d-acb1-fe29bef278d8\n",
      "The target expression denotes a matrix composed of K column vectors, where each column is a weight vector associated with one of the K classes in a multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "fb9259e2-e70f-4869-8359-37154e90c2d1\n",
      "The probability that the k-th component of the indicator vector equals one, representing the probability of the multinomial variable taking the k-th state.\n",
      "-----\n",
      "\n",
      "57452ba7-9f49-4040-8cf3-513b4afea2cd\n",
      "The target expression denotes the mean parameter of a distribution, which is defined as a function of the activation function applied to a linear combination of weights and features.\n",
      "-----\n",
      "\n",
      "e635519d-5013-41ab-842d-4baa7d510d3b\n",
      "the error function, dependent on the weight parameters and conditioned on the dataset, which is minimized during model training\n",
      "-----\n",
      "\n",
      "285fa18f-1a8f-4930-bcb0-c101549563e3\n",
      "The target expression denotes the class label corresponding to the negative class in binary logistic regression, specifically indicating the outcome where the response variable equals zero.\n",
      "-----\n",
      "\n",
      "db5f5586-e59e-4683-b872-5508b5ae7430\n",
      "The target expression is a rightwards arrow symbol, used here to indicate the start of an example section following the discussion of the softmax function.\n",
      "-----\n",
      "\n",
      "e0ecbdc5-2355-49a3-91eb-17b538fdf915\n",
      "the index representing a specific class in a multinomial logistic regression model, used to indicate which class an example is being assigned to or considered for\n",
      "-----\n",
      "\n",
      "1573110e-eb42-4bb1-85b1-e7d62a5d5720\n",
      "The index representing a specific class in a multiclass classification setting, as referenced in the context of the softmax model and its associated output function.\n",
      "-----\n",
      "\n",
      "fa182606-97d8-4dc8-a3c8-c732e3d9d910\n",
      "the dimensionality of the vector produced by evaluating the softmax function over the set of class scores, corresponding to the number of classes\n",
      "-----\n",
      "\n",
      "0db8e74a-1bad-4ddd-9201-ad7e341e8893\n",
      "the regularization factor that appears on the diagonal of the diagonal matrix used in regularized optimization, except for the element corresponding to the unregularized weight\n",
      "-----\n",
      "\n",
      "b4b938db-278b-4536-bdf5-718f00d05f78\n",
      "The k-th component of the indicator vector representing the outcome of a multinomial variable is equal to one, indicating that the variable has taken the k-th state.\n",
      "-----\n",
      "\n",
      "0a42d315-5116-4b03-ae76-69244d8ccba0\n",
      "The dimension of the input variable space in the context of feature mapping, specifically indicating that the input vectors have two components.\n",
      "-----\n",
      "\n",
      "c9e0883f-e8b9-426b-816f-f2111f58c3db\n",
      "The scalar product of the weight vector associated with class k and the input vector, representing the unnormalized score for class k in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "36fc41bd-6e0b-4ed1-b8e4-b6cb4298b83a\n",
      "The target expression denotes a type of regularization referenced in the context of extending Newton's method to regularized cross-entropy error, specifically as a form of penalty added to the loss function to prevent overfitting in logistic regression.\n",
      "-----\n",
      "\n",
      "1aa27890-d99f-460e-b703-1ef5b78c6e15\n",
      "the index value corresponding to the outcome of a multinomial variable, such that the k-th component of the indicator vector is set to one when the variable takes this outcome\n",
      "-----\n",
      "\n",
      "ea6595ce-e199-4103-9f2d-07beb0db0d40\n",
      "The total number of weight vectors in the matrix used for multinomial logistic regression, corresponding to the number of classes in the classification problem.\n",
      "-----\n",
      "\n",
      "f613f567-f569-4f54-a667-430b43238aab\n",
      "The target expression denotes the model corresponding to class k in a multinomial logistic regression framework, where each such model outputs the probability that a given input belongs to class k, as defined by the softmax function.\n",
      "-----\n",
      "\n",
      "c41e4779-1934-460f-9c3b-88e241908a2f\n",
      "The target expression refers to the function being minimized, specifically at the stage after the first update, where a quadratic approximation is performed at the new point in the iterative optimization process.\n",
      "-----\n",
      "\n",
      "13505949-8a6f-4be3-8ffb-5455eda117c6\n",
      "A vector consisting of K parameters, each denoted by a subscripted symbol, representing the probabilities associated with each of K possible outcomes in a categorical distribution.\n",
      "-----\n",
      "\n",
      "7c256f06-eb7d-43f0-8f96-34f6363feb6c\n",
      "The variable y is a binary label that can take the value 0 or 1, indicating it is a Bernoulli random variable used in binary logistic regression.\n",
      "-----\n",
      "\n",
      "ea3ae73a-ba8f-4e84-97b2-fb9ca374cd7a\n",
      "This expression represents the parameter update rule in gradient descent, where the current value of the parameter vector is updated by subtracting the product of a learning rate and the gradient of the function being minimized, both evaluated at the current parameter value.\n",
      "-----\n",
      "\n",
      "e635e9d6-27cd-4d5f-90bb-cd9e9f47f5bb\n",
      "A function of a vector variable, representing a real-valued function that takes a vector as input, used in the context of multivariate Taylor expansion and Hessian matrix computation.\n",
      "-----\n",
      "\n",
      "443dbf5b-19d1-4db6-86e9-fe5800c9dc3f\n",
      "the parameter representing the probability of success in the Bernoulli distribution, that is, the probability that the binary variable equals one\n",
      "-----\n",
      "\n",
      "e02db433-d188-4e90-87aa-1e701fa5a542\n",
      "The target expression defines the Hessian matrix for the cross-entropy error in logistic regression as the product of the transpose of the design matrix, a diagonal matrix whose entries are related to the derivatives of the logistic function evaluated at each training example, and the design matrix itself.\n",
      "-----\n",
      "\n",
      "1a588c32-c43f-4ed0-ab73-b82875b1ce82\n",
      "The variable represents the index of a specific class in a classification problem with K classes, where each model h_k is responsible for predicting the probability that an input belongs to class indexed by this variable.\n",
      "-----\n",
      "\n",
      "fa3fcda4-e236-4786-9683-3feb702819cc\n",
      "The target expression represents the number of dimensions of the input vector for a function that maps vectors to scalars, specifically indicating the dimensionality of the domain of the function.\n",
      "-----\n",
      "\n",
      "86f968d5-f408-4e82-8177-2eaa3d99452f\n",
      "The target expression refers to the squared L2 norm regularization term that is added to the loss function or gradient in the context of regularized logistic regression, commonly used to penalize large weights and prevent overfitting.\n",
      "-----\n",
      "\n",
      "ec82544a-d47d-4dc8-b5a8-00b8b772dcc6\n",
      "The target expression denotes a function of a vector variable, specifically representing the function under consideration when its argument is a vector, as in the case of functions defined on multidimensional parameter spaces.\n",
      "-----\n",
      "\n",
      "2bcbb003-f4c8-44a6-99f7-5171e4a242f6\n",
      "This expression represents the dimensions resulting from multiplying a matrix with N rows by a matrix with N rows and m plus one columns, indicating the resulting matrix will have N rows and m plus one columns.\n",
      "-----\n",
      "\n",
      "beb1c264-3f0f-4216-b015-ed0d7a104c31\n",
      "the point at which the quadratic approximation of the function is constructed, specifically the location where the parabola is tangential to the function during the minimization process\n",
      "-----\n",
      "\n",
      "48a9a28e-f6d5-42e9-873d-20b6cb9d7be7\n",
      "The target expression refers to the function being minimized, specifically at the point where a quadratic approximation is constructed that is tangential to the function at a given starting point.\n",
      "-----\n",
      "\n",
      "f4e2235e-6df7-442c-ae3f-d8ca99cfcddb\n",
      "The value assigned to the polynomial degree parameter in the context of a polynomial feature mapping example, specifically indicating that the degree of the polynomial mapping is two.\n",
      "-----\n",
      "\n",
      "34a47439-233a-47a7-b50c-ad2ecc06b114\n",
      "The expression represents the iterative update rule for gradient descent, where the parameter vector at the next iteration is obtained by subtracting the product of a step size and the gradient of the function evaluated at the current parameter vector from the current parameter vector. The update is indexed by iteration number.\n",
      "-----\n",
      "\n",
      "96029822-5cd6-42dd-930c-cdf61f027cd2\n",
      "the five-dimensional vector whose components are 1, 2, 0.5, 5, and 3, used as an input example for the softmax function in the given context\n",
      "-----\n",
      "\n",
      "90d7a53c-ac6b-476d-9dba-18a944ef3918\n",
      "The probability that the k-th indicator variable in the vector representation of a multinoulli variable equals one, meaning the probability that the multinoulli variable takes the k-th outcome.\n",
      "-----\n",
      "\n",
      "af684f3c-5eaf-4922-acb6-50a5ee521704\n",
      "A square matrix whose entries are the second-order partial derivatives of a scalar-valued function of n variables, arranged so that the entry in the i-th row and j-th column is the second partial derivative of the function with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "65fb2832-2b07-4ba9-b120-9d1e230ae7a1\n",
      "The target expression denotes the collection of models, each indexed by k, that together constitute the overall model for multinomial logistic regression, with each individual model corresponding to a specific class.\n",
      "-----\n",
      "\n",
      "748ab077-ad51-43ce-9526-e7a14fa9a00f\n",
      "The expression refers to the model associated with class k in multinomial logistic regression, which is part of a set of models where each one is responsible for a specific class.\n",
      "-----\n",
      "\n",
      "7653b754-8773-48a0-8c93-f4172abbe7a6\n",
      "The mean parameter of a distribution is given as the dot product of a weight vector and a feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "5f682523-fec6-41d7-b9f0-399f6cb1bac1\n",
      "The target expression refers to a type of regularization commonly used in optimization, specifically the variant that penalizes the squared magnitude of model parameters.\n",
      "-----\n",
      "\n",
      "e570128d-9122-452e-a04f-a44d701df680\n",
      "The target expression denotes the model used in multinomial logistic regression, which is further specified as a set of models indexed by class, each responsible for predicting the probability of an input belonging to a particular class using the softmax function.\n",
      "-----\n",
      "\n",
      "fce9505e-4c43-484e-a080-c75cb173f351\n",
      "The vector representing a multinomial (multinoulli) variable with four possible states, where the third state is indicated as the outcome by a one in the third position and zeros elsewhere.\n",
      "-----\n",
      "\n",
      "ffa345ed-073b-454e-9306-41b6ba3d2c74\n",
      "The target expression denotes the weight vector associated with class k, which is one of the K weight vectors that together form the weight matrix used in the softmax regression model.\n",
      "-----\n",
      "\n",
      "fe5876d9-bfc1-48bf-8600-0ef163d87a21\n",
      "This expression states that the sum of all elements in the parameter vector, where each element represents a probability associated with a category in a categorical distribution, must equal one.\n",
      "-----\n",
      "\n",
      "6950d8fb-de24-44b2-b4a9-6a117a137b1e\n",
      "the integer one less than the class index k, representing the set of all other classes except class k\n",
      "-----\n",
      "\n",
      "3849aebd-35a8-4a1a-ba05-4ff57149748a\n",
      "The expression asserts that for every non-zero vector, the quadratic form defined by the transpose of the vector, the Hessian matrix, and the vector itself is strictly greater than zero, which is the condition for the Hessian matrix to be positive definite.\n",
      "-----\n",
      "\n",
      "961ea654-380a-46f7-976d-e9cc7b456a01\n",
      "The gradient of the regularized error function with respect to the weight vector, given the dataset, is equal to the sum of the gradient of the unregularized error function with respect to the weight vector and the product of the regularization parameter with the weight vector.\n",
      "-----\n",
      "\n",
      "3d9c714e-316b-40c0-a574-c190c24b3d22\n",
      "The target expression represents a function of a single variable x, depicted as the black curve in a minimization problem, where the goal is to find the minimum value of this function using iterative methods such as Newton's method.\n",
      "-----\n",
      "\n",
      "8ad8dc7d-1893-43bc-9909-bff54781732d\n",
      "a non-zero vector used in the context of testing whether a matrix is positive definite, specifically in the condition that the quadratic form with the matrix is strictly greater than zero for each such vector\n",
      "-----\n",
      "\n",
      "4f04f9cd-5f52-4742-a8b5-9889dc8496d8\n",
      "The sum over all components of the indicator vector representing the categorical variable equals one, reflecting that exactly one outcome occurs among the mutually exclusive and collectively exhaustive possible states.\n",
      "-----\n",
      "\n",
      "828d726d-225f-4597-b596-bca93261e5b5\n",
      "The target expression gives the result of applying the softmax function to the vector with components 1, 2, 0.5, 5, and 3, yielding a five-dimensional vector of normalized values that sum to one.\n",
      "-----\n",
      "\n",
      "a36a35ae-0979-4adc-81e4-61301ef1d7d9\n",
      "The target expression denotes a multivariate function that maps an n-dimensional vector input to a scalar output, and is the function with respect to which the Hessian matrix is defined at a given point in the context of a quadratic expansion.\n",
      "-----\n",
      "\n",
      "9d039f02-a902-4154-9419-7916d8dfd3fe\n",
      "The index representing a particular class in a multi-class classification setting, used to specify which class is being referred to for a given example.\n",
      "-----\n",
      "\n",
      "d1089303-bdf4-4f67-9792-7e2a53f0e56c\n",
      "A function that maps an n-dimensional real vector to an (m+1)-dimensional real vector, where the output vector consists of the scalar 1 followed by the values of m basis functions evaluated at the input vector.\n",
      "-----\n",
      "\n",
      "94e98347-dc35-4e53-8a65-7ff6adf95733\n",
      "This expression states that the k-th component of the indicator vector representing a categorical (multinoulli) variable is zero, which occurs when the outcome of the variable is not the k-th category.\n",
      "-----\n",
      "\n",
      "0d36c960-aac7-475e-baf8-501f78226911\n",
      "This refers to a quadratic function used in the context of optimization, specifically as an expansion or approximation of a general function for the purpose of analyzing or finding its minimum using second-order methods such as Newton's method.\n",
      "-----\n",
      "\n",
      "18b1db1b-58da-43cf-b21c-ff40b4fcfd4d\n",
      "The target expression represents one of the two possible values that a Bernoulli random variable can take, specifically the value corresponding to the 'failure' or 'negative' outcome, as part of the definition of the Bernoulli distribution.\n",
      "-----\n",
      "\n",
      "ae3ae68a-c52e-4c2c-9232-de8fd7da0c1a\n",
      "The target expression denotes the event or class label where the variable y takes the value one, as used in the context of binary logistic regression classification.\n",
      "-----\n",
      "\n",
      "f596ec8d-474f-43de-b640-8d7f77771a15\n",
      "the total number of classes considered in the generalization of the cross-entropy error function, as indicated by the upper limit of the summations over class index in the loss and error expressions\n",
      "-----\n",
      "\n",
      "5e5eb05c-f2f1-4269-b808-49fc87f50b2f\n",
      "The target expression gives the Taylor series expansion of a differentiable function about a point, showing both the explicit sum of terms involving derivatives of the function at that point and the equivalent summation notation for the infinite series.\n",
      "-----\n",
      "\n",
      "b591e2c3-a412-4e4e-92ce-f6fc17285459\n",
      "The expression represents an iterative update rule for a parameter vector at iteration t, where the next value is obtained by subtracting the product of a step size, the inverse of the Hessian matrix evaluated at the current parameter vector, and the gradient of a function evaluated at the current parameter vector from the current parameter vector.\n",
      "-----\n",
      "\n",
      "3062e52e-42f1-44ef-9cf6-efa17d9bae96\n",
      "The expression defines a generalized linear model where the output is obtained by applying an activation function to a linear combination of basis functions of the input, with the basis functions and their coefficients indexed from zero to m.\n",
      "-----\n",
      "\n",
      "0a11fbc2-9596-483a-b063-cd283e5eb01b\n",
      "The label indicator for example i and class k, which takes the value 1 if example i belongs to class k and 0 otherwise, as used in the context of multi-class classification and cross-entropy loss.\n",
      "-----\n",
      "\n",
      "c133771c-6e2e-45a8-a075-61b72c9aca17\n",
      "the Hessian matrix whose invertibility is required for applying Newton's optimization method, specifically in the context where it is positive semi-definite but not positive definite, resulting in the absence of an inverse\n",
      "-----\n",
      "\n",
      "0632e594-4b61-465e-938c-24e4eb819bfc\n",
      "the dataset consisting of input-output pairs, where the outputs are the labels whose probability is being modeled and whose log-probability is being computed in the subsequent expressions\n",
      "-----\n",
      "\n",
      "12bb4ad9-7b8e-490b-a765-f8261081e441\n",
      "the point in the domain of the multivariate function f, about which the quadratic (second-order) Taylor expansion is performed; it is a vector representing the expansion center in the context of the Taylor series for functions of several variables\n",
      "-----\n",
      "\n",
      "209aaa2f-a8f1-421e-8148-e639dbfcc905\n",
      "the number of possible values or categories for the variable being modeled, which generalizes the Bernoulli case to a categorical case\n",
      "-----\n",
      "\n",
      "8ba4491a-d6b3-446d-b5d3-a0ee705e4719\n",
      "The expression indicates that the number of possible values or categories, denoted by K, is greater than two, in the context of generalizing from a Bernoulli (two-category) variable to a categorical variable with more than two categories.\n",
      "-----\n",
      "\n",
      "e03a6b39-8425-4070-bb94-1f2ef931e4c3\n",
      "The expression represents a linear combination of feature mappings of an input vector, where a weight vector is transposed and multiplied by a feature vector derived from the input.\n",
      "-----\n",
      "\n",
      "88a9d567-cd84-4748-b237-9c82dcae85db\n",
      "The target expression gives the probability mass function for a categorical random variable, where the probability of observing a one-hot encoded outcome vector is expressed as the product over all classes of the class probabilities raised to the power of the corresponding indicator variables.\n",
      "-----\n",
      "\n",
      "8a9256fc-a695-4eec-82c2-dd3fd653810b\n",
      "The target expression represents the dimensions of the Hessian matrix associated with the cross-entropy error in logistic regression, indicating that the matrix has (m plus one) rows and (m plus one) columns, where m is the number of features in the feature space after mapping.\n",
      "-----\n",
      "\n",
      "9a2c0ef1-ca32-4a2a-ada2-53ceeb691b27\n",
      "A matrix multiplication where the first matrix has dimensions m plus one by N, and the second matrix has dimensions N by an unspecified number, indicating a check of matrix compatibility for multiplication in the context of Hessian matrix construction.\n",
      "-----\n",
      "\n",
      "13f66829-ed08-4a26-b1cc-5df7b812232d\n",
      "This expression represents the weight update rule in stochastic gradient descent for a model, where the current weight vector is adjusted by subtracting the product of the learning rate, the difference between the model's prediction for a specific input and the true label, and the feature vector of that input.\n",
      "-----\n",
      "\n",
      "7c21b461-d184-4343-80cf-f281598dc016\n",
      "The label associated with the i-th data point in a dataset, as referenced in the context of probabilistic modeling and distributions used for modeling labels.\n",
      "-----\n",
      "\n",
      "fdbda175-6069-4b19-9d98-e67463a5e65b\n",
      "the Hessian matrix in the context of Newton's method, specifically referring to the case where it is positive semi-definite but not positive definite, and thus does not have an inverse\n",
      "-----\n",
      "\n",
      "5e1d95e4-ec96-47bd-b8e0-c75182b78a10\n",
      "The expression represents the gradient of the quadratic error function for linear regression, evaluated with respect to the weight vector, for a single training example. It is given as the product of the prediction error for that example and the feature vector corresponding to the example.\n",
      "-----\n",
      "\n",
      "6dfeb566-64e7-4e0d-b8d4-e0f44c540b15\n",
      "A function labeled with subscript j that maps an n-dimensional real vector to a real number, representing one of a set of m basis functions used in feature mapping for generalized linear models.\n",
      "-----\n",
      "\n",
      "fecf809f-3185-486b-8639-2cd52bc3cfbb\n",
      "the natural logarithm of the model's predicted probability for a given input, in the case where this probability is close to one, resulting in the logarithm being approximately zero\n",
      "-----\n",
      "\n",
      "6753b0f1-42d6-46dc-bd19-d41726bfbb3a\n",
      "The target expression denotes the activation function used in the context of generalized linear models, which serves as the mean function mapping the linear predictor to the mean parameter of a distribution from the exponential family.\n",
      "-----\n",
      "\n",
      "b2e88e83-613f-4729-9e77-e73594148d77\n",
      "the activation function used in the generalized linear model, which is applied to the linear combination of the weights and the feature mapping of the input\n",
      "-----\n",
      "\n",
      "cc97fc37-254d-4b10-bab1-d0344c7e818a\n",
      "The k-th element of a parameter vector representing the probability assigned to the k-th category in a categorical distribution, where each element is non-negative and the elements sum to one.\n",
      "-----\n",
      "\n",
      "a05e935e-dd2d-4487-abb6-d0139fa9df2c\n",
      "The update rule for the weight vector in Newton's method applied to logistic regression, where the new weight vector is obtained by subtracting the product of the inverse of the Hessian matrix of the error function with respect to the weights and the gradient of the error function with respect to the weights, evaluated on the dataset, from the current weight vector, with the step size set to one.\n",
      "-----\n",
      "\n",
      "63ceb296-0afa-431c-a564-3b4fb7eec2c8\n",
      "This expression represents the second-order Taylor approximation of a differentiable function at a point, using the value of the function, its first derivative, and its second derivative at that point, and is obtained by truncating the Taylor series after the quadratic term.\n",
      "-----\n",
      "\n",
      "3377ad1e-6ac8-4acf-8d2c-aead139ff034\n",
      "The target expression denotes the input vector corresponding to the i-th training example in the dataset, as referenced in the context of linear regression and stochastic gradient descent.\n",
      "-----\n",
      "\n",
      "833ee48a-a40e-4bf4-9068-b92442e971e2\n",
      "The target expression represents the result of taking the dot product between the weight vector associated with class k and the input example vector, producing a scalar value that serves as the input to the softmax function for class k.\n",
      "-----\n",
      "\n",
      "da4d43a7-fffd-4f65-af68-067c5d203a09\n",
      "The target expression denotes a function, referenced in the context as the objective function being minimized, and is considered as a function of a single variable in the discussion of Newton's method.\n",
      "-----\n",
      "\n",
      "196334ad-8d56-4fce-aab9-4976c038ac79\n",
      "The target expression defines a general form for the output of a generalized linear model, where the model's prediction for an input vector and weight vector is given by applying an activation function to the dot product of the weight vector and a feature transformation of the input vector.\n",
      "-----\n",
      "\n",
      "e7d7e414-4d28-4a1c-b7b4-14e2ace71ae2\n",
      "The target expression represents the gradient of the error function with respect to the weight vector for class k. It is given as a sum over all training examples, where each term in the sum is the difference between the model's predicted value for class k and the true label for class k, multiplied by the feature vector of the corresponding example.\n",
      "-----\n",
      "\n",
      "f9d2b1ef-2681-45a7-9f43-f492e7ef6419\n",
      "This expression represents the error function to be minimized in a multi-class classification setting, defined as the negative sum over all training examples and all classes of the product of the true label indicator and the logarithm of the predicted probability for each class. It generalizes the cross-entropy loss to the case of multiple classes.\n",
      "-----\n",
      "\n",
      "76a55aa5-bec4-442c-a02c-54330d6ed2cf\n",
      "The target expression defines the entry in the i-th row and j-th column of the Hessian matrix for a function, specifying that this entry is the second partial derivative of the function with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "e2d3b00e-ba2a-4b69-9ea3-ea1957feca47\n",
      "The target expression denotes a column vector whose entries are indicator variables corresponding to each of the K possible outcomes of a categorical variable, where each entry is binary and only one entry is equal to one, indicating the observed outcome.\n",
      "-----\n",
      "\n",
      "345cfa7b-dc71-4679-b2b4-f47a05aa9871\n",
      "The target expression refers to a type of regularization commonly used in optimization and machine learning, which is distinct from the previously discussed L2 regularization. It is associated with a norm that is not differentiable, making gradient-based optimization more challenging and often requiring alternative methods such as subgradient computation or coordinate descent.\n",
      "-----\n",
      "\n",
      "307066f4-d4e0-43b2-9f7a-0c98735e1e0e\n",
      "This expression defines the loss function for a single example in a K-class classification problem, where the loss is computed as the negative sum over all classes of the product of the true label indicator for each class and the logarithm of the predicted probability for that class, as given by the model's output.\n",
      "-----\n",
      "\n",
      "291c787d-b5d4-444a-9abe-7794512c3840\n",
      "This expression defines the output of a binary logistic regression model as the probability that the class label equals one, given an input vector and a weight vector. It shows that this probability is computed by applying the sigmoid function to the inner product of the weight vector and a feature transformation of the input.\n",
      "-----\n",
      "\n",
      "8dc8c7c8-9bb2-4ed4-b627-20ffe1fca56c\n",
      "This expression gives the second-order Taylor expansion of a multivariate function about a point, where the function at a general vector is approximated by the sum of its value at a reference point, the inner product of its gradient at that point with the displacement from the reference, and one half times the quadratic form involving the Hessian matrix at the reference point and the displacement vector.\n",
      "-----\n",
      "\n",
      "ba8ec022-fa36-452e-a0cf-84d3e28964b6\n",
      "The condition that each parameter in the parameter vector, indexed by k, must be greater than or equal to zero, reflecting the requirement that these parameters represent probabilities.\n",
      "-----\n",
      "\n",
      "317dd1b9-2511-4d1d-a034-d01a110a0e67\n",
      "The target expression represents the weight update rule for stochastic gradient descent applied to the quadratic error function of linear regression, where the current weight vector is updated by subtracting the product of the learning rate and the gradient of the error with respect to a single training example.\n",
      "-----\n",
      "\n",
      "f251a305-6900-466c-8ed6-7e3ad3b3f35d\n",
      "The target expression defines the quadratic error function for linear regression, representing the sum over all N data points of the squared difference between the predicted value, given by the transpose of a weight vector multiplied by a feature mapping of the input, and the actual target value, with the sum scaled by one half.\n",
      "-----\n",
      "\n",
      "01740c91-48eb-4515-ab5d-f64733c59478\n",
      "The target expression gives the probability mass function of a Bernoulli random variable, where the probability of success is parameterized by a variable representing the probability of the outcome being one, and the probability of failure is one minus this parameter. The formula expresses the probability of observing a particular outcome, which can be either zero or one, as a function of the parameter.\n",
      "-----\n",
      "\n",
      "e9c4e34c-98a1-40ce-a135-87c5cc57420a\n",
      "The target expression gives the probability that an input vector belongs to class k under a multinomial logistic regression model, computed as the exponential of the dot product between the weight vector for class k and a feature transformation of the input, divided by the sum of such exponentials over all classes; this is also known as the softmax probability for class k.\n",
      "-----\n",
      "\n",
      "dd94d5ef-4bc8-47c5-8f7d-843b3b28520b\n",
      "The target expression refers to a type of regularization, specifically the one that is contrasted with another regularization method in the context of optimization techniques such as gradient descent, Newton's method, and least squares for linear regression. It is described as being straightforward to incorporate into these methods.\n",
      "-----\n",
      "\n",
      "4f976ad6-a861-4300-b523-b3aba86c37a9\n",
      "This expression defines a specific feature mapping for a two-dimensional input vector, where the mapping outputs a vector consisting of the constant one, the two input components, their product, and the squares of each component. This corresponds to a polynomial feature mapping of degree two for two input variables.\n",
      "-----\n",
      "\n",
      "47d91399-f2b5-46d8-8a32-0a49e76b6ff0\n",
      "The target expression denotes the input example vector whose class membership probability is being evaluated by the multinomial logistic regression model. It is the same vector as previously defined in the context, specifically the one with components 1, 2, 0.5, 5, and 3.\n",
      "-----\n",
      "\n",
      "a012e7aa-81ce-46fb-8530-61d4913de753\n",
      "The expression represents a matrix product where the transpose of a matrix denoted by a bold capital Greek letter is multiplied on the left, a matrix denoted by a bold capital Latin letter is in the middle, and the original (non-transposed) bold capital Greek letter matrix is on the right. This product is used as a substitute for the Hessian matrix in the context of logistic regression optimization and is involved in the derivation of the iteratively reweighted least squares algorithm.\n",
      "-----\n",
      "\n",
      "ccef6221-6043-4fa8-a310-40319684fc8f\n",
      "The target expression denotes the j-th basis function used in the mapping of input variables within a generalized linear model, where these basis functions can be any functions of the input, not limited to polynomials or products of input features.\n",
      "-----\n",
      "\n",
      "b6daf5f5-f721-4d84-99c9-27b6d54d7242\n",
      "The target expression denotes the weight parameter associated with the i-th input variable in the weight vector of the j-th basis function within a generalized linear model that uses basis functions, where each basis function is itself parameterized by its own set of weights.\n",
      "-----\n",
      "\n",
      "36f5c1ad-2426-47b5-88d5-85c4d91e79a0\n",
      "The target expression refers to a specific type of regularization mentioned in the context, which is contrasted with another type of regularization and is described as being easily incorporated into optimization methods such as gradient descent, Newton's method, and least squares for linear regression.\n",
      "-----\n",
      "\n",
      "919f305a-69db-41de-bc20-37ff24c87214\n",
      "This expression presents four key components of the linear regression model: (1) the model output as a linear function of a feature transformation and weights, (2) the conditional probability of the target given the input and weights as a normal distribution with mean equal to the model output and fixed variance, (3) the loss function as the squared difference between the model output and the target, and (4) the gradient of the loss with respect to the weights as the product of the prediction error and the feature transformation.\n",
      "-----\n",
      "\n",
      "164b8f8c-b978-404f-a996-a532e4f73cac\n",
      "the index of the weight within the weight vector corresponding to a particular basis function\n",
      "-----\n",
      "\n",
      "6d06393b-0b0c-4582-b1e7-d4ca38b6c2f9\n",
      "the index that identifies a specific basis function within the model, used to distinguish between different basis functions and their associated parameters\n",
      "-----\n",
      "\n",
      "c640dc33-5ddf-4ae2-b825-d56f01d74090\n",
      "This expression defines the probability that an input vector belongs to class k, given the input and a set of parameters, as the k-th component of a probability vector. It equates this probability to the output of a model function, which is computed by exponentiating a linear transformation of a feature mapping of the input for class k, and normalizing by the sum of such exponentials over all classes. This is the softmax function used in multiclass classification.\n",
      "-----\n",
      "\n",
      "174bde9b-fcb6-4f08-9578-9ea32a22dd1e\n",
      "This is a set of four equations that together define the logistic regression algorithm: the first equation gives the model output as the sigmoid of a linear function of the input features, which is also the probability of the positive class; the second equation expresses the probability of the label given the input and parameters as a Bernoulli distribution parameterized by the model output; the third equation defines the loss function as the negative log-likelihood (cross-entropy) for a binary label; and the fourth equation gives the gradient of the loss with respect to the model parameters, showing it as the product of the prediction error and the feature vector.\n",
      "-----\n",
      "\n",
      "02562926-1fdb-4f17-8e7a-b9a85d545ad2\n",
      "The target expression defines a function of an input vector and a parameter vector, where the function is constructed by first applying a linear transformation to the input using a matrix of weights, then applying a nonlinear activation function elementwise, followed by another linear combination with a second set of weights, and finally applying the activation function again. The expression also shows that each basis function is itself a generalized linear model, and the overall function is a composition of these basis functions and another generalized linear model.\n",
      "-----\n",
      "\n",
      "f639c277-167d-4286-a074-d16216596fc3\n",
      "This expression gives the logarithm of the probability of the set of labels given the set of inputs, expressed in several equivalent forms: as the log of the product over all data points of the probability of each label given its input, as the log of the product over all data points and classes of the class probabilities raised to the power of the corresponding label indicator, as the log of the product over all data points and classes of the model's predicted probabilities raised to the label indicators, and finally as the sum over all data points and classes of the label indicators times the log of the predicted probabilities.\n",
      "-----\n",
      "\n",
      "a3caf3fb-f92e-43ff-b23c-a5c546da8213\n",
      "This expression defines the general form of a probability distribution belonging to the exponential family, where the probability of a random variable given parameters is expressed as a product of a base measure and an exponential function involving the parameters, sufficient statistics, and a log-partition function.\n",
      "-----\n",
      "\n",
      "3d41d5a3-86f0-4bb5-9a3e-89193238fae2\n",
      "the index that identifies a specific basis function within the model, used to distinguish between different weight vectors and basis functions\n",
      "-----\n",
      "\n",
      "6a394e3e-7c98-40b7-9f7e-f5ca3965a25b\n",
      "This is a set of four expressions related to multinomial logistic regression: (1) the predicted probability for class k given an input and weight matrix, defined via the softmax function and also interpreted as the conditional probability of class k; (2) the probability of a one-hot encoded label vector given the input and weights, expressed as a product over classes of predicted probabilities raised to the power of the corresponding label indicator; (3) the loss function, given by the negative sum over classes of the label indicators times the logarithm of the predicted probabilities; and (4) the gradient of the loss with respect to the weight vector for class k, given by the difference between the predicted probability and the label indicator, times the feature vector.\n",
      "-----\n",
      "\n",
      "5551377f-dc29-4b1b-9b72-0173f6d5d141\n",
      "The superscript label used to distinguish the second set of weights in the model, indicating their position in the sequence of operations for the model prediction.\n",
      "-----\n",
      "\n",
      "76a71c57-b440-4d40-816e-ff820d776497\n",
      "The superscript label used to distinguish the first set of weights in the model, specifically those used in the initial stage of the model's prediction calculation.\n",
      "-----\n",
      "\n",
      "b29593a6-e223-4b6a-b80a-4a3dc82eb071\n",
      "the weight matrix associated with the first layer of the model, where each column or row corresponds to the weights applied to the input features for a specific basis function\n",
      "-----\n",
      "\n",
      "4a2d5957-8d89-4417-95c0-0fa17f69b8e8\n",
      "The target expression denotes a vector of weights that are used to combine the outputs of the basis functions (which themselves are computed using another weight matrix applied to the input features) in a two-layer neural network model.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in math_expression_descriptions:\n",
    "    print(x.id)\n",
    "    print(x.text)\n",
    "    print('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ebf8e",
   "metadata": {},
   "source": [
    "### 4. MathExpressionDescriptionOpt, requires: MathExpressionDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55912beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionDescriptionOptimizer as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionDescriptionOpt\n",
    "\n",
    "\n",
    "inputs, input_id_to_math_expression_description = InputCreatorUtil.create(\n",
    "    math_expression_descriptions, lambda x: AssistantInput(description=x.text)\n",
    ")\n",
    "outputs = await math_expression_description_optimizer_assistant.concurrent_assist(inputs)\n",
    "math_expression_descriptions_opt = [\n",
    "    MathExpressionDescriptionOpt(\n",
    "        math_expression_id=input_id_to_math_expression_description[\n",
    "            output.input_id\n",
    "        ].math_expression_id,\n",
    "        math_expression_description_id=input_id_to_math_expression_description[output.input_id].id,\n",
    "        math_expression_index_id=index.id,\n",
    "        text=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf0aa5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57cedca5-a07e-46a8-a814-9c63d020e2c2\n",
      "A classification problem with more than two classes.\n",
      "-----\n",
      "\n",
      "575cc528-f0d7-4adb-a103-4c736ca7d0ee\n",
      "Number of feature space dimensions.\n",
      "-----\n",
      "\n",
      "cf7b23bd-c558-402f-9792-e6a609a703d1\n",
      "Number of basis functions used for feature mapping in generalized linear models.\n",
      "-----\n",
      "\n",
      "fb890d36-4a5f-45f1-97a2-33e2a8250eac\n",
      "square matrix with n rows and n columns\n",
      "-----\n",
      "\n",
      "e077e3f3-c74e-47a5-a630-c334e4e6e2bb\n",
      "general differentiable function for Taylor series expansion at a point\n",
      "-----\n",
      "\n",
      "6a139fd0-2530-4eb2-88ee-3c435d363a42\n",
      "Hessian matrix of the function, evaluated at point t\n",
      "-----\n",
      "\n",
      "f12212c9-b711-4ef0-9d2a-d4f2910a6487\n",
      "A function mapping an n-dimensional real vector to an n-dimensional real vector.\n",
      "-----\n",
      "\n",
      "276b5a23-68c5-41e1-859d-7779797f1e1b\n",
      "parameter vector value at iteration t in iterative optimization process\n",
      "-----\n",
      "\n",
      "3b3c1871-78ed-49b7-ba60-5cf60782112f\n",
      "the point at which a differentiable function's Taylor series is expanded\n",
      "-----\n",
      "\n",
      "e72fe7f3-1020-4674-b66e-4f656c07399b\n",
      "Total number of classes in a classification problem, as referenced in the context of the softmax function.\n",
      "-----\n",
      "\n",
      "59e92971-5d6b-4817-862e-e2e5258724c3\n",
      "weight vector for the k-th class in a multinomial logistic regression model\n",
      "-----\n",
      "\n",
      "09feb954-e55a-4742-aecc-72bd8da5bb1b\n",
      "Number of classes in a multinomial logistic regression model, each with its own weight vector.\n",
      "-----\n",
      "\n",
      "85ac7049-1b86-4e78-88fb-c587439a902d\n",
      "Predicted probability from logistic regression, calculated by applying the logistic function to a generic input vector.\n",
      "-----\n",
      "\n",
      "f0a70670-5382-4058-aa2a-11e8de25fd19\n",
      "Number of examples in the training set.\n",
      "-----\n",
      "\n",
      "59bfb940-78b7-4150-a74e-9ce06bc9000e\n",
      "In multiclass regression, the number of possible output classes exceeds two.\n",
      "-----\n",
      "\n",
      "e5fb6dd0-b1c1-4dea-995c-7c7c7296eac9\n",
      "the point at which a differentiable function's Taylor series is expanded\n",
      "-----\n",
      "\n",
      "b31742b1-1009-448c-b420-c17accc18c67\n",
      "Function of a single variable minimized using quadratic approximation at the starting point.\n",
      "-----\n",
      "\n",
      "4dfa273f-bc91-41be-bd4a-9fc521bd5170\n",
      "Activation function specifying the mean parameter of an exponential family distribution in generalized linear models.\n",
      "-----\n",
      "\n",
      "ece6aa03-12eb-4ba4-a83a-a38b66ad8f79\n",
      "Mean parameter of an exponential family distribution, defined by the mean or activation function in generalized linear models.\n",
      "-----\n",
      "\n",
      "b58e2409-c55f-4e30-b788-1fca9d2cbc80\n",
      "Function minimized using quadratic approximation at each iteration to determine next search point.\n",
      "-----\n",
      "\n",
      "7bfeb9ad-d2b5-4286-afba-11d48dbdf85d\n",
      "Point in n-dimensional space where function f is evaluated and Hessian matrix is computed.\n",
      "-----\n",
      "\n",
      "4e67ef8b-48cd-46c4-ad12-bd3685414f85\n",
      "Weight parameter corresponding to the upper-left element of the parameter vector, excluded from L2 regularization.\n",
      "-----\n",
      "\n",
      "179b2530-f047-4c9f-b055-fe1cd7de5738\n",
      "The Hessian matrix of the cross-entropy error function in logistic regression may be ill-conditioned due to multicollinearity.\n",
      "-----\n",
      "\n",
      "2395ef70-240f-47be-9ede-ed62e1410db8\n",
      "Symbol denoting a non-zero vector used to test the positive semi-definiteness of the Hessian matrix.\n",
      "-----\n",
      "\n",
      "f75d9a87-588b-4778-af93-f5074f8bce70\n",
      "Index of a class in a classification problem with K classes, indicating the class to which an example belongs.\n",
      "-----\n",
      "\n",
      "e0ecbdc5-2355-49a3-91eb-17b538fdf915\n",
      "Index representing a specific class in a multinomial logistic regression model, indicating which class an example is assigned to or considered for.\n",
      "-----\n",
      "\n",
      "875c14da-59b2-4447-824f-d704c7a0be25\n",
      "Input feature vector evaluated for class membership probability in a categorical variable model.\n",
      "-----\n",
      "\n",
      "b6748ff0-9999-4b39-a4a1-544b9aa7d7f3\n",
      "Hessian matrix of the cross-entropy error function in logistic regression, with focus on positive definiteness and invertibility.\n",
      "-----\n",
      "\n",
      "32328d67-7f1a-4409-b361-e5b292dbc8f7\n",
      "Error function dependent on vector variable w, conditioned on dataset D.\n",
      "-----\n",
      "\n",
      "fb9259e2-e70f-4869-8359-37154e90c2d1\n",
      "Probability that the k-th component of the indicator vector equals one, representing the probability of the multinomial variable taking the k-th state.\n",
      "-----\n",
      "\n",
      "e9241e79-3344-4702-9167-dfabbb8710db\n",
      "Collection of m basis functions, each representing a nonlinear function of input variables, used in feature mapping for generalized linear models.\n",
      "-----\n",
      "\n",
      "db3e5e24-8ba8-4d23-b417-72957fb3d84b\n",
      "Hessian matrix required to be positive definite for invertibility in Newton's optimization method.\n",
      "-----\n",
      "\n",
      "e635e9d6-27cd-4d5f-90bb-cd9e9f47f5bb\n",
      "Real-valued function of a vector variable, used in multivariate Taylor expansion and Hessian matrix computation.\n",
      "-----\n",
      "\n",
      "e5c13bc7-526d-4173-9279-cd46c5e14cfb\n",
      "Hessian matrix in logistic regression parameter optimization, applied in Newton's method and iteratively reweighted least squares (IRLS) algorithm.\n",
      "-----\n",
      "\n",
      "e8c1f1fa-5661-4247-8620-fd51f5b175d0\n",
      "Objective function of a vector variable to be minimized in optimization methods including gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "78141615-654c-4c43-a3c2-5c4881d517b2\n",
      "L1 norm, used as a regularization term in logistic regression, encourages sparsity in model parameters.\n",
      "-----\n",
      "\n",
      "c9e0883f-e8b9-426b-816f-f2111f58c3db\n",
      "Scalar product of the weight vector for class k and the input vector, representing the unnormalized score for class k in multinomial logistic regression.\n",
      "-----\n",
      "\n",
      "5f682523-fec6-41d7-b9f0-399f6cb1bac1\n",
      "Regularization method in optimization that penalizes the squared magnitude of model parameters.\n",
      "-----\n",
      "\n",
      "1aa27890-d99f-460e-b703-1ef5b78c6e15\n",
      "Index value for a multinomial variable outcome, where the k-th component of the indicator vector is one if the variable equals this outcome.\n",
      "-----\n",
      "\n",
      "1573110e-eb42-4bb1-85b1-e7d62a5d5720\n",
      "Index of a specific class in multiclass classification, as referenced in the softmax model and its output function.\n",
      "-----\n",
      "\n",
      "44fa4563-06de-4bba-a64d-2d7c2daa2eab\n",
      "Hessian matrix with (m + 1) rows and (m + 1) columns.\n",
      "-----\n",
      "\n",
      "6d2231cb-cc4f-40cd-b91c-44387adfa86f\n",
      "Hessian matrix in Newton's optimization method, characterized by positive semi-definiteness and positive definiteness properties.\n",
      "-----\n",
      "\n",
      "3d8ba9cb-7ddb-4e4e-8575-efe40d1d9894\n",
      "The value representing the outcome one of a Bernoulli random variable, which can take values zero or one.\n",
      "-----\n",
      "\n",
      "7c256f06-eb7d-43f0-8f96-34f6363feb6c\n",
      "y is a binary label taking values 0 or 1, representing a Bernoulli random variable used in binary logistic regression.\n",
      "-----\n",
      "\n",
      "bdb09bfa-398f-4db1-b5ba-e321c6dae35e\n",
      "Index specifying a component of the output vector from the softmax function, with each component corresponding to a class or dimension.\n",
      "-----\n",
      "\n",
      "0a42d315-5116-4b03-ae76-69244d8ccba0\n",
      "Input variable space dimension in feature mapping, indicating input vectors have two components.\n",
      "-----\n",
      "\n",
      "7185716c-65e6-498e-b294-58b20ea48a63\n",
      "Norm associated with L1 regularization; not differentiable; requires subgradient methods or specialized optimization techniques.\n",
      "-----\n",
      "\n",
      "48a9a28e-f6d5-42e9-873d-20b6cb9d7be7\n",
      "The target expression is the function being minimized, with a quadratic approximation constructed tangentially at a given starting point.\n",
      "-----\n",
      "\n",
      "1cfb0ba4-2463-41c7-a051-236a6a90d0e5\n",
      "Value of a function at variable x approximated by a Taylor series expansion about point a.\n",
      "-----\n",
      "\n",
      "fa65176a-773f-4a75-9004-30e9803c628d\n",
      "Number of features in the feature space after mapping, corresponding to the dimensions of the Hessian matrix.\n",
      "-----\n",
      "\n",
      "dd634378-72b6-449f-aa96-f6cd838de3f4\n",
      "Initial point for starting the minimization process when minimizing a single-variable function using Newton's method.\n",
      "-----\n",
      "\n",
      "beb0450f-62b3-468e-81f7-e8666c93c8fc\n",
      "the point obtained after the first step of minimizing the quadratic approximation of a function, following the initial point in an iterative minimization process\n",
      "-----\n",
      "\n",
      "7b553a8f-a347-4826-91e6-0d4275762b8d\n",
      "Hessian matrix of the cross-entropy error function with dimensions (m+1) by (m+1), where m is the number of features after mapping.\n",
      "-----\n",
      "\n",
      "fe5876d9-bfc1-48bf-8600-0ef163d87a21\n",
      "The sum of all elements in the parameter vector, each representing a probability for a category in a categorical distribution, must equal one.\n",
      "-----\n",
      "\n",
      "f4e2235e-6df7-442c-ae3f-d8ca99cfcddb\n",
      "Polynomial degree parameter set to two in a polynomial feature mapping example, indicating a degree-two polynomial mapping.\n",
      "-----\n",
      "\n",
      "40cafbef-37b1-456d-8634-9e4549b3928b\n",
      "Function of a vector variable with convexity determined by the positive semi-definiteness of its Hessian matrix.\n",
      "-----\n",
      "\n",
      "7d6aa5aa-1c5f-4853-ae85-e88ae32d5d86\n",
      "Index identifying a specific class in multiclass classification, used to reference or update that class's weights or outputs.\n",
      "-----\n",
      "\n",
      "285fa18f-1a8f-4930-bcb0-c101549563e3\n",
      "Class label for the negative class in binary logistic regression, representing the outcome where the response variable equals zero.\n",
      "-----\n",
      "\n",
      "a8c4726e-e161-475c-bf19-73c1b430846b\n",
      "The norm of the parameter vector w increases without bound, approaching infinity.\n",
      "-----\n",
      "\n",
      "83c2d89c-3311-42d8-9df1-5b5a48211d2e\n",
      "Diagonal matrix with diagonal entries equal to the regularization factor, except the upper-left element, which may be zero if the corresponding weight is not regularized.\n",
      "-----\n",
      "\n",
      "fa3fcda4-e236-4786-9683-3feb702819cc\n",
      "Number of dimensions of the input vector for a function mapping vectors to scalars, indicating the dimensionality of the function's domain.\n",
      "-----\n",
      "\n",
      "4a958b9d-3d76-415d-acb1-fe29bef278d8\n",
      "A matrix of K column vectors, each representing a weight vector for one of the K classes in a multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "f0b9017a-f1fc-4275-8ee3-9d646c637e32\n",
      "Regularized Hessian matrix: sum of the original Hessian matrix and a scaled identity matrix, with the scaling factor as the regularization parameter.\n",
      "-----\n",
      "\n",
      "ec82544a-d47d-4dc8-b5a8-00b8b772dcc6\n",
      "Denotes a function of a vector variable, representing the function evaluated when its argument is a vector, applicable to functions defined on multidimensional parameter spaces.\n",
      "-----\n",
      "\n",
      "fa182606-97d8-4dc8-a3c8-c732e3d9d910\n",
      "Dimensionality of the vector from the softmax function over class scores, equal to the number of classes.\n",
      "-----\n",
      "\n",
      "b4b938db-278b-4536-bdf5-718f00d05f78\n",
      "The k-th component of the indicator vector for a multinomial variable equals one when the variable is in the k-th state.\n",
      "-----\n",
      "\n",
      "d191234c-ea79-490d-83e4-d4cea46f3580\n",
      "function minimized by Newton's method, evaluated at variable x\n",
      "-----\n",
      "\n",
      "a36a35ae-0979-4adc-81e4-61301ef1d7d9\n",
      "Multivariate function mapping n-dimensional vector input to scalar output, serving as the function for which the Hessian matrix is defined at a given point in quadratic expansion.\n",
      "-----\n",
      "\n",
      "86f968d5-f408-4e82-8177-2eaa3d99452f\n",
      "Squared L2 norm regularization term added to the loss function or gradient in regularized logistic regression to penalize large weights and prevent overfitting.\n",
      "-----\n",
      "\n",
      "bf922eb8-7390-491b-95b6-f82ae32360cc\n",
      "Hessian matrix of the cross-entropy error function in logistic regression; positive semi-definite, and under certain conditions, positive definite and invertible.\n",
      "-----\n",
      "\n",
      "beb1c264-3f0f-4216-b015-ed0d7a104c31\n",
      "Location where the quadratic approximation of a function is constructed, corresponding to the point where the parabola is tangential to the function during minimization.\n",
      "-----\n",
      "\n",
      "deeacc88-cbb2-466b-aee2-6ab11a16b38f\n",
      "Function mapping n-dimensional real vectors to real numbers; input is a vector from n-dimensional real space, output is a real scalar.\n",
      "-----\n",
      "\n",
      "e635519d-5013-41ab-842d-4baa7d510d3b\n",
      "Error function dependent on weight parameters and conditioned on the dataset, minimized during model training.\n",
      "-----\n",
      "\n",
      "0db8e74a-1bad-4ddd-9201-ad7e341e8893\n",
      "Regularization factor applied to the diagonal elements of a diagonal matrix in regularized optimization, excluding the element for the unregularized weight.\n",
      "-----\n",
      "\n",
      "aac728b1-fd11-490c-a0eb-ee2da4ac97a6\n",
      "Activation function applied to the scalar product of the weight vector and feature vector in a generalized linear model.\n",
      "-----\n",
      "\n",
      "65ce18fb-cd51-4230-87ab-2dcba4854d08\n",
      "Decomposition of the Hessian matrix for cross-entropy error in logistic regression as the product of the transpose of a matrix, a diagonal matrix, and the original matrix.\n",
      "-----\n",
      "\n",
      "13505949-8a6f-4be3-8ffb-5455eda117c6\n",
      "Vector of K parameters, each subscripted, representing probabilities for each of K outcomes in a categorical distribution.\n",
      "-----\n",
      "\n",
      "94e98347-dc35-4e53-8a65-7ff6adf95733\n",
      "The k-th component of the indicator vector for a categorical (multinoulli) variable is zero when the outcome is not the k-th category.\n",
      "-----\n",
      "\n",
      "db5f5586-e59e-4683-b872-5508b5ae7430\n",
      "Rightwards arrow symbol indicating the start of an example section after the discussion of the softmax function.\n",
      "-----\n",
      "\n",
      "6d06393b-0b0c-4582-b1e7-d4ca38b6c2f9\n",
      "Index identifying a specific basis function within the model, used to distinguish between different basis functions and their associated parameters.\n",
      "-----\n",
      "\n",
      "ae3ae68a-c52e-4c2c-9232-de8fd7da0c1a\n",
      "Event or class label where variable y equals one in binary logistic regression classification.\n",
      "-----\n",
      "\n",
      "ba45f30b-0ce1-41bc-961b-ef832622eb9e\n",
      "Hessian matrix for cross-entropy error in logistic regression; dimensions are (number of features plus one) squared.\n",
      "-----\n",
      "\n",
      "e4665be5-9045-468e-ac35-f1fa21677ffb\n",
      "For every non-zero vector, the quadratic form with the Hessian matrix is greater than or equal to zero.\n",
      "-----\n",
      "\n",
      "d4b1cc61-fd04-454a-a99e-166202a30d32\n",
      "Input example vector to the softmax function representing a data point for class probability computation.\n",
      "-----\n",
      "\n",
      "ea6595ce-e199-4103-9f2d-07beb0db0d40\n",
      "Total number of weight vectors in the matrix for multinomial logistic regression, equal to the number of classes in the classification problem.\n",
      "-----\n",
      "\n",
      "3d41d5a3-86f0-4bb5-9a3e-89193238fae2\n",
      "Index identifying a specific basis function within the model, used to distinguish between different weight vectors and basis functions.\n",
      "-----\n",
      "\n",
      "da4d43a7-fffd-4f65-af68-067c5d203a09\n",
      "The target expression is the objective function being minimized, considered as a function of a single variable in the context of Newton's method.\n",
      "-----\n",
      "\n",
      "18b1db1b-58da-43cf-b21c-ff40b4fcfd4d\n",
      "The target expression denotes the value associated with the 'failure' or 'negative' outcome in a Bernoulli random variable, as defined in the Bernoulli distribution.\n",
      "-----\n",
      "\n",
      "5e5eb05c-f2f1-4269-b808-49fc87f50b2f\n",
      "Taylor series expansion of a differentiable function about a point, including explicit sum of terms with derivatives at that point and equivalent infinite series summation notation.\n",
      "-----\n",
      "\n",
      "9d039f02-a902-4154-9419-7916d8dfd3fe\n",
      "Index identifying a specific class in multi-class classification, used to indicate the class associated with a given example.\n",
      "-----\n",
      "\n",
      "4f04f9cd-5f52-4742-a8b5-9889dc8496d8\n",
      "The indicator vector for a categorical variable has components that sum to one, indicating exactly one outcome among mutually exclusive and collectively exhaustive states.\n",
      "-----\n",
      "\n",
      "c133771c-6e2e-45a8-a075-61b72c9aca17\n",
      "Hessian matrix that is positive semi-definite but not positive definite, lacking an inverse and preventing application of Newton's optimization method.\n",
      "-----\n",
      "\n",
      "7653b754-8773-48a0-8c93-f4172abbe7a6\n",
      "The mean parameter of a distribution equals the dot product of a weight vector and a feature mapping of the input vector.\n",
      "-----\n",
      "\n",
      "ffa345ed-073b-454e-9306-41b6ba3d2c74\n",
      "Weight vector for class k, one of K weight vectors comprising the weight matrix in the softmax regression model.\n",
      "-----\n",
      "\n",
      "748ab077-ad51-43ce-9526-e7a14fa9a00f\n",
      "The expression denotes the model for class k in multinomial logistic regression, where each model corresponds to a specific class within a set of models.\n",
      "-----\n",
      "\n",
      "3849aebd-35a8-4a1a-ba05-4ff57149748a\n",
      "For every non-zero vector, the quadratic form defined by the vector transpose, the Hessian matrix, and the vector is strictly greater than zero, indicating the Hessian matrix is positive definite.\n",
      "-----\n",
      "\n",
      "ccef6221-6043-4fa8-a310-40319684fc8f\n",
      "The j-th basis function in a generalized linear model maps input variables using arbitrary functions, not restricted to polynomials or products of input features.\n",
      "-----\n",
      "\n",
      "f613f567-f569-4f54-a667-430b43238aab\n",
      "The target expression represents the model for class k in multinomial logistic regression, outputting the probability that an input belongs to class k as defined by the softmax function.\n",
      "-----\n",
      "\n",
      "fce9505e-4c43-484e-a080-c75cb173f351\n",
      "Multinomial (multinoulli) variable with four states, represented as a vector with a one in the third position and zeros elsewhere, indicating the third state as the outcome.\n",
      "-----\n",
      "\n",
      "65fb2832-2b07-4ba9-b120-9d1e230ae7a1\n",
      "Collection of models indexed by k, each corresponding to a specific class, collectively forming the overall multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "d1089303-bdf4-4f67-9792-7e2a53f0e56c\n",
      "Function mapping an n-dimensional real vector to an (m+1)-dimensional real vector; output vector contains scalar 1 followed by m basis functions evaluated at the input vector.\n",
      "-----\n",
      "\n",
      "6dfeb566-64e7-4e0d-b8d4-e0f44c540b15\n",
      "A function with subscript j that maps an n-dimensional real vector to a real number, representing one of m basis functions in feature mapping for generalized linear models.\n",
      "-----\n",
      "\n",
      "209aaa2f-a8f1-421e-8148-e639dbfcc905\n",
      "Number of possible values or categories for the modeled variable, generalizing the Bernoulli case to the categorical case.\n",
      "-----\n",
      "\n",
      "b5603743-8220-4d9f-bfeb-10dab2b8ae76\n",
      "The k-th component of the softmax function for an n-dimensional input vector equals the exponential of the k-th input divided by the sum of exponentials of all input components.\n",
      "-----\n",
      "\n",
      "8ad8dc7d-1893-43bc-9909-bff54781732d\n",
      "Non-zero vector used to test if a matrix is positive definite, requiring the quadratic form with the matrix to be strictly greater than zero for each such vector.\n",
      "-----\n",
      "\n",
      "6950d8fb-de24-44b2-b4a9-6a117a137b1e\n",
      "The integer one less than class index k, representing the set of all classes except class k.\n",
      "-----\n",
      "\n",
      "7c21b461-d184-4343-80cf-f281598dc016\n",
      "Label of the i-th data point in a dataset, referenced in probabilistic modeling and label distribution contexts.\n",
      "-----\n",
      "\n",
      "828d726d-225f-4597-b596-bca93261e5b5\n",
      "Applying the softmax function to the vector [1, 2, 0.5, 5, 3] produces a five-dimensional vector of normalized values summing to one.\n",
      "-----\n",
      "\n",
      "5e1d95e4-ec96-47bd-b8e0-c75182b78a10\n",
      "Gradient of the quadratic error function for linear regression with respect to the weight vector for a single training example, equal to the product of the prediction error and the example's feature vector.\n",
      "-----\n",
      "\n",
      "af684f3c-5eaf-4922-acb6-50a5ee521704\n",
      "Square matrix with entries as second-order partial derivatives of a scalar-valued function of n variables; the entry in the i-th row and j-th column is the second partial derivative with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "8a9256fc-a695-4eec-82c2-dd3fd653810b\n",
      "Dimensions of the Hessian matrix for the cross-entropy error in logistic regression are (m + 1) × (m + 1), where m is the number of features after mapping.\n",
      "-----\n",
      "\n",
      "443dbf5b-19d1-4db6-86e9-fe5800c9dc3f\n",
      "Parameter representing the probability of success in the Bernoulli distribution, i.e., the probability that the binary variable equals one.\n",
      "-----\n",
      "\n",
      "833ee48a-a40e-4bf4-9068-b92442e971e2\n",
      "The dot product of the weight vector for class k and the input example vector yields a scalar used as the softmax input for class k.\n",
      "-----\n",
      "\n",
      "96029822-5cd6-42dd-930c-cdf61f027cd2\n",
      "Five-dimensional vector with components 1, 2, 0.5, 5, and 3, used as input for the softmax function.\n",
      "-----\n",
      "\n",
      "0549af99-079a-4cd8-ab82-7e531440d93d\n",
      "Assigns value one to parameter eta, setting step size in Newton's method parameter update rule to one when Hessian matrix is computed exactly.\n",
      "-----\n",
      "\n",
      "63ceb296-0afa-431c-a564-3b4fb7eec2c8\n",
      "Second-order Taylor approximation of a differentiable function at a point, using the function value, first derivative, and second derivative at that point, obtained by truncating the Taylor series after the quadratic term.\n",
      "-----\n",
      "\n",
      "0632e594-4b61-465e-938c-24e4eb819bfc\n",
      "Dataset of input-output pairs; outputs are labels with modeled probabilities and computed log-probabilities in subsequent expressions.\n",
      "-----\n",
      "\n",
      "12bb4ad9-7b8e-490b-a765-f8261081e441\n",
      "Point in the domain of a multivariate function f serving as the vector expansion center for the quadratic (second-order) Taylor series of functions of several variables.\n",
      "-----\n",
      "\n",
      "76a71c57-b440-4d40-816e-ff820d776497\n",
      "Superscript label identifying the first set of weights used in the initial stage of the model's prediction calculation.\n",
      "-----\n",
      "\n",
      "164b8f8c-b978-404f-a996-a532e4f73cac\n",
      "Index of the weight in the weight vector associated with a specific basis function.\n",
      "-----\n",
      "\n",
      "6753b0f1-42d6-46dc-bd19-d41726bfbb3a\n",
      "Activation function in generalized linear models that maps the linear predictor to the mean parameter of an exponential family distribution.\n",
      "-----\n",
      "\n",
      "1a588c32-c43f-4ed0-ab73-b82875b1ce82\n",
      "Variable denotes the index of a class in a K-class classification problem; each model h_k predicts the probability that an input belongs to the class corresponding to this index.\n",
      "-----\n",
      "\n",
      "90d7a53c-ac6b-476d-9dba-18a944ef3918\n",
      "Probability that the k-th indicator variable in a multinoulli vector equals one, representing the probability that the multinoulli variable takes the k-th outcome.\n",
      "-----\n",
      "\n",
      "e03a6b39-8425-4070-bb94-1f2ef931e4c3\n",
      "A weight vector is transposed and multiplied by a feature vector derived from the input, forming a linear combination of feature mappings of the input vector.\n",
      "-----\n",
      "\n",
      "291c787d-b5d4-444a-9abe-7794512c3840\n",
      "Defines the output of a binary logistic regression model as the probability that the class label equals one, given an input vector and a weight vector, computed by applying the sigmoid function to the inner product of the weight vector and a feature transformation of the input.\n",
      "-----\n",
      "\n",
      "b2e88e83-613f-4729-9e77-e73594148d77\n",
      "Activation function in a generalized linear model applied to the linear combination of weights and feature mapping of the input.\n",
      "-----\n",
      "\n",
      "e02db433-d188-4e90-87aa-1e701fa5a542\n",
      "The Hessian matrix for the cross-entropy error in logistic regression equals the product of the transpose of the design matrix, a diagonal matrix with entries given by the derivatives of the logistic function at each training example, and the design matrix.\n",
      "-----\n",
      "\n",
      "b63f4c41-3b4b-4a0a-8298-d94a5c232a2c\n",
      "Model for class indexed by k outputs the probability that an input belongs to class k, computed using the softmax function and dependent on model parameters.\n",
      "-----\n",
      "\n",
      "f596ec8d-474f-43de-b640-8d7f77771a15\n",
      "Total number of classes defined by the upper limit of summations over the class index in cross-entropy loss and error expressions.\n",
      "-----\n",
      "\n",
      "8ba4491a-d6b3-446d-b5d3-a0ee705e4719\n",
      "K denotes the number of possible values or categories, with K > 2 indicating a generalization from a Bernoulli (two-category) variable to a categorical variable with more than two categories.\n",
      "-----\n",
      "\n",
      "196334ad-8d56-4fce-aab9-4976c038ac79\n",
      "Defines the output of a generalized linear model as the activation function applied to the dot product of the weight vector and a feature transformation of the input vector.\n",
      "-----\n",
      "\n",
      "e570128d-9122-452e-a04f-a44d701df680\n",
      "The target expression refers to the model in multinomial logistic regression, defined as a set of class-indexed models that predict the probability of an input belonging to each class using the softmax function.\n",
      "-----\n",
      "\n",
      "36fc41bd-6e0b-4ed1-b8e4-b6cb4298b83a\n",
      "A regularization method used in extending Newton's method to regularized cross-entropy error, serving as a penalty term in the loss function to prevent overfitting in logistic regression.\n",
      "-----\n",
      "\n",
      "e7d7e414-4d28-4a1c-b7b4-14e2ace71ae2\n",
      "Gradient of the error function with respect to the weight vector for class k, expressed as a sum over all training examples; each term is the difference between the model's predicted value for class k and the true label for class k, multiplied by the feature vector of the example.\n",
      "-----\n",
      "\n",
      "961ea654-380a-46f7-976d-e9cc7b456a01\n",
      "The gradient of the regularized error function with respect to the weight vector equals the sum of the gradient of the unregularized error function with respect to the weight vector and the product of the regularization parameter and the weight vector.\n",
      "-----\n",
      "\n",
      "2bcbb003-f4c8-44a6-99f7-5171e4a242f6\n",
      "Multiplying a matrix with N rows by a matrix with N rows and m+1 columns yields a matrix with N rows and m+1 columns.\n",
      "-----\n",
      "\n",
      "57452ba7-9f49-4040-8cf3-513b4afea2cd\n",
      "Mean parameter of a distribution defined as a function of the activation function applied to a linear combination of weights and features.\n",
      "-----\n",
      "\n",
      "0bd58e53-0014-44b8-808e-a56f461d0f3f\n",
      "The output vector dimension of the softmax function equals the number of classes and matches the number of input values to the softmax.\n",
      "-----\n",
      "\n",
      "317dd1b9-2511-4d1d-a034-d01a110a0e67\n",
      "Weight update rule for stochastic gradient descent on the quadratic error function in linear regression: current weight vector is updated by subtracting the product of the learning rate and the gradient of the error with respect to a single training example.\n",
      "-----\n",
      "\n",
      "0d36c960-aac7-475e-baf8-501f78226911\n",
      "Quadratic function used in optimization as an expansion or approximation of a general function to analyze or find its minimum with second-order methods such as Newton's method.\n",
      "-----\n",
      "\n",
      "34a47439-233a-47a7-b50c-ad2ecc06b114\n",
      "The expression defines the gradient descent update rule: the parameter vector at iteration k+1 equals the parameter vector at iteration k minus the step size multiplied by the gradient of the function evaluated at the parameter vector at iteration k. The update is indexed by iteration number.\n",
      "-----\n",
      "\n",
      "3955fb55-0aee-465b-9aae-66d1a37d59a6\n",
      "Index denoting a specific example in a dataset, typically ranging from 1 to N, where N is the total number of examples.\n",
      "-----\n",
      "\n",
      "fecf809f-3185-486b-8639-2cd52bc3cfbb\n",
      "Natural logarithm of the model's predicted probability for a given input; when the probability approaches one, the logarithm approaches zero.\n",
      "-----\n",
      "\n",
      "d5e96d9e-2cda-4b87-a235-0dbc70d2624e\n",
      "Symbol denotes the Hessian matrix of the cross-entropy error function in logistic regression, which is positive semi-definite if the function is convex.\n",
      "-----\n",
      "\n",
      "13f66829-ed08-4a26-b1cc-5df7b812232d\n",
      "Weight update rule in stochastic gradient descent: current weight vector minus learning rate times the difference between model prediction for a specific input and true label, multiplied by the input's feature vector.\n",
      "-----\n",
      "\n",
      "ea3ae73a-ba8f-4e84-97b2-fb9ca374cd7a\n",
      "Parameter update rule in gradient descent: the current parameter vector is updated by subtracting the product of the learning rate and the gradient of the minimized function, both evaluated at the current parameter value.\n",
      "-----\n",
      "\n",
      "24a1272f-257c-4818-818d-cfcefcc5926e\n",
      "Diagonal matrix with entries equal to the product of the logistic function evaluated at each training example and one minus that value, for all training set examples.\n",
      "-----\n",
      "\n",
      "b591e2c3-a412-4e4e-92ce-f6fc17285459\n",
      "Iterative update rule for a parameter vector at iteration t: the next value is the current parameter vector minus the product of a step size, the inverse Hessian matrix evaluated at the current parameter vector, and the gradient of a function evaluated at the current parameter vector.\n",
      "-----\n",
      "\n",
      "cc97fc37-254d-4b10-bab1-d0344c7e818a\n",
      "The k-th element of a parameter vector for a categorical distribution, representing the probability of the k-th category; each element is non-negative and all elements sum to one.\n",
      "-----\n",
      "\n",
      "88a9d567-cd84-4748-b237-9c82dcae85db\n",
      "Probability mass function for a categorical random variable, expressing the probability of a one-hot encoded outcome vector as the product over all classes of class probabilities raised to the power of corresponding indicator variables.\n",
      "-----\n",
      "\n",
      "36f5c1ad-2426-47b5-88d5-85c4d91e79a0\n",
      "A specific type of regularization, contrasted with another type, that is easily incorporated into optimization methods including gradient descent, Newton's method, and least squares for linear regression.\n",
      "-----\n",
      "\n",
      "b29593a6-e223-4b6a-b80a-4a3dc82eb071\n",
      "Weight matrix for the model's first layer; each column or row represents weights assigned to input features for a specific basis function.\n",
      "-----\n",
      "\n",
      "3377ad1e-6ac8-4acf-8d2c-aead139ff034\n",
      "Input vector for the i-th training example in a dataset, used in linear regression and stochastic gradient descent.\n",
      "-----\n",
      "\n",
      "307066f4-d4e0-43b2-9f7a-0c98735e1e0e\n",
      "Defines the loss function for a single example in a K-class classification problem as the negative sum over all classes of the product of the true label indicator for each class and the logarithm of the predicted probability for that class from the model's output.\n",
      "-----\n",
      "\n",
      "3d9c714e-316b-40c0-a574-c190c24b3d22\n",
      "Function of a single variable x, shown as a black curve in a minimization problem, with the objective to find its minimum value using iterative methods like Newton's method.\n",
      "-----\n",
      "\n",
      "4a2d5957-8d89-4417-95c0-0fa17f69b8e8\n",
      "Vector of weights used to combine outputs of basis functions, where basis functions are computed by applying another weight matrix to input features in a two-layer neural network model.\n",
      "-----\n",
      "\n",
      "f251a305-6900-466c-8ed6-7e3ad3b3f35d\n",
      "Quadratic error function for linear regression: sum over N data points of squared differences between predicted values (weight vector transpose times feature mapping of input) and actual target values, scaled by one half.\n",
      "-----\n",
      "\n",
      "ba8ec022-fa36-452e-a0cf-84d3e28964b6\n",
      "Each parameter in the parameter vector, indexed by k, must be greater than or equal to zero to represent probabilities.\n",
      "-----\n",
      "\n",
      "0a11fbc2-9596-483a-b063-cd283e5eb01b\n",
      "Label indicator for example i and class k; equals 1 if example i belongs to class k, 0 otherwise; used in multi-class classification and cross-entropy loss.\n",
      "-----\n",
      "\n",
      "b6daf5f5-f721-4d84-99c9-27b6d54d7242\n",
      "Weight parameter for the i-th input variable in the weight vector of the j-th basis function in a generalized linear model with basis functions, where each basis function has its own set of weights.\n",
      "-----\n",
      "\n",
      "47d91399-f2b5-46d8-8a32-0a49e76b6ff0\n",
      "The target expression refers to the input example vector with components 1, 2, 0.5, 5, and 3, whose class membership probability is evaluated by the multinomial logistic regression model.\n",
      "-----\n",
      "\n",
      "c41e4779-1934-460f-9c3b-88e241908a2f\n",
      "The target expression is the function being minimized after the first update, where a quadratic approximation is performed at the new point in the iterative optimization process.\n",
      "-----\n",
      "\n",
      "e2d3b00e-ba2a-4b69-9ea3-ea1957feca47\n",
      "A column vector with K binary entries, each representing an indicator variable for one of K categorical outcomes; exactly one entry equals one, indicating the observed outcome, and all others are zero.\n",
      "-----\n",
      "\n",
      "8dc8c7c8-9bb2-4ed4-b627-20ffe1fca56c\n",
      "Second-order Taylor expansion of a multivariate function about a point: approximates the function at a general vector as the sum of its value at a reference point, the inner product of its gradient at that point with the displacement from the reference, and one half times the quadratic form involving the Hessian matrix at the reference point and the displacement vector.\n",
      "-----\n",
      "\n",
      "76a55aa5-bec4-442c-a02c-54330d6ed2cf\n",
      "The i-th row and j-th column entry of the Hessian matrix for a function is the second partial derivative of the function with respect to the i-th and j-th variables.\n",
      "-----\n",
      "\n",
      "3062e52e-42f1-44ef-9cf6-efa17d9bae96\n",
      "A generalized linear model where the output is the result of an activation function applied to a linear combination of basis functions of the input, with basis functions and coefficients indexed from zero to m.\n",
      "-----\n",
      "\n",
      "c640dc33-5ddf-4ae2-b825-d56f01d74090\n",
      "Defines the probability that an input vector belongs to class k as the k-th component of a probability vector, equal to the output of a model function computed by exponentiating a linear transformation of a feature mapping of the input for class k and normalizing by the sum of such exponentials over all classes; this is the softmax function used in multiclass classification.\n",
      "-----\n",
      "\n",
      "919f305a-69db-41de-bc20-37ff24c87214\n",
      "Four components of the linear regression model: (1) model output is a linear function of feature transformation and weights; (2) conditional probability of target given input and weights is a normal distribution with mean equal to model output and fixed variance; (3) loss function is the squared difference between model output and target; (4) gradient of loss with respect to weights is the product of prediction error and feature transformation.\n",
      "-----\n",
      "\n",
      "3d84377f-34ba-46d6-a2ae-830b9c3ee56f\n",
      "total number of classes in a multinomial logistic regression model\n",
      "-----\n",
      "\n",
      "f9d2b1ef-2681-45a7-9f43-f492e7ef6419\n",
      "Error function for multi-class classification: negative sum over all training examples and classes of the product of the true label indicator and the logarithm of the predicted probability for each class; generalization of cross-entropy loss to multiple classes.\n",
      "-----\n",
      "\n",
      "e9c4e34c-98a1-40ce-a135-87c5cc57420a\n",
      "Probability that an input vector belongs to class k in multinomial logistic regression, calculated as the exponential of the dot product between the class k weight vector and a feature transformation of the input, divided by the sum of exponentials for all classes; also called the softmax probability for class k.\n",
      "-----\n",
      "\n",
      "02562926-1fdb-4f17-8e7a-b9a85d545ad2\n",
      "Defines a function of an input vector and a parameter vector by applying a linear transformation with a weight matrix, an elementwise nonlinear activation, a second linear combination with another set of weights, and a final application of the activation function. Each basis function is a generalized linear model, and the overall function composes these basis functions with another generalized linear model.\n",
      "-----\n",
      "\n",
      "345cfa7b-dc71-4679-b2b4-f47a05aa9871\n",
      "A type of regularization used in optimization and machine learning, distinct from L2 regularization, associated with a non-differentiable norm that complicates gradient-based optimization and often requires subgradient computation or coordinate descent methods.\n",
      "-----\n",
      "\n",
      "174bde9b-fcb6-4f08-9578-9ea32a22dd1e\n",
      "Set of four equations defining logistic regression: (1) model output as sigmoid of linear input, representing probability of positive class; (2) probability of label given input and parameters as Bernoulli distribution parameterized by model output; (3) loss function as negative log-likelihood (cross-entropy) for binary label; (4) gradient of loss with respect to parameters as product of prediction error and feature vector.\n",
      "-----\n",
      "\n",
      "a3caf3fb-f92e-43ff-b23c-a5c546da8213\n",
      "General form of a probability distribution in the exponential family, expressing the probability of a random variable given parameters as the product of a base measure and an exponential function involving parameters, sufficient statistics, and a log-partition function.\n",
      "-----\n",
      "\n",
      "fdbda175-6069-4b19-9d98-e67463a5e65b\n",
      "Hessian matrix in Newton's method that is positive semi-definite but not positive definite, lacking an inverse.\n",
      "-----\n",
      "\n",
      "a012e7aa-81ce-46fb-8530-61d4913de753\n",
      "Matrix product with the transpose of a bold capital Greek letter matrix on the left, a bold capital Latin letter matrix in the middle, and the original bold capital Greek letter matrix on the right; used as a substitute for the Hessian matrix in logistic regression optimization and in the derivation of the iteratively reweighted least squares algorithm.\n",
      "-----\n",
      "\n",
      "4f976ad6-a861-4300-b523-b3aba86c37a9\n",
      "Feature mapping for a two-dimensional input vector outputs [1, x₁, x₂, x₁x₂, x₁², x₂²], representing a degree-two polynomial feature mapping for two variables.\n",
      "-----\n",
      "\n",
      "f639c277-167d-4286-a074-d16216596fc3\n",
      "Expression for the logarithm of the probability of a set of labels given inputs, represented as: (1) the log of the product over all data points of the probability of each label given its input; (2) the log of the product over all data points and classes of class probabilities raised to the corresponding label indicator; (3) the log of the product over all data points and classes of model-predicted probabilities raised to the label indicators; (4) the sum over all data points and classes of label indicators times the log of predicted probabilities.\n",
      "-----\n",
      "\n",
      "5551377f-dc29-4b1b-9b72-0173f6d5d141\n",
      "Superscript label identifying the second set of weights in the model, indicating their position in the sequence of operations for model prediction.\n",
      "-----\n",
      "\n",
      "6a394e3e-7c98-40b7-9f7e-f5ca3965a25b\n",
      "Set of four expressions for multinomial logistic regression: (1) predicted probability for class k given input and weight matrix, defined by the softmax function and interpreted as the conditional probability of class k; (2) probability of a one-hot encoded label vector given input and weights, expressed as the product over classes of predicted probabilities raised to the corresponding label indicator; (3) loss function as the negative sum over classes of label indicators times the logarithm of predicted probabilities; (4) gradient of the loss with respect to the weight vector for class k, given by the difference between predicted probability and label indicator, times the feature vector.\n",
      "-----\n",
      "\n",
      "01740c91-48eb-4515-ab5d-f64733c59478\n",
      "Probability mass function of a Bernoulli random variable with success probability parameter p; probability of outcome x (0 or 1) is p^x * (1-p)^(1-x), where p is the probability of outcome one and 1-p is the probability of outcome zero.\n",
      "-----\n",
      "\n",
      "9a2c0ef1-ca32-4a2a-ada2-53ceeb691b27\n",
      "Matrix multiplication with the first matrix of dimensions (m+1) × N and the second matrix of dimensions N × unspecified, used to check matrix compatibility for multiplication in Hessian matrix construction.\n",
      "-----\n",
      "\n",
      "dd94d5ef-4bc8-47c5-8f7d-843b3b28520b\n",
      "A type of regularization contrasted with another method in optimization techniques such as gradient descent, Newton's method, and least squares for linear regression; described as straightforward to incorporate into these methods.\n",
      "-----\n",
      "\n",
      "a05e935e-dd2d-4487-abb6-d0139fa9df2c\n",
      "In Newton's method for logistic regression, the weight vector is updated by subtracting the product of the inverse Hessian matrix of the error function with respect to the weights and the gradient of the error function with respect to the weights, both evaluated on the dataset, from the current weight vector, with step size one.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in math_expression_descriptions_opt:\n",
    "    print(x.math_expression_description_id)\n",
    "    print(x.text)\n",
    "    print('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72734a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import unzip\n",
    "\n",
    "from math_rag.application.models.embedders import EmbedderInput\n",
    "\n",
    "\n",
    "inputs, input_id_to_item = InputCreatorUtil.create(\n",
    "    math_expression_descriptions_opt, lambda x: EmbedderInput(text=x.text)\n",
    ")\n",
    "outputs = await default_embedder.concurrent_embed(inputs)\n",
    "descriptions, embeddings = unzip(\n",
    "    (input_id_to_item[output.input_id], output.embedding) for output in outputs\n",
    ")\n",
    "descriptions, embeddings = list(descriptions), list(embeddings)\n",
    "await math_expression_description_opt_repository.insert_many(descriptions)\n",
    "await math_expression_description_opt_embedding_repository.upsert_many(descriptions, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dd1ea",
   "metadata": {},
   "source": [
    "### 5. MathExpressionGroup, requires: MathExpressionDescription, MathExpressionContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88992ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_descriptions = await math_expression_description_opt_embedding_repository.group(\n",
    "    grouper_service.group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80ad33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import Record\n",
    "\n",
    "\n",
    "grouped_records: list[list[Record]] = []\n",
    "\n",
    "for descriptions in grouped_descriptions:\n",
    "    ids = [x.id for x in descriptions]\n",
    "    records = await math_expression_description_opt_embedding_repository.client.retrieve(\n",
    "        collection_name=math_expression_description_opt_embedding_repository.collection_name,\n",
    "        ids=[str(id) for id in ids],\n",
    "        with_payload=True,\n",
    "        with_vectors=True,\n",
    "    )\n",
    "\n",
    "    for record in records:\n",
    "        # remove some data for a clener diagram\n",
    "        record.payload['text'] = record.payload['text'][:50]\n",
    "        record.payload.pop('math_expression_description_id')\n",
    "        record.payload.pop('math_expression_index_id')\n",
    "        record.payload.pop('timestamp')\n",
    "\n",
    "    grouped_records.append(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56eb769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "os.environ['NUMBA_CPU_FEATURES'] = str()  # avoid kernel crash on arm\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b2d88",
   "metadata": {},
   "source": [
    "#### Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a4ac99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    centers=5,\n",
    "    n_features=10,\n",
    "    cluster_std=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    metric='euclidean',\n",
    "    random_state=None,\n",
    ")\n",
    "X_umap = reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba0cfb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": {
          "bdata": "BAQDAQIBAAQAAwQDBAACAQQDAwAEAgMBAAACAAQDAgEEBAICAAAABAEAAwIEBAMEAgACBAMBAQEBAgQBAAABAQAEAQIBAwMDAQQCAwMDBAMBAgQCAAICAwABBAQEAgABAAQDBAQCAwACAQMAAAQEBAIEAAQDAgIEBAQCAgIEAAIAAgMBAQMDAQMABAECAAECAAICAgIDAgQEAwADAAEDAAQCAQQCBAABAAEEAgMEAAQCAQADBAAAAAQEAgMBAQMDAAMDAAMAAQADBAQCAgADBAMEAQQCBAIEAAACAgQABAMEAAECBAMCAwMAAAQDAQEBAAIEAgEEBAMABAICAAICBAMDAgEEAQQAAAMBAQMEAQQCAQMBAwMBBAAAAAIDAgEEAQAABAEAAQMAAAADAwEDAAIDAwECBAAAAAICAAACAgEAAwAEAwIBAAEDAwECAwIBAQQBAgIDAAMEAAMCAAIEAgQCAQEDAwIBAQIBAwMCAQADAwMCAQMBAAMCAQEBAQMEAAQBAwIAAAQCAQEDAwEBAgADAQIDAAECAAIBAwECAQQAAAQBAQMEBAIDBAQEBAECAwIBAgIBAQQCAQACBAEDAAMBBAQAAAQDBAIEAAABAwEAAQQDAAIBAAABAgEAAwMDBAECAQQCAwICAwIAAAQDAwAEAQA=",
          "dtype": "i1",
          "shape": "500, 1"
         },
         "hovertemplate": "UMAP1=%{x}<br>UMAP2=%{y}<br>cluster=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "BAQDAQIBAAQAAwQDBAACAQQDAwAEAgMBAAACAAQDAgEEBAICAAAABAEAAwIEBAMEAgACBAMBAQEBAgQBAAABAQAEAQIBAwMDAQQCAwMDBAMBAgQCAAICAwABBAQEAgABAAQDBAQCAwACAQMAAAQEBAIEAAQDAgIEBAQCAgIEAAIAAgMBAQMDAQMABAECAAECAAICAgIDAgQEAwADAAEDAAQCAQQCBAABAAEEAgMEAAQCAQADBAAAAAQEAgMBAQMDAAMDAAMAAQADBAQCAgADBAMEAQQCBAIEAAACAgQABAMEAAECBAMCAwMAAAQDAQEBAAIEAgEEBAMABAICAAICBAMDAgEEAQQAAAMBAQMEAQQCAQMBAwMBBAAAAAIDAgEEAQAABAEAAQMAAAADAwEDAAIDAwECBAAAAAICAAACAgEAAwAEAwIBAAEDAwECAwIBAQQBAgIDAAMEAAMCAAIEAgQCAQEDAwIBAQIBAwMCAQADAwMCAQMBAAMCAQEBAQMEAAQBAwIAAAQCAQEDAwEBAgADAQIDAAECAAIBAwECAQQAAAQBAQMEBAIDBAQEBAECAwIBAgIBAQQCAQACBAEDAAMBBAQAAAQDBAIEAAABAwEAAQQDAAIBAAABAgEAAwMDBAECAQQCAwICAwIAAAQDAwAEAQA=",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "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",
          "dtype": "f4"
         },
         "xaxis": "x",
         "y": {
          "bdata": "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",
          "dtype": "f4"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP1"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'UMAP1': X_umap[:, 0],\n",
    "        'UMAP2': X_umap[:, 1],\n",
    "        'cluster': y,\n",
    "    }\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    color='cluster',\n",
    "    hover_data=['cluster'],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff01fd",
   "metadata": {},
   "source": [
    "#### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51f69ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [r for grp in grouped_records for r in grp]\n",
    "vectors = [r.vector for r in records]\n",
    "cluster_labels = [i for i, grp in enumerate(grouped_records) for _ in grp]\n",
    "\n",
    "# figure out which payload keys exist across all records\n",
    "payload_keys = set().union(*(r.payload.keys() for r in records))\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    metric='cosine',\n",
    "    random_state=None,\n",
    ")\n",
    "X_umap = reducer.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00c622bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           "Function minimized using quadratic approximation a",
           "8fa78e70-df34-4f0a-b031-5fc98e0645d5",
           "03e4862b-72c3-406d-bf58-cc4bdb1a3ccf",
           0
          ],
          [
           "The target expression is the objective function be",
           "5e6f86ad-4c9a-437e-a534-3dab0e8ff4e0",
           "09a8f9a8-cdb8-4dde-ad81-7cf905c722e8",
           0
          ],
          [
           "The target expression denotes the value associated",
           "6016e48f-51ba-47fc-8444-40e0521e590d",
           "0a5b7e0e-b2db-46aa-85b6-24e3e8e3316d",
           0
          ],
          [
           "Function of a single variable x, shown as a black ",
           "1a08f173-2495-4dde-9593-bb5e6a96150b",
           "103923f1-309b-48c4-a77c-0660753f6712",
           0
          ],
          [
           "function minimized by Newton's method, evaluated a",
           "d0ab355b-222b-4aac-b7cb-78af184c84cc",
           "2454e996-0047-466d-8219-13141c5565b7",
           0
          ],
          [
           "Initial point for starting the minimization proces",
           "10a1b4f2-7c7a-4d71-8dd6-055c2121e32c",
           "28337988-42c8-48d4-b181-7c7a7b658679",
           0
          ],
          [
           "The target expression is the function being minimi",
           "c43f41f5-73c7-4319-8f21-adb984b11790",
           "30ef815d-0b56-4abe-a785-c5a09a6fb097",
           0
          ],
          [
           "Function of a single variable minimized using quad",
           "a936ea0a-7f31-4d7e-bbf9-94e68d7f2fb1",
           "572e5285-88d1-4d2f-a03c-888facca0341",
           0
          ],
          [
           "the point obtained after the first step of minimiz",
           "6742746c-946a-46b9-b1ea-27fddcfd325f",
           "581b23f2-3ed2-4197-916a-6ecdf950f8ca",
           0
          ],
          [
           "Location where the quadratic approximation of a fu",
           "f95e2a5e-d517-491c-b757-43694efb1cbd",
           "5a1659b3-c8ab-4807-8f63-5a29811c4a7a",
           0
          ],
          [
           "The target expression is the function being minimi",
           "5bf228e6-7298-4144-b28f-db47a637c550",
           "70806774-f3ab-42f8-8994-8cb57fa9d04b",
           0
          ],
          [
           "Quadratic function used in optimization as an expa",
           "75aed128-da46-42cf-a78f-678b4796743a",
           "9cde1df5-f0b0-414c-b0e0-956c009753e2",
           0
          ],
          [
           "Objective function of a vector variable to be mini",
           "77537328-db21-4be6-b425-33255ddeacc7",
           "e3a41001-8e80-4a20-b77a-dc79bbccf313",
           0
          ],
          [
           "A type of regularization contrasted with another m",
           "b6c69cf6-8359-4e03-9139-776fccebe363",
           "7926ba60-9531-4303-9899-0c86c985fb2f",
           1
          ],
          [
           "L1 norm, used as a regularization term in logistic",
           "8a06dcbd-b8be-4aa6-870e-6e812603ccf7",
           "81daf29c-2f27-41ca-ab95-728a29ade2ef",
           1
          ],
          [
           "A type of regularization used in optimization and ",
           "e2b1ba44-98d5-4b30-856a-0c40fd43bf4f",
           "9fe5fb2b-32f6-483f-9c64-b5f48188d4c6",
           1
          ],
          [
           "Norm associated with L1 regularization; not differ",
           "a0df5bc1-ed35-4164-9bd6-833b032e9b2e",
           "ac575672-a09a-4d4b-802d-dd330193c9cc",
           1
          ],
          [
           "A specific type of regularization, contrasted with",
           "6801fe8d-998c-4858-9643-31c501a8a0b3",
           "c67c7419-d1cc-45da-9d23-830a1b9d4f69",
           1
          ],
          [
           "Event or class label where variable y equals one i",
           "74bc6ff9-56c6-4e79-b035-b780d83b6b7e",
           "a42abb2d-853c-4dd4-873a-c670d88bd249",
           2
          ],
          [
           "The value representing the outcome one of a Bernou",
           "4c28a145-6cb6-4da3-9b7b-0dbcce57947a",
           "abcbc280-738d-43e4-b602-5b1992494401",
           2
          ],
          [
           "y is a binary label taking values 0 or 1, represen",
           "8f495771-abbd-4b7a-9a3f-468903ef753d",
           "d3f485ee-596c-4913-b095-5334d6d7984a",
           2
          ],
          [
           "Parameter representing the probability of success ",
           "e0bf37c7-3ca4-43a3-97f8-404946c14084",
           "da4a2758-648c-4ee3-bc66-d356ef4e14d3",
           2
          ],
          [
           "Applying the softmax function to the vector [1, 2,",
           "ef7b80ea-f2b1-44e4-a65e-84a9543236ce",
           "358f099c-73ad-4647-9bb5-dfd0e1f0de59",
           3
          ],
          [
           "Input example vector to the softmax function repre",
           "58eb2a95-a0e9-435f-9edc-8f6aaf65556d",
           "452429aa-d63f-4037-8d30-a242e3f641fa",
           3
          ],
          [
           "Regularization factor applied to the diagonal elem",
           "2b9d3572-b560-413d-b763-1615e85699fa",
           "279a1971-8dec-409c-be4b-dd50e6c7a0a9",
           4
          ],
          [
           "Diagonal matrix with diagonal entries equal to the",
           "7bac3824-9191-4fbf-b991-2448f0de6650",
           "b1734925-2213-4b1d-8d02-d1c48d8c6367",
           4
          ],
          [
           "A column vector with K binary entries, each repres",
           "58208881-25b4-4551-b4c1-cb2d5cbbb929",
           "2d6c3d90-bcd5-446f-94c0-f9373fbe2860",
           5
          ],
          [
           "The k-th element of a parameter vector for a categ",
           "d25ed4ea-d3e3-4a59-ac35-f0a1e8671c8d",
           "4d26d20d-a72c-4c7f-9e84-14f49d51e2a9",
           5
          ],
          [
           "Vector of K parameters, each subscripted, represen",
           "cccb3924-7a84-4a59-a273-96b0004bb113",
           "dc4d7374-26a9-4cd3-b93a-d3049e9e6da2",
           5
          ],
          [
           "Variable denotes the index of a class in a K-class",
           "2c9bba3a-e730-40d2-9cef-9e644ea44681",
           "22e993e8-4488-469d-90a0-11a3ad02ffe9",
           6
          ],
          [
           "Index identifying a specific class in multiclass c",
           "181875d6-5dbb-4c59-b6b6-3eadc71dbdd5",
           "24379b21-2066-4cdf-954d-5fa8ffd1318a",
           6
          ],
          [
           "Index identifying a specific class in multi-class ",
           "6334cc29-15ad-4dfc-938c-4bd2650b3cd4",
           "3d0f6a6e-8b4e-41c2-84b7-536a4a06cf21",
           6
          ],
          [
           "Index representing a specific class in a multinomi",
           "fdb90394-e868-4e40-a1ea-1056a5e8c2b3",
           "499d63bd-ed03-4ee7-9240-b50e099fe4da",
           6
          ],
          [
           "Index of a class in a classification problem with ",
           "ef99238a-25d1-494c-bd7f-b1afa2fa269b",
           "6431867f-811c-4478-a5cd-f33abb51d0ef",
           6
          ],
          [
           "Index of a specific class in multiclass classifica",
           "8156acb7-3435-444c-b630-328879f69284",
           "de0cedc4-ac0d-456a-b3c4-eff30eaa3fe5",
           6
          ],
          [
           "Index of the weight in the weight vector associate",
           "e65a896b-31c4-4cfd-a1a9-675a3f0e6acd",
           "2d69c58c-219b-4534-bc85-002ffc4973a3",
           7
          ],
          [
           "Index identifying a specific basis function within",
           "ff231356-d299-4d4b-8120-3c8059ef93af",
           "35a67779-bb57-4f2f-8ead-546569494f5a",
           7
          ],
          [
           "Index identifying a specific basis function within",
           "48650be2-c727-4aeb-908a-eaccf91af5dc",
           "b4786f28-df7b-435a-91d4-4d0531e85815",
           7
          ],
          [
           "Weight parameter for the i-th input variable in th",
           "bd21ac6b-0a8a-456c-9ba5-fbaccd9fc70d",
           "cca09176-0726-4e85-9b86-d5ff86cf4afa",
           7
          ],
          [
           "The k-th component of the indicator vector for a c",
           "cab8f0ce-7463-4e05-8165-cd3aceef8e63",
           "20a926b8-8080-48d9-a222-407561990f74",
           8
          ],
          [
           "Probability that the k-th component of the indicat",
           "d839398f-e4df-4e81-ad78-bb455e20896f",
           "9823abca-dc45-48c3-83fc-b538c152d739",
           8
          ],
          [
           "Probability that the k-th indicator variable in a ",
           "3d47bb24-032d-48ce-a9ac-6ed0b7b7f46e",
           "d020b4b9-39bd-41b8-bbe3-3d34c1a9ccfe",
           8
          ],
          [
           "The k-th component of the indicator vector for a m",
           "2ab5fc36-0120-4d92-926d-0edeeaf99be2",
           "d3ff2c89-74e3-493f-be20-e0c1716a3891",
           8
          ],
          [
           "Index value for a multinomial variable outcome, wh",
           "5ba5501a-3088-4fbc-96c3-92c617301eef",
           "e60b57c6-781f-4464-9ba1-0e86c006762e",
           8
          ],
          [
           "Dimensionality of the vector from the softmax func",
           "2ad2bc59-7142-4b44-9bea-d31f04034994",
           "06fc380d-2743-4662-ba69-763ce550fa30",
           9
          ],
          [
           "Total number of classes in a classification proble",
           "e2df0804-dc43-412b-a90c-f4d376a3cc52",
           "b2e550c7-d8d4-432d-bbe1-07c5d45c1029",
           9
          ],
          [
           "the point at which a differentiable function's Tay",
           "e03a96db-3bd7-4172-b883-5eff7f8f6c0b",
           "3160c832-1bb5-4356-a520-df7904687108",
           10
          ],
          [
           "the point at which a differentiable function's Tay",
           "9eb3257a-ef19-4aa0-b3b7-c1e08c6b02a5",
           "a49d99ce-6370-4716-8975-653f64b182b4",
           10
          ],
          [
           "Taylor series expansion of a differentiable functi",
           "ebbcc2f6-be05-48ff-9303-4ed19f9fd78e",
           "ae6e45a4-b527-4324-aa3f-10772b701b7c",
           10
          ],
          [
           "general differentiable function for Taylor series ",
           "541d3829-e210-4e71-9e49-86cdb2b94810",
           "f428fa88-b08a-4027-9d7c-86322fefe5d8",
           10
          ],
          [
           "Set of four equations defining logistic regression",
           "4d1642b3-60a6-438c-beda-a9619eee5115",
           "051e5886-3854-4b67-be2e-9962bed50367",
           11
          ],
          [
           "The target expression represents the model for cla",
           "c413493a-cd20-431d-a8c4-d9bfe5965945",
           "47a99ba5-2390-49ab-9a50-c19223b26b8e",
           11
          ],
          [
           "The expression denotes the model for class k in mu",
           "ee5d8940-7805-41be-9e85-7838d805ec68",
           "547b8a2c-477e-4615-b0a6-cfd17210d750",
           11
          ],
          [
           "The target expression refers to the model in multi",
           "491e76de-5e20-4628-b610-dea2db46ee81",
           "6f8e7002-21f4-4aad-adce-da4a38950d92",
           11
          ],
          [
           "Set of four expressions for multinomial logistic r",
           "dd20286f-075e-477f-8bf3-98467f4fd1e9",
           "783e39d7-6724-47a1-9b4c-d722dcb5e8a4",
           11
          ],
          [
           "Model for class indexed by k outputs the probabili",
           "0ad2623a-a46d-4472-b68b-44faa71aba37",
           "8f865a50-16eb-4295-a2d7-42b1f1653808",
           11
          ],
          [
           "Probability that an input vector belongs to class ",
           "03b5058c-e356-4c1a-b8ba-26c2e70f7b96",
           "91b12bfc-7480-4e2f-b78e-41ce64700dec",
           11
          ],
          [
           "Defines the probability that an input vector belon",
           "a6914d4b-3a27-4de4-9eeb-aad75713704f",
           "ec799a3f-a567-4416-a009-350ec499c1e8",
           11
          ],
          [
           "total number of classes in a multinomial logistic ",
           "33a49ee6-3f80-4dbf-9884-222b47a25ccd",
           "06d4ec6f-339a-48e8-944c-071ccc803e26",
           12
          ],
          [
           "The dot product of the weight vector for class k a",
           "012a8731-0112-4df7-a994-cc6ed47f4441",
           "581b2a0f-f71e-47fc-b4ec-9ce7c4b32e87",
           12
          ],
          [
           "Scalar product of the weight vector for class k an",
           "4e773519-1999-4d32-8510-df50aa71c1ed",
           "581d1a27-ccc3-4f16-b7db-929fd49156fc",
           12
          ],
          [
           "Total number of weight vectors in the matrix for m",
           "ce14c127-0925-470b-876b-d56429e15829",
           "768a3df3-3afe-4051-b5f0-3afaaef2cd53",
           12
          ],
          [
           "Weight vector for class k, one of K weight vectors",
           "85ffccc5-b49f-4162-9a52-ab42a5665714",
           "80900ca1-9949-415a-9588-a5237b187ef2",
           12
          ],
          [
           "Number of classes in a multinomial logistic regres",
           "21f28c22-bd69-4253-b0f6-0aef98d7714b",
           "81a5b03d-198e-4830-b605-d645b437ac41",
           12
          ],
          [
           "weight vector for the k-th class in a multinomial ",
           "ef9dd0c7-2853-4ab5-add8-51693328b84d",
           "89d7b9c6-c19d-44f3-ade3-c0f679cc1228",
           12
          ],
          [
           "A matrix of K column vectors, each representing a ",
           "1b489ace-f602-418c-ab7c-21f49e52b592",
           "a1af7728-0877-4a44-9b8f-d51d9c351d41",
           12
          ],
          [
           "For every non-zero vector, the quadratic form with",
           "62d36fa1-69c0-4e51-9141-36a2199f0bd7",
           "5ea2c4b6-4cf9-4349-817c-e77b66c6a8d9",
           13
          ],
          [
           "Non-zero vector used to test if a matrix is positi",
           "cf651ec0-e666-4331-9292-0e4c485ed705",
           "d1b36d4e-4711-4276-aaa8-364fd79ee2aa",
           13
          ],
          [
           "For every non-zero vector, the quadratic form defi",
           "c2dee8e0-a43b-449d-9d52-9ed181ea486d",
           "ee6c4305-b258-4504-9ae5-31a2214351bb",
           13
          ],
          [
           "A weight vector is transposed and multiplied by a ",
           "b450c527-2c00-4af6-9c60-837746300e18",
           "9de7863e-2804-41fa-959d-88b229328361",
           14
          ],
          [
           "Vector of weights used to combine outputs of basis",
           "c1ef1999-d09d-4c7d-8146-b63f2439b32c",
           "f11112b0-faf5-4213-8ae4-3e109f8d9536",
           14
          ],
          [
           "Weight update rule for stochastic gradient descent",
           "560155e2-e7f4-41ec-b0bc-03beefdbd568",
           "198cccab-d19f-4fac-96da-74028a25d3ad",
           15
          ],
          [
           "Gradient of the quadratic error function for linea",
           "31e0e631-fd50-42d9-8c27-92b5a586cd3b",
           "4f1f963c-5395-44d3-903e-2b80943c08e0",
           15
          ],
          [
           "Iterative update rule for a parameter vector at it",
           "5e6cf308-f252-47a7-bb5e-7251877ff0c7",
           "0399dcd4-0d3e-42f8-ae0f-6a3f3a1ebe45",
           16
          ],
          [
           "The expression defines the gradient descent update",
           "b1a967b8-6c2e-4da4-a4ce-b5de65578f9a",
           "2a5140c5-9848-4eb0-aecb-bc41d68ae49c",
           16
          ],
          [
           "Parameter update rule in gradient descent: the cur",
           "85d2bfbf-2e50-4053-a51f-a8f6a04cf88a",
           "dcc535d7-7ba7-41b1-b2dc-40350ec4a3a3",
           16
          ],
          [
           "Symbol denotes the Hessian matrix of the cross-ent",
           "5676acaf-3e78-486a-af80-784ee6f736f8",
           "0d42282c-ad37-401b-b181-ed2a8351c25a",
           17
          ],
          [
           "Decomposition of the Hessian matrix for cross-entr",
           "43c280b6-bd63-4d89-b6db-cdd208ab5cd4",
           "2317d526-3da9-411c-985f-abbd7f336440",
           17
          ],
          [
           "Hessian matrix of the cross-entropy error function",
           "94d333c3-a1da-4283-a368-c3c9fb2f589d",
           "2b2a2b95-d154-46ef-972e-a8e216ce6284",
           17
          ],
          [
           "Hessian matrix of the cross-entropy error function",
           "7c7dde6a-2d20-40b8-af33-73e37289c279",
           "82da78c2-e892-4d4c-a1dd-d1a96f43d8a3",
           17
          ],
          [
           "The Hessian matrix for the cross-entropy error in ",
           "82157081-ff7d-4b03-9316-1d04c0b4b3ec",
           "8b7c59cf-4fda-45e5-b267-94949626288d",
           17
          ],
          [
           "Hessian matrix for cross-entropy error in logistic",
           "d3e48fcd-f624-43ed-872b-7a34a9bc3dbb",
           "bdcf68f9-6647-40ac-bcea-a950b139be79",
           17
          ],
          [
           "Hessian matrix of the cross-entropy error function",
           "efc95dc4-8c74-407e-be96-5dacd6232db0",
           "d9431bf8-6809-47ca-b3e5-1b897bfa39f5",
           17
          ],
          [
           "Dimensions of the Hessian matrix for the cross-ent",
           "c3a0108b-28c6-4c13-8cff-bba631e5a9bd",
           "f939f890-c1f4-4da8-8941-dd65682fa977",
           17
          ],
          [
           "Hessian matrix in Newton's optimization method, ch",
           "150b2917-6366-4b18-b4ee-8bac79b2f08b",
           "0e1c7b0d-c153-47a5-a9d0-09f4d32e543b",
           18
          ],
          [
           "Hessian matrix in Newton's method that is positive",
           "95acb795-e420-44f1-9cce-ad2c6642390c",
           "30b63871-4e6c-4791-8547-a5503b7ecfab",
           18
          ],
          [
           "Hessian matrix required to be positive definite fo",
           "87b9b4a8-a30e-4db0-9efd-6fba21a89c50",
           "3b1e130e-e62a-422f-bf4f-362768ac70af",
           18
          ],
          [
           "Hessian matrix that is positive semi-definite but ",
           "3876ad10-a915-4366-b47b-45f5b938abc7",
           "d9b82f58-52dd-452c-89d5-2534a30de500",
           18
          ],
          [
           "Mean parameter of an exponential family distributi",
           "bec36427-865c-4c65-9563-00e3b226aeca",
           "0d07ea4d-8a35-4e43-8ca4-1aaa61a912fb",
           19
          ],
          [
           "Activation function specifying the mean parameter ",
           "2cc8039a-67e5-43d0-9084-4848f3e79737",
           "137eb66f-e5cb-4cf1-a09b-3f6cbd9de09d",
           19
          ],
          [
           "Mean parameter of a distribution defined as a func",
           "2ea8fdb6-656d-4706-b174-c7b677113fd2",
           "6bd46d0e-3cb0-4ac1-a70a-202232ad99f1",
           19
          ],
          [
           "Activation function in generalized linear models t",
           "2b0a59db-a3dc-425d-93a5-75a618b21468",
           "769bf24e-9658-4099-bdb9-91a54a75b4f1",
           19
          ],
          [
           "Activation function applied to the scalar product ",
           "ca12e2f6-8438-481c-8aa0-20caa07f9471",
           "6d8affa9-5f32-4569-b1d1-504e86a33eca",
           20
          ],
          [
           "A generalized linear model where the output is the",
           "36bff184-e1db-4263-8d2b-6904c8a3dee7",
           "6ed581bc-ec92-4721-9b5a-46b1a7c3c20c",
           20
          ],
          [
           "Activation function in a generalized linear model ",
           "1081d3ab-9a85-475a-bf5f-233b56e12e66",
           "9d9259c1-373b-405b-8435-b931dbf5e2f9",
           20
          ],
          [
           "Defines the output of a generalized linear model a",
           "70f0c18f-6d49-4fe1-8227-68d128a36763",
           "af869f40-50b0-4e97-bb42-e96ca3e911e8",
           20
          ],
          [
           "Defines a function of an input vector and a parame",
           "180df43e-5c31-49ca-8603-9c1fd687209a",
           "cea17b32-dea9-4a75-a126-724b7461795c",
           20
          ],
          [
           "The j-th basis function in a generalized linear mo",
           "c1ff6c7e-bb1c-4aab-8c1b-2898ea0b7114",
           "56a3e8e1-ccde-4412-847c-485b3c1fc656",
           21
          ],
          [
           "Number of basis functions used for feature mapping",
           "4669d894-e98b-49c7-be0c-96c26954ee8e",
           "73a9b8de-2a27-4885-b7cd-ed918480a7e6",
           21
          ],
          [
           "Collection of m basis functions, each representing",
           "51024dc2-44fc-41ec-954d-15e3c4fb63c1",
           "c354e020-2af9-49f0-83e3-775abd4cc056",
           21
          ],
          [
           "A function with subscript j that maps an n-dimensi",
           "81d433c2-29c5-4e74-b75a-81e3234e7589",
           "d58e4f10-bdb2-4c5a-9b14-71a9fccc513d",
           21
          ],
          [
           "Real-valued function of a vector variable, used in",
           "2c6d4932-e91b-4c44-b7b0-4fb1b4618423",
           "1a58ed29-58f9-47aa-9905-faf542207b28",
           22
          ],
          [
           "Multivariate function mapping n-dimensional vector",
           "113c4998-6480-49ca-a6d8-df63e8e026dc",
           "32377814-010d-49b8-bf13-8a78c3290bb8",
           22
          ],
          [
           "Function mapping an n-dimensional real vector to a",
           "e0437fdb-b7d0-4398-9be5-5fefbb3c03e4",
           "8b3b6a0f-6871-4590-aa15-88a0080057d0",
           23
          ],
          [
           "Function mapping n-dimensional real vectors to rea",
           "dcfa48ea-3725-4fc7-87c7-56683476c9da",
           "ae82c8b2-4f4a-424c-a79b-84d9a392ef2f",
           23
          ],
          [
           "A function mapping an n-dimensional real vector to",
           "b26d529f-c813-4e91-85c9-c856cc5283fe",
           "bf94b7c1-7f8b-426e-af31-a96c2bb67d92",
           23
          ]
         ],
         "hovertemplate": "UMAP_1=%{x}<br>UMAP_2=%{y}<br>text=%{customdata[0]}<br>math_expression_id=%{customdata[1]}<br>id=%{customdata[2]}<br>cluster=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": {
           "bdata": "AAAAAAAAAAAAAAAAAAEBAQEBAgICAgMDBAQFBQUGBgYGBgYHBwcHCAgICAgJCQoKCgoLCwsLCwsLCwwMDAwMDAwMDQ0NDg4PDxAQEBEREREREREREhISEhMTExMUFBQUFBUVFRUWFhcXFw==",
           "dtype": "i1"
          },
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": {
          "bdata": "QDXiQI381kAa/so/iirTQNJt00AJxuJA1+XeQE6g40CJf+pAy47nQHyz5EB9L99AT4/MQMtJvkAL8rxAxo/CQD84wkAk2cRAD+lYP0Lzsj9RfY8/wmqcPxzTCL8thfS+H7yzQLSqrUBkH/Y+lzwiPw/3sj7LXSA/pkxhP0HcUD9eli0/ltUdP47E8z4powNA/6bpP0pq3D8IshNAYMRMP3RcXz+IDnc/ZntxPyTvhT96LDy/2hzvvuft+EC7//tARHL/QCqf+UCWuxI/y2sdPjv7VT40awE+jIfcvL7Iuz0+zeA80h+8veCvJb/rpi+/QfsBvx5Jbb+2Vla/0a42vw4HLb94V4K/9ZJwQBstb0B8rGVArikAQJcOC0BzNatAfTWcQFXdsUCWt61ATfqxQKzdVUAsAmNAQm9pQLTxZ0DGdGFAWC5YQAAASECN/EpA4Td7QKuPfkBRXYRA7sN+QPIQ+D/hTQRAs6j8P+XnEEDdSwlAbyAiQBa/DkA7RgpAP1MiQDhMN0BaDTBAiN8+QOJLR0Dn1nJAqspeQCnDUUCmsVZA/xhTQA==",
          "dtype": "f4"
         },
         "xaxis": "x",
         "y": {
          "bdata": "hxavPzOkjD9x052+2UyZPyS7sj8VQIc/m0GeP+Fasj/Z4pg/dxOkP9RGfD9/qcs/pqfMPwJhSkDRoWRAU5RHQAIHTUBmhEFA3lMNPbxKGr9M/M2+6Ik3vx62pj37Cqq9M7VQQFR7ZUCwMJW/yAeMv3gzir8nosM+4hFsP8dWaz9KSxA/1AZjP9pBgD/3OAVAG3LmPyDm1j+uwhtAAY27v2Iok78CU6m/9o+1v8Cfpb+/3+I+0sIzP2PI9z9+cvk/jDoNQJKyA0DK77K97U1HvQk0kz5JQac+QiwUvmvIjT1rUOO+rgzovoWQBT8fU86+PnOcvlUSBj/x73a+qSjMPr914746SC+8Xvu0QPu3uEAyxrNA55o2QNLmJUCNlBtAWb0WQOL4B0CBiQBAiyARQAY4y0D2xtdA+QXRQHsuz0C7JddASGLZQLG60kBOW9pAJw+/QNbNwUASecRAYVPAQBTHYkDb+VVAEYBTQJhVYEDNTj1APSRMQB4vQ0DaVUpALeVBQJQeU0DPdjlA8r9GQHXxZ0CVao1A1biGQIxjfUCV1H1AUuKCQA==",
          "dtype": "f4"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "cluster"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP_1"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "UMAP_2"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = []\n",
    "for x, y, label, record in zip(X_umap[:, 0], X_umap[:, 1], cluster_labels, records):\n",
    "    row = {\n",
    "        'UMAP_1': x,\n",
    "        'UMAP_2': y,\n",
    "        'cluster': label,\n",
    "        'id': record.id,\n",
    "    }\n",
    "    row.update(record.payload or {})  # add all payload fields\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='UMAP_1',\n",
    "    y='UMAP_2',\n",
    "    color='cluster',\n",
    "    hover_data=list(payload_keys) + ['id', 'cluster'],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f8f4d",
   "metadata": {},
   "source": [
    "#### Continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathExpressionGroup\n",
    "\n",
    "\n",
    "grouped_math_expression_ids = [\n",
    "    [description.math_expression_id for description in descriptions]\n",
    "    for descriptions in grouped_descriptions\n",
    "]\n",
    "\n",
    "for math_expression_ids in grouped_math_expression_ids:\n",
    "    # group requires at least two elements\n",
    "    if len(math_expression_ids) < 2:\n",
    "        continue\n",
    "\n",
    "    math_expression_group = MathExpressionGroup(math_expression_index_id=index.id)\n",
    "    await math_expression_group_repository.insert_one(math_expression_group)\n",
    "    await math_expression_group_graph_repository.insert_one_node(math_expression_group)\n",
    "\n",
    "    # add all candidates to a group, remove some of them in the next step\n",
    "    await math_expression_repository.update_group_id(math_expression_ids, math_expression_group.id)\n",
    "    await math_expression_graph_repository.update_many_nodes(\n",
    "        filter=dict(id=math_expression_ids),\n",
    "        update=dict(math_expression_group_id=math_expression_group.id),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3e827",
   "metadata": {},
   "source": [
    "### 6. MathExpressionGroupRelationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "from math_rag.application.models.assistants.inputs import MathExpressionComparator as AssistantInput\n",
    "from math_rag.application.utils import GroupPrunerUtil\n",
    "from math_rag.core.models import MathExpressionGroupRelationship\n",
    "\n",
    "\n",
    "math_expression_groups = await math_expression_group_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")\n",
    "\n",
    "for math_expression_group in math_expression_groups:\n",
    "    math_expressions = await math_expression_repository.find_many(\n",
    "        filter=dict(math_expression_group_id=math_expression_group.id)\n",
    "    )\n",
    "    math_expression_ids = [math_expression.id for math_expression in math_expressions]\n",
    "    math_expression_contexts = await math_expression_context_repository.find_many(\n",
    "        filter=dict(math_expression_id=math_expression_ids)\n",
    "    )\n",
    "    pairs = list(combinations(zip(math_expressions, math_expression_contexts), 2))\n",
    "\n",
    "    print(len(math_expression_ids))\n",
    "    print(len(pairs))\n",
    "    print('----')\n",
    "\n",
    "    if not pairs:\n",
    "        continue\n",
    "\n",
    "    inputs: list[AssistantInput] = []\n",
    "    input_id_to_candidate_pair: dict[UUID, tuple[UUID, UUID]] = {}\n",
    "\n",
    "    for pair, other_pair in pairs:\n",
    "        math_expression, math_expression_context = pair\n",
    "        other_math_expression, other_math_expression_context = other_pair\n",
    "        input = AssistantInput(\n",
    "            katex=math_expression.katex,\n",
    "            context=math_expression_context.text,\n",
    "            other_katex=other_math_expression.katex,\n",
    "            other_context=other_math_expression_context.text,\n",
    "        )\n",
    "        inputs.append(input)\n",
    "        input_id_to_candidate_pair[input.id] = (math_expression.id, other_math_expression.id)\n",
    "\n",
    "    outputs = await math_expression_comparator_assistant.concurrent_assist(inputs)\n",
    "\n",
    "    candidates = math_expression_ids\n",
    "    candidate_pair_to_is_connected = {\n",
    "        input_id_to_candidate_pair[output.input_id]: output.is_identical for output in outputs\n",
    "    }\n",
    "\n",
    "    math_expression_ids = [math_expression.id for math_expression in math_expressions]\n",
    "    math_expression_ids_to_group = GroupPrunerUtil.prune(candidates, candidate_pair_to_is_connected)\n",
    "    math_expression_ids_to_ungroup = list(\n",
    "        set(math_expression_ids) - set(math_expression_ids_to_group)\n",
    "    )\n",
    "\n",
    "    if not math_expression_ids_to_group:\n",
    "        continue\n",
    "\n",
    "    math_expression_group_relationships = [\n",
    "        MathExpressionGroupRelationship(\n",
    "            math_expression_index_id=index.id,\n",
    "            math_expression_id=math_expression_id,\n",
    "            math_expression_group_id=math_expression_group.id,\n",
    "        )\n",
    "        for math_expression_id in math_expression_ids_to_group\n",
    "    ]\n",
    "    await math_expression_repository.update_group_id(math_expression_ids_to_ungroup, None)\n",
    "    await math_expression_graph_repository.update_many_nodes(\n",
    "        filter=dict(id=math_expression_ids_to_ungroup),\n",
    "        update=dict(math_expression_group_id=None),\n",
    "    )\n",
    "\n",
    "    math_expression_group_relationships = [\n",
    "        MathExpressionGroupRelationship(\n",
    "            math_expression_index_id=index.id,\n",
    "            math_expression_id=math_expression_id,\n",
    "            math_expression_group_id=math_expression_group.id,\n",
    "        )\n",
    "        for math_expression_id in math_expression_ids_to_group\n",
    "    ]\n",
    "\n",
    "    # print(len(math_expression_ids))\n",
    "    # print(len(math_expression_ids_to_group))\n",
    "    # print(len(math_expression_ids_to_ungroup))\n",
    "    # print(len(math_expression_group_relationships))\n",
    "    # print('----')\n",
    "\n",
    "    await math_expression_group_graph_repository.insert_many_rels(\n",
    "        math_expression_group_relationships, rel_to_cls=MathExpression\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ee39e",
   "metadata": {},
   "source": [
    "## Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f44bd",
   "metadata": {},
   "source": [
    "### 1. MathArticleChunk, requires: MathExpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e063fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expressions = await math_expression_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b9bfa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathArticleChunk\n",
    "\n",
    "\n",
    "index_to_katex = {\n",
    "    math_expression.index: math_expression.katex for math_expression in math_expressions\n",
    "}\n",
    "chunk_templates = TemplateChunkerUtil.chunk(template, max_window_size=2048, max_padding=256)\n",
    "math_article_chunks: list[MathArticleChunk] = []\n",
    "\n",
    "for i, chunk_template in enumerate(chunk_templates):\n",
    "    indexes = TemplateIndexFinderUtil.find(chunk_template)\n",
    "    formatted_chunk, _ = TemplateFormatterUtil.format(\n",
    "        chunk_template, index_to_katex, omit_wrapper=False\n",
    "    )\n",
    "    # print(_)\n",
    "    math_article_chunk = MathArticleChunk(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_index_id=index.id,\n",
    "        index=i,\n",
    "        indexes=indexes,\n",
    "        text=formatted_chunk,\n",
    "    )\n",
    "    math_article_chunks.append(math_article_chunk)\n",
    "\n",
    "await math_article_chunk_repository.insert_many(math_article_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f6fdc",
   "metadata": {},
   "source": [
    "### 2. MathExpressionRelationship, requires: MathExpression, MathArticleChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89922f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expressions = await math_expression_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")\n",
    "math_article_chunks = await math_article_chunk_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5259e873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2735"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_expected_chunks = sum(\n",
    "    len(math_article_chunk.indexes) - 1 for math_article_chunk in math_article_chunks\n",
    ")\n",
    "num_expected_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionRelationshipDetector as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionRelationship\n",
    "\n",
    "\n",
    "for math_article_chunk in math_article_chunks:\n",
    "    if len(math_article_chunk.indexes) < 2:\n",
    "        continue\n",
    "\n",
    "    start_indexes = math_article_chunk.indexes[:-1]\n",
    "    last_index = math_article_chunk.indexes[-1]\n",
    "    index_pairs = [(index, last_index) for index in start_indexes]\n",
    "\n",
    "    inputs: list[AssistantInput] = []\n",
    "    input_id_to_math_expression_id_pair: dict[UUID, tuple[UUID, UUID]] = {}\n",
    "    input_id_to_math_expression_index_pair: dict[UUID, tuple[int, int]] = {}\n",
    "\n",
    "    for source_index, target_index in index_pairs:\n",
    "        input = AssistantInput(\n",
    "            chunk=math_article_chunk.text, source=source_index, target=target_index\n",
    "        )\n",
    "        inputs.append(input)\n",
    "\n",
    "        source_math_expression = next(\n",
    "            (x for x in math_expressions if x.index == source_index), None\n",
    "        )\n",
    "        target_math_expression = next(\n",
    "            (x for x in math_expressions if x.index == target_index), None\n",
    "        )\n",
    "\n",
    "        if source_math_expression is None or target_math_expression is None:\n",
    "            raise ValueError()\n",
    "\n",
    "        input_id_to_math_expression_id_pair[input.id] = (\n",
    "            source_math_expression.id,\n",
    "            target_math_expression.id,\n",
    "        )\n",
    "        input_id_to_math_expression_index_pair[input.id] = source_index, target_index\n",
    "\n",
    "    outputs = await math_expression_relationship_detector_assistant.concurrent_assist(inputs)\n",
    "    math_expression_relationships = [\n",
    "        MathExpressionRelationship(\n",
    "            math_article_chunk_id=math_article_chunk.id,\n",
    "            math_expression_index_id=index.id,\n",
    "            math_expression_source_id=input_id_to_math_expression_id_pair[output.input_id][0],\n",
    "            math_expression_target_id=input_id_to_math_expression_id_pair[output.input_id][1],\n",
    "            math_expression_source_index=input_id_to_math_expression_index_pair[output.input_id][0],\n",
    "            math_expression_target_index=input_id_to_math_expression_index_pair[output.input_id][1],\n",
    "        )\n",
    "        for output in outputs\n",
    "        if output.relationship_exists\n",
    "    ]\n",
    "\n",
    "    # print(len(outputs))\n",
    "    # print(len(math_expression_relationships))\n",
    "\n",
    "    await math_expression_relationship_repository.insert_many(math_expression_relationships)\n",
    "    await math_expression_graph_repository.insert_many_rels(\n",
    "        math_expression_relationships, rel_to_cls=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b852a9",
   "metadata": {},
   "source": [
    "### 3. MathExpressionRelationshipDescription, requires: MathArticleChunk, MathExpressionRelationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2b89f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2714"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_expression_relationships = await math_expression_relationship_repository.find_many(\n",
    "    filter=dict(math_expression_index_id=index.id)\n",
    ")\n",
    "len(math_expression_relationships)\n",
    "# 2389 < 2735 because llm decided that some of them are not connected\n",
    "# gpt 4o: 2389\n",
    "# gpt 4o nano: 2692 (bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f5b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionRelationshipDescriptionWriter as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionRelationshipDescription\n",
    "\n",
    "\n",
    "math_article_chunk_ids = [\n",
    "    math_expression_relationship.math_article_chunk_id\n",
    "    for math_expression_relationship in math_expression_relationships\n",
    "]\n",
    "math_article_chunks = await math_article_chunk_repository.find_many(\n",
    "    filter=dict(id=math_article_chunk_ids)\n",
    ")\n",
    "\n",
    "inputs: list[AssistantInput] = []\n",
    "input_id_to_math_expression_relationship_id: dict[UUID, UUID] = {}\n",
    "\n",
    "for math_article_chunk, math_expression_relationship in zip(\n",
    "    math_article_chunks, math_expression_relationships\n",
    "):\n",
    "    input = AssistantInput(\n",
    "        chunk=math_article_chunk.text,\n",
    "        source=math_expression_relationship.math_expression_source_index,\n",
    "        target=math_expression_relationship.math_expression_target_index,\n",
    "    )\n",
    "    inputs.append(input)\n",
    "    input_id_to_math_expression_relationship_id[input.id] = math_expression_relationship.id\n",
    "\n",
    "outputs = await math_expression_relationship_description_writer_assistant.concurrent_assist(inputs)\n",
    "descriptions = [\n",
    "    MathExpressionRelationshipDescription(\n",
    "        math_expression_index_id=index.id,\n",
    "        math_expression_relationship_id=input_id_to_math_expression_relationship_id[\n",
    "            output.input_id\n",
    "        ],\n",
    "        text=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]\n",
    "await math_expression_relationship_description_repository.insert_many(descriptions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
