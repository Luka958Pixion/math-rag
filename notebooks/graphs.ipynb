{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32ba622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from math_rag.application.containers import ApplicationContainer\n",
    "    from math_rag.infrastructure.containers import InfrastructureContainer\n",
    "\n",
    "    application_container: ApplicationContainer\n",
    "    infrastructure_container: InfrastructureContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c020d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 19:16:02,497 - INFO - datasets - config.py:54 - PyTorch version 2.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "RESET = False\n",
    "%load_ext hooks.notebook_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb39d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-28 19:47:06,688 - INFO - googleapiclient.discovery_cache - __init__.py:49 - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "google_drive_repository = infrastructure_container.google_drive_repository()\n",
    "math_article_parser_service = infrastructure_container.math_article_parser_service()\n",
    "\n",
    "file_id = google_drive_repository.get_file_id(\n",
    "    Path('ml/lectures/L07-LogisticRegression2/2024_08_10_2174b40686820b4cb591g.tex')\n",
    ")\n",
    "\n",
    "if not file_id:\n",
    "    raise ValueError()\n",
    "\n",
    "file_content = google_drive_repository.get_file_by_id(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081db754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import Index\n",
    "\n",
    "\n",
    "index = Index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92db620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "katex_corrector_assistant = application_container.katex_corrector_assistant()\n",
    "katex_corrector_retrier_assistant = application_container.katex_corrector_retrier_assistant()\n",
    "math_expression_description_writer_assistant = (\n",
    "    application_container.math_expression_description_writer_assistant()\n",
    ")\n",
    "math_expression_description_optimizer_assistant = (\n",
    "    application_container.math_expression_description_optimizer_assistant()\n",
    ")\n",
    "math_expression_comparator_assistant = application_container.math_expression_comparator_assistant()\n",
    "math_expression_relationship_description_writer_assistant = (\n",
    "    application_container.math_expression_relationship_description_writer_assistant()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816adeb6",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aad9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.core.models import MathArticle, MathExpression\n",
    "\n",
    "\n",
    "math_article = MathArticle(\n",
    "    math_expression_dataset_id=None,\n",
    "    index_id=None,\n",
    "    name='article',\n",
    "    bytes=file_content.getvalue(),\n",
    ")\n",
    "math_nodes, _, template = math_article_parser_service.parse_for_index(math_article)\n",
    "katexes = [math_node.latex.strip('$') for math_node in math_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "katex_corrector_service = application_container.katex_corrector_service()\n",
    "\n",
    "valid_katexes = await katex_corrector_service.correct(katexes, max_num_retries=3)\n",
    "math_expressions = [\n",
    "    MathExpression(\n",
    "        math_article_id=math_article.id,\n",
    "        math_expression_dataset_id=None,\n",
    "        index_id=index.id,\n",
    "        latex=node.latex,\n",
    "        katex=katex,\n",
    "        position=node.position,\n",
    "        is_inline=node.is_inline,\n",
    "    )\n",
    "    for node, katex in zip(math_nodes, valid_katexes)\n",
    "]\n",
    "math_expressions.sort(key=lambda x: x.position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd377fa0",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff0d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from uuid import UUID\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class MathExpressionContext(BaseModel):  # TODO unused at the moment\n",
    "    math_expression_id: UUID  # TODO\n",
    "    text: str\n",
    "    index_to_katex: dict[int, str]\n",
    "\n",
    "\n",
    "# TODO connect everything with math expression id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.infrastructure.utils import TemplateContextChunkerUtil, TemplateFormatterUtil\n",
    "\n",
    "\n",
    "context_chunks = TemplateContextChunkerUtil.chunk(template, max_context_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c764cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionDescriptionWriter as AssistantInput,\n",
    ")\n",
    "from math_rag.core.models import MathExpressionDescription\n",
    "\n",
    "\n",
    "SLICE = 5  # TODO remove later, just for testing\n",
    "\n",
    "\n",
    "index_to_katex = {i: math_expression.katex for i, math_expression in enumerate(math_expressions)}\n",
    "index_to_math_expression_id = {\n",
    "    i: math_expression.id for i, math_expression in enumerate(math_expressions)\n",
    "}\n",
    "\n",
    "inputs: list[AssistantInput] = []\n",
    "input_id_to_math_expression_id: dict[UUID, UUID] = {}\n",
    "\n",
    "for chunk in context_chunks[:SLICE]:\n",
    "    context, indexes = TemplateFormatterUtil.format(chunk, index_to_katex, omit_wrapper=False)\n",
    "\n",
    "    for i in indexes:\n",
    "        katex = index_to_katex[index]\n",
    "        input = AssistantInput(katex=f'[{i} | {katex}]', context=context)\n",
    "        inputs.append(input)\n",
    "\n",
    "        input_id_to_math_expression_id[input.id] = index_to_math_expression_id[i]\n",
    "\n",
    "outputs = await math_expression_description_writer_assistant.concurrent_assist(inputs)\n",
    "\n",
    "math_expression_descriptions = [\n",
    "    MathExpressionDescription(\n",
    "        index_id=index.id,\n",
    "        math_expression_id=input_id_to_math_expression_id[output.input_id],\n",
    "        description=output.description,\n",
    "    )\n",
    "    for output in outputs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605301ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 | \\mathbf{x}_{t}]\n",
      "the value of the parameter vector at iteration t in an iterative optimization process\n",
      "-----\n",
      "\n",
      "[5 | f(\\mathrm{x})]\n",
      "the function whose minimization is being considered in the context of optimization methods, specifically in the discussion of gradient descent and Newton's method\n",
      "-----\n",
      "\n",
      "[1 | f(\\mathbf{x})]\n",
      "the function being minimized in the context of optimization methods, where the input is a vector variable\n",
      "-----\n",
      "\n",
      "[4 | \\mathbf{x}_{t}]\n",
      "The current value of the parameter vector at iteration t in an iterative optimization algorithm.\n",
      "-----\n",
      "\n",
      "[6 | f]\n",
      "the function being minimized in the context of optimization methods, specifically referenced as the objective function in both gradient descent and Newton's method\n",
      "-----\n",
      "\n",
      "[1 | f(\\mathbf{x})]\n",
      "A function evaluated at a vector variable, representing the objective function to be minimized in the context of optimization methods such as gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "[8 | x_{0}]\n",
      "the initial value or starting point for the variable x in the context of an iterative minimization process\n",
      "-----\n",
      "\n",
      "[1 | f(\\mathbf{x})]\n",
      "The target expression represents a general function evaluated at a vector variable, which is the objective function being minimized in the context of optimization methods such as gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "[3 | \n",
      "\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta \\nabla f\\left(\\mathbf{x}_{t}\\right)\n",
      "]\n",
      "The expression represents the iterative update rule for gradient descent, where the next iterate is obtained by subtracting the product of a step size and the gradient of the function evaluated at the current iterate from the current iterate.\n",
      "-----\n",
      "\n",
      "[0 | L_{2}]\n",
      "The target expression refers to the L2 regularization term, which is a commonly used penalty in optimization processes to prevent overfitting by discouraging large parameter values.\n",
      "-----\n",
      "\n",
      "[2 | \n",
      "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathrm{x})\n",
      "]\n",
      "The target expression represents the standard parameter update rule in gradient descent, where the current parameter vector is updated by subtracting the product of a learning rate and the gradient of a function with respect to the parameter vector.\n",
      "-----\n",
      "\n",
      "[5 | f(\\mathrm{x})]\n",
      "the function being minimized in the context of Newton's method, evaluated at a variable x\n",
      "-----\n",
      "\n",
      "[4 | \\mathbf{x}_{t}]\n",
      "the value of the parameter vector at iteration t in an iterative optimization process\n",
      "-----\n",
      "\n",
      "[1 | f(\\mathbf{x})]\n",
      "the function being minimized in the context of optimization methods, specifically referenced as the objective function whose value depends on a vector input\n",
      "-----\n",
      "\n",
      "[4 | \\mathbf{x}_{t}]\n",
      "the value of the parameter vector at iteration t in an iterative optimization algorithm\n",
      "-----\n",
      "\n",
      "[2 | \n",
      "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathrm{x})\n",
      "]\n",
      "The target expression represents the update rule for gradient descent, where the current value of the variable is updated by subtracting the product of a step size and the gradient of the function being minimized, both evaluated at the current variable value.\n",
      "-----\n",
      "\n",
      "[7 | f(x)]\n",
      "The target expression represents a function of a single variable, which is the function being minimized in the context of Newton's method for optimization. It is depicted as the black curve in the referenced image and is the subject of the minimization process described.\n",
      "-----\n",
      "\n",
      "[6 | f]\n",
      "The target expression refers to a function, denoted by a single letter, that is being minimized in the context of optimization methods. It is discussed as a general function, potentially of one variable, and is the main objective function in the minimization problem described.\n",
      "-----\n",
      "\n",
      "[8 | x_{0}]\n",
      "The target expression denotes the initial value or starting point for the variable x in the context of minimizing a function of one variable using Newton's method.\n",
      "-----\n",
      "\n",
      "[6 | f]\n",
      "The target expression denotes a function, referenced in the context as the objective function being minimized, and is specifically mentioned as a function of one variable in the discussion of Newton's method.\n",
      "-----\n",
      "\n",
      "[3 | \n",
      "\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta \\nabla f\\left(\\mathbf{x}_{t}\\right)\n",
      "]\n",
      "The expression represents the iterative update rule for gradient descent, where the next iterate is obtained by subtracting the product of a step size and the gradient of the function evaluated at the current iterate from the current iterate.\n",
      "-----\n",
      "\n",
      "[2 | \n",
      "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathrm{x})\n",
      "]\n",
      "This expression represents the parameter update rule in gradient descent, where the current value of the parameter vector is updated by subtracting the product of a learning rate and the gradient of the function being minimized, evaluated at the current parameter value.\n",
      "-----\n",
      "\n",
      "[5 | f(\\mathrm{x})]\n",
      "The target expression represents a function, denoted by a single letter, evaluated at a variable, and is the function being minimized in the context of optimization methods such as gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "[5 | f(\\mathrm{x})]\n",
      "The target expression denotes a function of a variable, which is the function being minimized in the context of optimization methods such as gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "[3 | \n",
      "\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta \\nabla f\\left(\\mathbf{x}_{t}\\right)\n",
      "]\n",
      "The target expression represents the iterative update rule for gradient descent, where the parameter vector at the next iteration is obtained by subtracting the product of a step size and the gradient of the objective function (evaluated at the current parameter vector) from the current parameter vector.\n",
      "-----\n",
      "\n",
      "[6 | f]\n",
      "The target expression refers to a function named f, mentioned in the context of optimization methods, specifically as a function to be minimized. It is discussed both as a general function of a vector variable and, in this instance, as a function of a single variable.\n",
      "-----\n",
      "\n",
      "[3 | \n",
      "\\mathbf{x}_{t+1}=\\mathbf{x}_{t}-\\eta \\nabla f\\left(\\mathbf{x}_{t}\\right)\n",
      "]\n",
      "The target expression represents the iterative update rule for gradient descent, where the next value of the variable at iteration t+1 is obtained by subtracting the product of a step size and the gradient of the function evaluated at the current value from the current value itself.\n",
      "-----\n",
      "\n",
      "[7 | f(x)]\n",
      "The target expression represents a function of a single variable x, which is being minimized in the context of Newton's method. It is depicted as the black curve in a referenced image and is the subject of quadratic approximation at a specific starting point.\n",
      "-----\n",
      "\n",
      "[2 | \n",
      "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathrm{x})\n",
      "]\n",
      "This expression represents the parameter update rule in gradient descent, where the current value of the parameter vector is updated by subtracting the product of a learning rate and the gradient of the objective function with respect to the parameter vector.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_id_to_input = {input.id: input for input in inputs}\n",
    "\n",
    "for output in outputs:\n",
    "    print(input_id_to_input[output.input_id].katex)\n",
    "    print(output.description)\n",
    "    print('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4eabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expression_description_repository = (\n",
    "    infrastructure_container.math_expression_description_repository()\n",
    ")\n",
    "\n",
    "math_expression_description_repository.insert_many(math_expression_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ebf8e",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55912beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionDescriptionOptimizer as AssistantInput,\n",
    ")\n",
    "\n",
    "\n",
    "inputs = [AssistantInput(description=description) for description in math_expression_descriptions]\n",
    "outputs = await math_expression_description_optimizer_assistant.concurrent_assist(inputs)\n",
    "math_expression_descriptions_optimized = [output.description for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0aa5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value of the parameter vector at iteration t in an iterative optimization algorithm.\n",
      "-----\n",
      "\n",
      "Objective function minimized in optimization methods, specifically in gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "Function targeted for minimization in optimization methods, particularly in gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "Function of a variable minimized in optimization methods such as gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "value of the parameter vector at iteration t in an iterative optimization process\n",
      "-----\n",
      "\n",
      "Function minimized using Newton's method, evaluated at variable x.\n",
      "-----\n",
      "\n",
      "L2 regularization is a penalty term in optimization that prevents overfitting by discouraging large parameter values.\n",
      "-----\n",
      "\n",
      "Standard parameter update rule in gradient descent: the current parameter vector is updated by subtracting the product of the learning rate and the gradient of a function with respect to the parameter vector.\n",
      "-----\n",
      "\n",
      "Function minimized in optimization methods with a vector variable input.\n",
      "-----\n",
      "\n",
      "A function of a vector variable representing the objective to be minimized in optimization methods including gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "The target expression is the objective function being minimized, specifically a function of one variable in the context of Newton's method.\n",
      "-----\n",
      "\n",
      "Initial value or starting point for variable x in minimizing a single-variable function using Newton's method.\n",
      "-----\n",
      "\n",
      "Parameter update rule in gradient descent: the parameter vector is updated by subtracting the product of the learning rate and the gradient of the objective function with respect to the parameter vector.\n",
      "-----\n",
      "\n",
      "Parameter update rule in gradient descent: the parameter vector is updated by subtracting the product of the learning rate and the gradient of the minimized function, evaluated at the current parameter value.\n",
      "-----\n",
      "\n",
      "The expression defines the gradient descent update rule: at iteration t+1, the variable is updated by subtracting the product of the step size and the gradient of the function at the current value from the current value.\n",
      "-----\n",
      "\n",
      "The expression defines the gradient descent update rule: the next parameter vector equals the current parameter vector minus the step size multiplied by the gradient of the objective function evaluated at the current parameter vector.\n",
      "-----\n",
      "\n",
      "Objective function in optimization methods; its value depends on a vector input and is minimized.\n",
      "-----\n",
      "\n",
      "A single-variable function f(x) minimized using Newton's method, represented as the black curve in a referenced image and subject to quadratic approximation at a specific starting point.\n",
      "-----\n",
      "\n",
      "initial value or starting point for variable x in an iterative minimization process\n",
      "-----\n",
      "\n",
      "value of the parameter vector at iteration t in an iterative optimization process\n",
      "-----\n",
      "\n",
      "A single-letter function evaluated at a variable, representing the objective function minimized in optimization methods like gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "The expression defines the gradient descent update: the next iterate equals the current iterate minus the step size times the gradient of the function evaluated at the current iterate.\n",
      "-----\n",
      "\n",
      "Function of a single variable minimized using Newton's method, depicted as the black curve in the referenced image and subject of the described minimization process.\n",
      "-----\n",
      "\n",
      "The expression defines the gradient descent update rule: the variable is updated by subtracting the product of the step size and the gradient of the function, both evaluated at the current variable value.\n",
      "-----\n",
      "\n",
      "parameter vector value at iteration t in an iterative optimization algorithm\n",
      "-----\n",
      "\n",
      "Function f, referenced in optimization methods, is to be minimized and is considered both as a general function of a vector variable and, in this instance, as a function of a single variable.\n",
      "-----\n",
      "\n",
      "The expression defines the gradient descent update rule: the next iterate equals the current iterate minus the step size multiplied by the gradient of the function at the current iterate.\n",
      "-----\n",
      "\n",
      "General function evaluated at a vector variable, serving as the objective function minimized in optimization methods including gradient descent and Newton's method.\n",
      "-----\n",
      "\n",
      "A single-letter function representing the main objective in an optimization problem, discussed as a general function, potentially of one variable, and subject to minimization.\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_id_to_input = {input.id: input for input in inputs}\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.description)\n",
    "    print('-----')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_embedder = application_container.default_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72734a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO embeddings\n",
    "\n",
    "embeddings = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d71cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_expression_description_optimized_repository = (\n",
    "    infrastructure_container.math_expression_description_optimized_repository()\n",
    ")\n",
    "\n",
    "await math_expression_description_optimized_repository.upsert_many(\n",
    "    math_expression_descriptions_optimized, embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dd1ea",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_rag.application.models.assistants.inputs import (\n",
    "    MathExpressionComparator as AssistantInput,\n",
    ")\n",
    "\n",
    "\n",
    "input = AssistantInput(katex=None, context=None, other_katex=None, other_context=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
